{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee34dd8-831b-4fcb-b8b0-744b4199960f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-SXM2-16GB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be50412-6c66-40ee-ba9b-67456f972fba",
   "metadata": {},
   "source": [
    "## Reproducibility Attempts on Network Deconvolution\n",
    "\n",
    "https://github.com/yechengxi/deconvolution\n",
    "\n",
    "### Parameters\n",
    "* Model Architectures \n",
    "    * VGG-16 --- vgg16\n",
    "    * ResNet-18 --- resnet18d\n",
    "    * Preact-18 --- preact\n",
    "    * DenseNet-121 --- densenet121\n",
    "    * ResNext-29 --- resnext\n",
    "    * MobileNet v2 --- mobilev2\n",
    "    * DPN-92 --- dpn\n",
    "    * PNASNetA --- pnasnetA\n",
    "    * SENet-18 --- senet\n",
    "    * EfficientNet --- efficient\n",
    "* Datasets --- [CIFAR-10, CIFAR-100]\n",
    "* Performance enhancement techniques --- [Batch Normalization, Network Deconvolution]\n",
    "* Epochs --- [1, 20, 100]\n",
    "* Attempts --- [1, 2, 3]\n",
    "* Optimizer --- SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a200f5c-2148-4c1f-b118-390476245aaa",
   "metadata": {},
   "source": [
    "### Hyper-Parameters\n",
    "* batch\\_size = 128 \n",
    "* learning_rate = 0.1\n",
    "* Weight-Decay = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8ae369-413b-4d3a-a058-0ac61cc3ae71",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">CIFAR-10</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09cb9c5-1322-4340-88e5-253776eb71bf",
   "metadata": {},
   "source": [
    "# VGG-16 --- vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a2875e-d8eb-47ba-8232-ad1d264533d8",
   "metadata": {},
   "source": [
    "#### batch normalization --- vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05bf0b2-89b8-4b4c-9548-5e957cc8df74",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "| Preparing CIFAR-10 dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "2024-03-13 20:55:48.025766: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "==> Building model..\n",
      "Namespace(msg=1, resume='', use_gpu=True, num_workers=16, result_path='checkpoints/cifar10,vgg16,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-13-20.55', checkpoint_path='checkpoints/cifar10,vgg16,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-13-20.55', checkpoint_epoch=-1, print_freq=20, seed=0, optimizer='SGD', lr=0.1, lr_scheduler='cosine', momentum=0.9, weight_decay=0.001, batch_size=128, epochs=1, milestone=0.4, multistep_gamma=0.1, arch='vgg16', dataset='cifar10', init='kaiming_1', save_plot=True, tensorboard=True, loss='CE', method=3, batchnorm=True, deconv=None, delinear=None, block_fc=0, block=64, deconv_iter=5, eps=1e-05, bias=True, stride=3, freeze=False, log_path='checkpoints/cifar10,vgg16,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-13-20.55', channel_deconv=None, start_epoch=0, in_planes=3, input_size=32, classes=('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'), num_outputs=10, train_epoch_logger=<net_util.Logger object at 0x7fc040c13d90>, train_batch_logger=<net_util.Logger object at 0x7fc040c13df0>, test_epoch_logger=<net_util.Logger object at 0x7fc040c13ee0>, writer=<torch.utils.tensorboard.writer.SummaryWriter object at 0x7fc040c5c040>, criterion=CrossEntropyLoss(), logger_n_iter=0)\n",
      "14728266 trainable parameters in the network.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Current learning rate: 0.1\n",
      "training at epoch 1\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Loss: 2.557 | top1: 5.469% ,top5: 42.969%\n",
      "Loss: 3.649 | top1: 12.891% ,top5: 48.438%\n",
      "Loss: 3.740 | top1: 11.458% ,top5: 49.219%\n",
      "Loss: 3.768 | top1: 11.133% ,top5: 49.609%\n",
      "Loss: 3.749 | top1: 10.781% ,top5: 49.688%\n",
      "Loss: 4.432 | top1: 11.328% ,top5: 51.302%\n",
      "Loss: 4.756 | top1: 11.049% ,top5: 52.009%\n",
      "Loss: 5.120 | top1: 10.938% ,top5: 51.562%\n",
      "Loss: 5.176 | top1: 10.851% ,top5: 51.649%\n",
      "Loss: 5.454 | top1: 10.859% ,top5: 50.703%\n",
      "Loss: 5.713 | top1: 10.653% ,top5: 50.284%\n",
      "Loss: 5.586 | top1: 10.417% ,top5: 50.130%\n",
      "Loss: 5.614 | top1: 10.397% ,top5: 50.120%\n",
      "Loss: 5.537 | top1: 10.547% ,top5: 50.391%\n",
      "Loss: 5.460 | top1: 10.365% ,top5: 50.365%\n",
      "Loss: 5.330 | top1: 10.303% ,top5: 50.195%\n",
      "Loss: 5.170 | top1: 10.524% ,top5: 50.460%\n",
      "Loss: 5.094 | top1: 10.503% ,top5: 50.217%\n",
      "Loss: 5.090 | top1: 10.609% ,top5: 50.699%\n",
      "Loss: 5.028 | top1: 10.820% ,top5: 51.133%\n",
      "Loss: 4.931 | top1: 11.124% ,top5: 51.711%\n",
      "Loss: 4.874 | top1: 11.151% ,top5: 51.882%\n",
      "Loss: 4.811 | top1: 11.073% ,top5: 51.936%\n",
      "Loss: 4.744 | top1: 11.100% ,top5: 52.148%\n",
      "Loss: 4.695 | top1: 11.281% ,top5: 52.344%\n",
      "Loss: 4.605 | top1: 11.088% ,top5: 52.374%\n",
      "Loss: 4.520 | top1: 11.082% ,top5: 52.402%\n",
      "Loss: 4.458 | top1: 10.938% ,top5: 52.344%\n",
      "Loss: 4.403 | top1: 11.126% ,top5: 52.425%\n",
      "Loss: 4.362 | top1: 11.094% ,top5: 52.422%\n",
      "Loss: 4.302 | top1: 10.963% ,top5: 52.495%\n",
      "Loss: 4.242 | top1: 10.938% ,top5: 52.490%\n",
      "Loss: 4.186 | top1: 11.009% ,top5: 52.628%\n",
      "Loss: 4.131 | top1: 11.029% ,top5: 52.734%\n",
      "Loss: 4.078 | top1: 11.094% ,top5: 52.679%\n",
      "Loss: 4.036 | top1: 11.024% ,top5: 52.604%\n",
      "Loss: 4.007 | top1: 11.085% ,top5: 52.576%\n",
      "Loss: 3.969 | top1: 11.102% ,top5: 52.467%\n",
      "Loss: 3.928 | top1: 11.118% ,top5: 52.664%\n",
      "Loss: 3.888 | top1: 11.016% ,top5: 52.754%\n",
      "Loss: 3.860 | top1: 11.052% ,top5: 52.915%\n",
      "Loss: 3.834 | top1: 11.068% ,top5: 52.827%\n",
      "Loss: 3.808 | top1: 11.047% ,top5: 52.798%\n",
      "Loss: 3.779 | top1: 11.097% ,top5: 52.859%\n",
      "Loss: 3.760 | top1: 11.111% ,top5: 52.830%\n",
      "Loss: 3.732 | top1: 11.073% ,top5: 52.921%\n",
      "Loss: 3.705 | top1: 10.971% ,top5: 52.809%\n",
      "Loss: 3.676 | top1: 11.068% ,top5: 52.832%\n",
      "Loss: 3.650 | top1: 11.113% ,top5: 52.806%\n",
      "Loss: 3.627 | top1: 11.031% ,top5: 52.953%\n",
      "Loss: 3.604 | top1: 11.106% ,top5: 53.110%\n",
      "Loss: 3.586 | top1: 11.058% ,top5: 52.930%\n",
      "Loss: 3.563 | top1: 11.100% ,top5: 52.978%\n",
      "Loss: 3.550 | top1: 11.256% ,top5: 53.067%\n",
      "Loss: 3.527 | top1: 11.236% ,top5: 52.940%\n",
      "Loss: 3.510 | top1: 11.258% ,top5: 52.972%\n",
      "Loss: 3.494 | top1: 11.239% ,top5: 53.098%\n",
      "Loss: 3.486 | top1: 11.274% ,top5: 53.071%\n",
      "Loss: 3.467 | top1: 11.255% ,top5: 53.032%\n",
      "Loss: 3.448 | top1: 11.289% ,top5: 53.112%\n",
      "Loss: 3.430 | top1: 11.245% ,top5: 53.138%\n",
      "Loss: 3.418 | top1: 11.240% ,top5: 53.175%\n",
      "Loss: 3.400 | top1: 11.210% ,top5: 53.224%\n",
      "Loss: 3.391 | top1: 11.182% ,top5: 53.149%\n",
      "Loss: 3.373 | top1: 11.262% ,top5: 53.197%\n",
      "Loss: 3.356 | top1: 11.328% ,top5: 53.279%\n",
      "Loss: 3.344 | top1: 11.299% ,top5: 53.207%\n",
      "Loss: 3.331 | top1: 11.282% ,top5: 53.125%\n",
      "Loss: 3.316 | top1: 11.322% ,top5: 53.227%\n",
      "Loss: 3.302 | top1: 11.350% ,top5: 53.304%\n",
      "Loss: 3.287 | top1: 11.367% ,top5: 53.367%\n",
      "Loss: 3.275 | top1: 11.306% ,top5: 53.375%\n",
      "Loss: 3.264 | top1: 11.291% ,top5: 53.446%\n",
      "Loss: 3.250 | top1: 11.349% ,top5: 53.399%\n",
      "Loss: 3.237 | top1: 11.354% ,top5: 53.448%\n",
      "Loss: 3.224 | top1: 11.431% ,top5: 53.546%\n",
      "Loss: 3.213 | top1: 11.425% ,top5: 53.582%\n",
      "Loss: 3.201 | top1: 11.398% ,top5: 53.496%\n",
      "Loss: 3.189 | top1: 11.402% ,top5: 53.530%\n",
      "Loss: 3.177 | top1: 11.396% ,top5: 53.574%\n",
      "Loss: 3.167 | top1: 11.381% ,top5: 53.549%\n",
      "Loss: 3.157 | top1: 11.357% ,top5: 53.535%\n",
      "Loss: 3.147 | top1: 11.342% ,top5: 53.464%\n",
      "Loss: 3.136 | top1: 11.319% ,top5: 53.460%\n",
      "Loss: 3.126 | top1: 11.360% ,top5: 53.511%\n",
      "Loss: 3.118 | top1: 11.401% ,top5: 53.543%\n",
      "Loss: 3.109 | top1: 11.422% ,top5: 53.556%\n",
      "Loss: 3.100 | top1: 11.426% ,top5: 53.560%\n",
      "Loss: 3.091 | top1: 11.403% ,top5: 53.511%\n",
      "Loss: 3.083 | top1: 11.389% ,top5: 53.507%\n",
      "Loss: 3.074 | top1: 11.435% ,top5: 53.511%\n",
      "Loss: 3.066 | top1: 11.422% ,top5: 53.490%\n",
      "Loss: 3.057 | top1: 11.391% ,top5: 53.503%\n",
      "Loss: 3.050 | top1: 11.370% ,top5: 53.449%\n",
      "Loss: 3.041 | top1: 11.390% ,top5: 53.470%\n",
      "Loss: 3.033 | top1: 11.393% ,top5: 53.516%\n",
      "Loss: 3.027 | top1: 11.429% ,top5: 53.487%\n",
      "Loss: 3.020 | top1: 11.416% ,top5: 53.436%\n",
      "Loss: 3.012 | top1: 11.395% ,top5: 53.441%\n",
      "Loss: 3.005 | top1: 11.391% ,top5: 53.367%\n",
      "Loss: 3.000 | top1: 11.363% ,top5: 53.349%\n",
      "Loss: 2.993 | top1: 11.328% ,top5: 53.324%\n",
      "Loss: 2.988 | top1: 11.324% ,top5: 53.299%\n",
      "Loss: 2.981 | top1: 11.313% ,top5: 53.313%\n",
      "Loss: 2.974 | top1: 11.272% ,top5: 53.229%\n",
      "Loss: 2.968 | top1: 11.299% ,top5: 53.199%\n",
      "Loss: 2.961 | top1: 11.354% ,top5: 53.169%\n",
      "Loss: 2.955 | top1: 11.386% ,top5: 53.154%\n",
      "Loss: 2.950 | top1: 11.446% ,top5: 53.182%\n",
      "Loss: 2.945 | top1: 11.470% ,top5: 53.246%\n",
      "Loss: 2.940 | top1: 11.444% ,top5: 53.294%\n",
      "Loss: 2.934 | top1: 11.447% ,top5: 53.216%\n",
      "Loss: 2.929 | top1: 11.401% ,top5: 53.166%\n",
      "Loss: 2.923 | top1: 11.390% ,top5: 53.125%\n",
      "Loss: 2.918 | top1: 11.386% ,top5: 53.084%\n",
      "Loss: 2.912 | top1: 11.355% ,top5: 53.078%\n",
      "Loss: 2.907 | top1: 11.345% ,top5: 53.092%\n",
      "Loss: 2.902 | top1: 11.368% ,top5: 53.092%\n",
      "Loss: 2.898 | top1: 11.417% ,top5: 53.145%\n",
      "Loss: 2.893 | top1: 11.393% ,top5: 53.216%\n",
      "Loss: 2.889 | top1: 11.415% ,top5: 53.274%\n",
      "Loss: 2.883 | top1: 11.405% ,top5: 53.253%\n",
      "Loss: 2.879 | top1: 11.388% ,top5: 53.214%\n",
      "Loss: 2.875 | top1: 11.366% ,top5: 53.188%\n",
      "Loss: 2.871 | top1: 11.350% ,top5: 53.144%\n",
      "Loss: 2.865 | top1: 11.415% ,top5: 53.243%\n",
      "Loss: 2.864 | top1: 11.430% ,top5: 53.279%\n",
      "Loss: 2.859 | top1: 11.426% ,top5: 53.247%\n",
      "Loss: 2.862 | top1: 11.446% ,top5: 53.276%\n",
      "Loss: 2.859 | top1: 11.448% ,top5: 53.221%\n",
      "Loss: 2.855 | top1: 11.432% ,top5: 53.250%\n",
      "Loss: 2.851 | top1: 11.441% ,top5: 53.279%\n",
      "Loss: 2.847 | top1: 11.460% ,top5: 53.225%\n",
      "Loss: 2.844 | top1: 11.456% ,top5: 53.224%\n",
      "Loss: 2.841 | top1: 11.464% ,top5: 53.247%\n",
      "Loss: 2.838 | top1: 11.409% ,top5: 53.205%\n",
      "Loss: 2.834 | top1: 11.377% ,top5: 53.176%\n",
      "Loss: 2.831 | top1: 11.373% ,top5: 53.142%\n",
      "Loss: 2.828 | top1: 11.415% ,top5: 53.170%\n",
      "Loss: 2.824 | top1: 11.423% ,top5: 53.147%\n",
      "Loss: 2.820 | top1: 11.442% ,top5: 53.197%\n",
      "Loss: 2.817 | top1: 11.460% ,top5: 53.224%\n",
      "Loss: 2.813 | top1: 11.451% ,top5: 53.267%\n",
      "Loss: 2.810 | top1: 11.458% ,top5: 53.234%\n",
      "Loss: 2.806 | top1: 11.433% ,top5: 53.227%\n",
      "Loss: 2.803 | top1: 11.462% ,top5: 53.243%\n",
      "Loss: 2.799 | top1: 11.458% ,top5: 53.274%\n",
      "Loss: 2.795 | top1: 11.481% ,top5: 53.283%\n",
      "Loss: 2.792 | top1: 11.457% ,top5: 53.309%\n",
      "Loss: 2.789 | top1: 11.443% ,top5: 53.286%\n",
      "Loss: 2.785 | top1: 11.439% ,top5: 53.285%\n",
      "Loss: 2.782 | top1: 11.451% ,top5: 53.253%\n",
      "Loss: 2.778 | top1: 11.484% ,top5: 53.268%\n",
      "Loss: 2.775 | top1: 11.485% ,top5: 53.267%\n",
      "Loss: 2.772 | top1: 11.482% ,top5: 53.261%\n",
      "Loss: 2.769 | top1: 11.488% ,top5: 53.260%\n",
      "Loss: 2.766 | top1: 11.485% ,top5: 53.220%\n",
      "Loss: 2.765 | top1: 11.472% ,top5: 53.229%\n",
      "Loss: 2.763 | top1: 11.493% ,top5: 53.223%\n",
      "Loss: 2.760 | top1: 11.499% ,top5: 53.208%\n",
      "Loss: 2.758 | top1: 11.491% ,top5: 53.149%\n",
      "Loss: 2.755 | top1: 11.497% ,top5: 53.168%\n",
      "Loss: 2.752 | top1: 11.508% ,top5: 53.187%\n",
      "Loss: 2.749 | top1: 11.504% ,top5: 53.206%\n",
      "Loss: 2.746 | top1: 11.501% ,top5: 53.172%\n",
      "Loss: 2.743 | top1: 11.493% ,top5: 53.149%\n",
      "Loss: 2.740 | top1: 11.541% ,top5: 53.167%\n",
      "Loss: 2.737 | top1: 11.547% ,top5: 53.167%\n",
      "Loss: 2.734 | top1: 11.543% ,top5: 53.236%\n",
      "Loss: 2.732 | top1: 11.544% ,top5: 53.217%\n",
      "Loss: 2.729 | top1: 11.554% ,top5: 53.203%\n",
      "Loss: 2.727 | top1: 11.560% ,top5: 53.216%\n",
      "Loss: 2.724 | top1: 11.556% ,top5: 53.202%\n",
      "Loss: 2.721 | top1: 11.557% ,top5: 53.179%\n",
      "Loss: 2.719 | top1: 11.558% ,top5: 53.183%\n",
      "Loss: 2.716 | top1: 11.563% ,top5: 53.112%\n",
      "Loss: 2.714 | top1: 11.595% ,top5: 53.121%\n",
      "Loss: 2.711 | top1: 11.591% ,top5: 53.182%\n",
      "Loss: 2.709 | top1: 11.631% ,top5: 53.190%\n",
      "Loss: 2.707 | top1: 11.645% ,top5: 53.164%\n",
      "Loss: 2.705 | top1: 11.654% ,top5: 53.155%\n",
      "Loss: 2.703 | top1: 11.654% ,top5: 53.125%\n",
      "Loss: 2.700 | top1: 11.668% ,top5: 53.142%\n",
      "Loss: 2.698 | top1: 11.651% ,top5: 53.117%\n",
      "Loss: 2.696 | top1: 11.672% ,top5: 53.138%\n",
      "Loss: 2.694 | top1: 11.652% ,top5: 53.125%\n",
      "Loss: 2.692 | top1: 11.631% ,top5: 53.079%\n",
      "Loss: 2.689 | top1: 11.627% ,top5: 53.059%\n",
      "Loss: 2.687 | top1: 11.620% ,top5: 53.084%\n",
      "Loss: 2.685 | top1: 11.632% ,top5: 53.096%\n",
      "Loss: 2.683 | top1: 11.621% ,top5: 53.088%\n",
      "Loss: 2.681 | top1: 11.613% ,top5: 53.084%\n",
      "Loss: 2.678 | top1: 11.618% ,top5: 53.125%\n",
      "Loss: 2.676 | top1: 11.606% ,top5: 53.129%\n",
      "Loss: 2.674 | top1: 11.627% ,top5: 53.097%\n",
      "Loss: 2.672 | top1: 11.615% ,top5: 53.105%\n",
      "Loss: 2.670 | top1: 11.616% ,top5: 53.073%\n",
      "Loss: 2.668 | top1: 11.624% ,top5: 53.089%\n",
      "Loss: 2.667 | top1: 11.605% ,top5: 53.054%\n",
      "Loss: 2.667 | top1: 11.582% ,top5: 53.059%\n",
      "Loss: 2.665 | top1: 11.563% ,top5: 53.055%\n",
      "Loss: 2.663 | top1: 11.564% ,top5: 53.044%\n",
      "Loss: 2.661 | top1: 11.573% ,top5: 53.063%\n",
      "Loss: 2.659 | top1: 11.585% ,top5: 53.071%\n",
      "Loss: 2.658 | top1: 11.562% ,top5: 53.026%\n",
      "Loss: 2.656 | top1: 11.552% ,top5: 53.038%\n",
      "Loss: 2.654 | top1: 11.560% ,top5: 53.065%\n",
      "Loss: 2.652 | top1: 11.542% ,top5: 53.035%\n",
      "Loss: 2.650 | top1: 11.539% ,top5: 53.017%\n",
      "Loss: 2.648 | top1: 11.522% ,top5: 53.021%\n",
      "Loss: 2.647 | top1: 11.556% ,top5: 53.044%\n",
      "Loss: 2.646 | top1: 11.564% ,top5: 53.070%\n",
      "Loss: 2.644 | top1: 11.550% ,top5: 53.085%\n",
      "Loss: 2.642 | top1: 11.536% ,top5: 53.059%\n",
      "Loss: 2.640 | top1: 11.544% ,top5: 53.060%\n",
      "Loss: 2.639 | top1: 11.545% ,top5: 53.082%\n",
      "Loss: 2.637 | top1: 11.560% ,top5: 53.107%\n",
      "Loss: 2.635 | top1: 11.575% ,top5: 53.096%\n",
      "Loss: 2.633 | top1: 11.569% ,top5: 53.096%\n",
      "Loss: 2.632 | top1: 11.566% ,top5: 53.107%\n",
      "Loss: 2.630 | top1: 11.560% ,top5: 53.111%\n",
      "Loss: 2.629 | top1: 11.564% ,top5: 53.132%\n",
      "Loss: 2.627 | top1: 11.551% ,top5: 53.157%\n",
      "Loss: 2.626 | top1: 11.555% ,top5: 53.132%\n",
      "Loss: 2.624 | top1: 11.559% ,top5: 53.125%\n",
      "Loss: 2.623 | top1: 11.577% ,top5: 53.153%\n",
      "Loss: 2.621 | top1: 11.581% ,top5: 53.180%\n",
      "Loss: 2.620 | top1: 11.578% ,top5: 53.187%\n",
      "Loss: 2.618 | top1: 11.589% ,top5: 53.207%\n",
      "Loss: 2.616 | top1: 11.579% ,top5: 53.186%\n",
      "Loss: 2.615 | top1: 11.573% ,top5: 53.186%\n",
      "Loss: 2.614 | top1: 11.587% ,top5: 53.199%\n",
      "Loss: 2.613 | top1: 11.591% ,top5: 53.199%\n",
      "Loss: 2.612 | top1: 11.582% ,top5: 53.205%\n",
      "Loss: 2.610 | top1: 11.566% ,top5: 53.182%\n",
      "Loss: 2.609 | top1: 11.573% ,top5: 53.204%\n",
      "Loss: 2.607 | top1: 11.564% ,top5: 53.178%\n",
      "Loss: 2.606 | top1: 11.571% ,top5: 53.178%\n",
      "Loss: 2.604 | top1: 11.565% ,top5: 53.190%\n",
      "Loss: 2.603 | top1: 11.576% ,top5: 53.223%\n",
      "Loss: 2.602 | top1: 11.579% ,top5: 53.200%\n",
      "Loss: 2.601 | top1: 11.583% ,top5: 53.190%\n",
      "Loss: 2.599 | top1: 11.571% ,top5: 53.160%\n",
      "Loss: 2.598 | top1: 11.575% ,top5: 53.163%\n",
      "Loss: 2.597 | top1: 11.575% ,top5: 53.144%\n",
      "Loss: 2.595 | top1: 11.579% ,top5: 53.179%\n",
      "Loss: 2.594 | top1: 11.570% ,top5: 53.185%\n",
      "Loss: 2.593 | top1: 11.577% ,top5: 53.163%\n",
      "Loss: 2.591 | top1: 11.562% ,top5: 53.144%\n",
      "Loss: 2.590 | top1: 11.569% ,top5: 53.156%\n",
      "Loss: 2.589 | top1: 11.572% ,top5: 53.172%\n",
      "Loss: 2.588 | top1: 11.604% ,top5: 53.178%\n",
      "Loss: 2.586 | top1: 11.614% ,top5: 53.205%\n",
      "Loss: 2.585 | top1: 11.617% ,top5: 53.211%\n",
      "Loss: 2.584 | top1: 11.608% ,top5: 53.177%\n",
      "Loss: 2.583 | top1: 11.609% ,top5: 53.207%\n",
      "Loss: 2.581 | top1: 11.637% ,top5: 53.195%\n",
      "Loss: 2.580 | top1: 11.652% ,top5: 53.183%\n",
      "Loss: 2.579 | top1: 11.655% ,top5: 53.194%\n",
      "Loss: 2.578 | top1: 11.653% ,top5: 53.155%\n",
      "Loss: 2.576 | top1: 11.662% ,top5: 53.179%\n",
      "Loss: 2.575 | top1: 11.668% ,top5: 53.164%\n",
      "Loss: 2.574 | top1: 11.674% ,top5: 53.158%\n",
      "Loss: 2.573 | top1: 11.657% ,top5: 53.110%\n",
      "Loss: 2.572 | top1: 11.657% ,top5: 53.113%\n",
      "Loss: 2.571 | top1: 11.681% ,top5: 53.122%\n",
      "Loss: 2.569 | top1: 11.689% ,top5: 53.131%\n",
      "Loss: 2.568 | top1: 11.704% ,top5: 53.169%\n",
      "Loss: 2.567 | top1: 11.727% ,top5: 53.169%\n",
      "Loss: 2.566 | top1: 11.725% ,top5: 53.186%\n",
      "Loss: 2.565 | top1: 11.722% ,top5: 53.217%\n",
      "Loss: 2.563 | top1: 11.733% ,top5: 53.234%\n",
      "Loss: 2.562 | top1: 11.719% ,top5: 53.242%\n",
      "Loss: 2.561 | top1: 11.719% ,top5: 53.239%\n",
      "Loss: 2.560 | top1: 11.705% ,top5: 53.250%\n",
      "Loss: 2.559 | top1: 11.702% ,top5: 53.272%\n",
      "Loss: 2.558 | top1: 11.713% ,top5: 53.263%\n",
      "Loss: 2.557 | top1: 11.727% ,top5: 53.296%\n",
      "Loss: 2.556 | top1: 11.736% ,top5: 53.321%\n",
      "Loss: 2.555 | top1: 11.733% ,top5: 53.351%\n",
      "Loss: 2.554 | top1: 11.747% ,top5: 53.372%\n",
      "Loss: 2.553 | top1: 11.735% ,top5: 53.363%\n",
      "Loss: 2.552 | top1: 11.752% ,top5: 53.360%\n",
      "Loss: 2.551 | top1: 11.774% ,top5: 53.381%\n",
      "Loss: 2.550 | top1: 11.771% ,top5: 53.383%\n",
      "Loss: 2.549 | top1: 11.760% ,top5: 53.387%\n",
      "Loss: 2.548 | top1: 11.751% ,top5: 53.362%\n",
      "Loss: 2.548 | top1: 11.762% ,top5: 53.358%\n",
      "Loss: 2.546 | top1: 11.784% ,top5: 53.395%\n",
      "Loss: 2.545 | top1: 11.789% ,top5: 53.408%\n",
      "Loss: 2.544 | top1: 11.805% ,top5: 53.404%\n",
      "Loss: 2.543 | top1: 11.810% ,top5: 53.403%\n",
      "Loss: 2.543 | top1: 11.801% ,top5: 53.397%\n",
      "Loss: 2.542 | top1: 11.806% ,top5: 53.404%\n",
      "Loss: 2.540 | top1: 11.811% ,top5: 53.419%\n",
      "Loss: 2.540 | top1: 11.816% ,top5: 53.429%\n",
      "Loss: 2.539 | top1: 11.827% ,top5: 53.451%\n",
      "Loss: 2.538 | top1: 11.834% ,top5: 53.458%\n",
      "Loss: 2.537 | top1: 11.852% ,top5: 53.465%\n",
      "Loss: 2.536 | top1: 11.867% ,top5: 53.495%\n",
      "Loss: 2.535 | top1: 11.885% ,top5: 53.504%\n",
      "Loss: 2.534 | top1: 11.908% ,top5: 53.503%\n",
      "Loss: 2.533 | top1: 11.912% ,top5: 53.525%\n",
      "Loss: 2.532 | top1: 11.909% ,top5: 53.523%\n",
      "Loss: 2.532 | top1: 11.913% ,top5: 53.494%\n",
      "Loss: 2.531 | top1: 11.908% ,top5: 53.505%\n",
      "Loss: 2.530 | top1: 11.912% ,top5: 53.530%\n",
      "Loss: 2.530 | top1: 11.922% ,top5: 53.528%\n",
      "Loss: 2.529 | top1: 11.921% ,top5: 53.530%\n",
      "Loss: 2.528 | top1: 11.908% ,top5: 53.536%\n",
      "Loss: 2.527 | top1: 11.925% ,top5: 53.542%\n",
      "Loss: 2.526 | top1: 11.939% ,top5: 53.543%\n",
      "Loss: 2.525 | top1: 11.928% ,top5: 53.559%\n",
      "Loss: 2.525 | top1: 11.910% ,top5: 53.550%\n",
      "Loss: 2.524 | top1: 11.910% ,top5: 53.554%\n",
      "Loss: 2.523 | top1: 11.904% ,top5: 53.530%\n",
      "Loss: 2.522 | top1: 11.896% ,top5: 53.544%\n",
      "Loss: 2.521 | top1: 11.905% ,top5: 53.582%\n",
      "Loss: 2.520 | top1: 11.915% ,top5: 53.590%\n",
      "Loss: 2.520 | top1: 11.909% ,top5: 53.586%\n",
      "Loss: 2.519 | top1: 11.940% ,top5: 53.614%\n",
      "Loss: 2.518 | top1: 11.954% ,top5: 53.627%\n",
      "Loss: 2.517 | top1: 11.958% ,top5: 53.652%\n",
      "Loss: 2.516 | top1: 11.953% ,top5: 53.668%\n",
      "Loss: 2.516 | top1: 11.947% ,top5: 53.678%\n",
      "Loss: 2.515 | top1: 11.942% ,top5: 53.700%\n",
      "Loss: 2.514 | top1: 11.941% ,top5: 53.689%\n",
      "Loss: 2.513 | top1: 11.940% ,top5: 53.690%\n",
      "Loss: 2.512 | top1: 11.935% ,top5: 53.685%\n",
      "Loss: 2.512 | top1: 11.941% ,top5: 53.674%\n",
      "Loss: 2.511 | top1: 11.934% ,top5: 53.663%\n",
      "Loss: 2.510 | top1: 11.942% ,top5: 53.678%\n",
      "Loss: 2.509 | top1: 11.951% ,top5: 53.707%\n",
      "Loss: 2.508 | top1: 11.957% ,top5: 53.707%\n",
      "Loss: 2.508 | top1: 11.959% ,top5: 53.706%\n",
      "Loss: 2.507 | top1: 11.963% ,top5: 53.678%\n",
      "Loss: 2.506 | top1: 11.962% ,top5: 53.691%\n",
      "Loss: 2.505 | top1: 11.966% ,top5: 53.707%\n",
      "Loss: 2.505 | top1: 11.984% ,top5: 53.710%\n",
      "Loss: 2.504 | top1: 12.008% ,top5: 53.718%\n",
      "Loss: 2.503 | top1: 12.017% ,top5: 53.730%\n",
      "Loss: 2.502 | top1: 12.011% ,top5: 53.733%\n",
      "Loss: 2.501 | top1: 12.019% ,top5: 53.733%\n",
      "Loss: 2.501 | top1: 12.009% ,top5: 53.736%\n",
      "Loss: 2.500 | top1: 11.995% ,top5: 53.721%\n",
      "Loss: 2.500 | top1: 12.001% ,top5: 53.728%\n",
      "Loss: 2.499 | top1: 12.011% ,top5: 53.722%\n",
      "Loss: 2.498 | top1: 12.013% ,top5: 53.697%\n",
      "Loss: 2.497 | top1: 12.019% ,top5: 53.680%\n",
      "Loss: 2.496 | top1: 12.031% ,top5: 53.692%\n",
      "Loss: 2.496 | top1: 12.039% ,top5: 53.697%\n",
      "Loss: 2.495 | top1: 12.025% ,top5: 53.700%\n",
      "Loss: 2.494 | top1: 12.020% ,top5: 53.698%\n",
      "Loss: 2.494 | top1: 12.023% ,top5: 53.679%\n",
      "Loss: 2.493 | top1: 12.027% ,top5: 53.666%\n",
      "Loss: 2.492 | top1: 12.030% ,top5: 53.667%\n",
      "Loss: 2.492 | top1: 12.038% ,top5: 53.679%\n",
      "Loss: 2.491 | top1: 12.044% ,top5: 53.688%\n",
      "Loss: 2.490 | top1: 12.054% ,top5: 53.682%\n",
      "Loss: 2.490 | top1: 12.070% ,top5: 53.709%\n",
      "Loss: 2.489 | top1: 12.072% ,top5: 53.714%\n",
      "Loss: 2.488 | top1: 12.066% ,top5: 53.701%\n",
      "Loss: 2.488 | top1: 12.061% ,top5: 53.710%\n",
      "Loss: 2.487 | top1: 12.064% ,top5: 53.743%\n",
      "Loss: 2.487 | top1: 12.068% ,top5: 53.737%\n",
      "Loss: 2.486 | top1: 12.069% ,top5: 53.727%\n",
      "Loss: 2.485 | top1: 12.074% ,top5: 53.740%\n",
      "Loss: 2.484 | top1: 12.084% ,top5: 53.760%\n",
      "Loss: 2.484 | top1: 12.102% ,top5: 53.775%\n",
      "Loss: 2.483 | top1: 12.107% ,top5: 53.792%\n",
      "Loss: 2.482 | top1: 12.117% ,top5: 53.814%\n",
      "Loss: 2.482 | top1: 12.118% ,top5: 53.818%\n",
      "Loss: 2.481 | top1: 12.117% ,top5: 53.802%\n",
      "Loss: 2.481 | top1: 12.111% ,top5: 53.808%\n",
      "Loss: 2.480 | top1: 12.121% ,top5: 53.808%\n",
      "Loss: 2.480 | top1: 12.130% ,top5: 53.815%\n",
      "Loss: 2.479 | top1: 12.131% ,top5: 53.811%\n",
      "Loss: 2.479 | top1: 12.130% ,top5: 53.815%\n",
      "Loss: 2.478 | top1: 12.137% ,top5: 53.803%\n",
      "Loss: 2.477 | top1: 12.136% ,top5: 53.803%\n",
      "Loss: 2.477 | top1: 12.133% ,top5: 53.814%\n",
      "Loss: 2.476 | top1: 12.136% ,top5: 53.818%\n",
      "Loss: 2.475 | top1: 12.143% ,top5: 53.829%\n",
      "Loss: 2.475 | top1: 12.130% ,top5: 53.839%\n",
      "Loss: 2.474 | top1: 12.137% ,top5: 53.827%\n",
      "Loss: 2.474 | top1: 12.128% ,top5: 53.829%\n",
      "Loss: 2.473 | top1: 12.112% ,top5: 53.848%\n",
      "Loss: 2.472 | top1: 12.119% ,top5: 53.864%\n",
      "Loss: 2.472 | top1: 12.120% ,top5: 53.864%\n",
      "Loss: 2.471 | top1: 12.137% ,top5: 53.888%\n",
      "Loss: 2.471 | top1: 12.138% ,top5: 53.872%\n",
      "Loss: 2.471 | top1: 12.138%, top5: 53.872% elasped time:  38 seconds.\n",
      "Testing at epoch 1\n",
      "Loss: 2.201 | top1: 14.062% ,top5: 56.250%\n",
      "Loss: 2.216 | top1: 12.891% ,top5: 58.594%\n",
      "Loss: 2.212 | top1: 13.281% ,top5: 60.417%\n",
      "Loss: 2.231 | top1: 12.305% ,top5: 58.789%\n",
      "Loss: 2.255 | top1: 13.438% ,top5: 58.750%\n",
      "Loss: 2.260 | top1: 13.802% ,top5: 58.333%\n",
      "Loss: 2.251 | top1: 13.951% ,top5: 57.254%\n",
      "Loss: 2.252 | top1: 13.770% ,top5: 56.836%\n",
      "Loss: 2.251 | top1: 13.455% ,top5: 56.597%\n",
      "Loss: 2.247 | top1: 13.438% ,top5: 55.938%\n",
      "Loss: 2.240 | top1: 13.920% ,top5: 56.747%\n",
      "Loss: 2.236 | top1: 14.128% ,top5: 56.315%\n",
      "Loss: 2.236 | top1: 13.942% ,top5: 56.611%\n",
      "Loss: 2.235 | top1: 13.728% ,top5: 56.529%\n",
      "Loss: 2.239 | top1: 13.438% ,top5: 56.094%\n",
      "Loss: 2.240 | top1: 13.184% ,top5: 56.348%\n",
      "Loss: 2.244 | top1: 13.006% ,top5: 56.618%\n",
      "Loss: 2.245 | top1: 12.891% ,top5: 56.076%\n",
      "Loss: 2.243 | top1: 13.035% ,top5: 56.373%\n",
      "Loss: 2.242 | top1: 13.164% ,top5: 56.289%\n",
      "Loss: 2.239 | top1: 13.170% ,top5: 56.771%\n",
      "Loss: 2.237 | top1: 13.494% ,top5: 56.996%\n",
      "Loss: 2.238 | top1: 13.417% ,top5: 57.099%\n",
      "Loss: 2.237 | top1: 13.151% ,top5: 57.161%\n",
      "Loss: 2.237 | top1: 13.094% ,top5: 57.000%\n",
      "Loss: 2.239 | top1: 13.101% ,top5: 56.701%\n",
      "Loss: 2.242 | top1: 12.992% ,top5: 56.597%\n",
      "Loss: 2.240 | top1: 12.919% ,top5: 56.417%\n",
      "Loss: 2.239 | top1: 13.308% ,top5: 56.789%\n",
      "Loss: 2.237 | top1: 13.281% ,top5: 56.667%\n",
      "Loss: 2.238 | top1: 13.256% ,top5: 56.578%\n",
      "Loss: 2.238 | top1: 13.452% ,top5: 56.470%\n",
      "Loss: 2.237 | top1: 13.494% ,top5: 56.416%\n",
      "Loss: 2.239 | top1: 13.373% ,top5: 56.204%\n",
      "Loss: 2.238 | top1: 13.438% ,top5: 56.161%\n",
      "Loss: 2.238 | top1: 13.368% ,top5: 56.076%\n",
      "Loss: 2.237 | top1: 13.366% ,top5: 56.144%\n",
      "Loss: 2.236 | top1: 13.590% ,top5: 56.271%\n",
      "Loss: 2.235 | top1: 13.642% ,top5: 56.270%\n",
      "Loss: 2.236 | top1: 13.633% ,top5: 56.270%\n",
      "Loss: 2.236 | top1: 13.605% ,top5: 56.326%\n",
      "Loss: 2.236 | top1: 13.542% ,top5: 56.362%\n",
      "Loss: 2.235 | top1: 13.699% ,top5: 56.504%\n",
      "Loss: 2.234 | top1: 13.707% ,top5: 56.676%\n",
      "Loss: 2.234 | top1: 13.646% ,top5: 56.788%\n",
      "Loss: 2.234 | top1: 13.706% ,top5: 56.776%\n",
      "Loss: 2.234 | top1: 13.747% ,top5: 56.799%\n",
      "Loss: 2.236 | top1: 13.688% ,top5: 56.673%\n",
      "Loss: 2.242 | top1: 13.616% ,top5: 56.569%\n",
      "Loss: 2.243 | top1: 13.688% ,top5: 56.562%\n",
      "Loss: 2.243 | top1: 13.741% ,top5: 56.449%\n",
      "Loss: 2.248 | top1: 13.792% ,top5: 56.460%\n",
      "Loss: 2.249 | top1: 13.812% ,top5: 56.560%\n",
      "Loss: 2.247 | top1: 13.947% ,top5: 56.684%\n",
      "Loss: 2.247 | top1: 13.878% ,top5: 56.534%\n",
      "Loss: 2.247 | top1: 13.881% ,top5: 56.557%\n",
      "Loss: 2.247 | top1: 13.953% ,top5: 56.593%\n",
      "Loss: 2.246 | top1: 13.995% ,top5: 56.560%\n",
      "Loss: 2.246 | top1: 13.904% ,top5: 56.581%\n",
      "Loss: 2.245 | top1: 13.906% ,top5: 56.628%\n",
      "Loss: 2.245 | top1: 13.883% ,top5: 56.596%\n",
      "Loss: 2.245 | top1: 13.823% ,top5: 56.489%\n",
      "Loss: 2.245 | top1: 13.765% ,top5: 56.473%\n",
      "Loss: 2.245 | top1: 13.708% ,top5: 56.421%\n",
      "Loss: 2.245 | top1: 13.702% ,top5: 56.310%\n",
      "Loss: 2.245 | top1: 13.696% ,top5: 56.226%\n",
      "Loss: 2.245 | top1: 13.631% ,top5: 56.238%\n",
      "Loss: 2.245 | top1: 13.649% ,top5: 56.204%\n",
      "Loss: 2.245 | top1: 13.644% ,top5: 56.216%\n",
      "Loss: 2.244 | top1: 13.694% ,top5: 56.205%\n",
      "Loss: 2.243 | top1: 13.743% ,top5: 56.140%\n",
      "Loss: 2.243 | top1: 13.715% ,top5: 56.207%\n",
      "Loss: 2.242 | top1: 13.784% ,top5: 56.175%\n",
      "Loss: 2.241 | top1: 13.809% ,top5: 56.208%\n",
      "Loss: 2.241 | top1: 13.917% ,top5: 56.198%\n",
      "Loss: 2.240 | top1: 13.877% ,top5: 56.137%\n",
      "Loss: 2.240 | top1: 13.839% ,top5: 56.149%\n",
      "Loss: 2.240 | top1: 13.832% ,top5: 56.120%\n",
      "Loss: 2.240 | top1: 13.820% ,top5: 56.130%\n",
      "Loss: 2.240 | top1: 13.820%, top5: 56.130%, elasped time:   7 seconds. Best Acc: 13.820%\n",
      "Training finished successfully. Model size:  14728266\n",
      "Best acc:  13.82\n"
     ]
    }
   ],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch vgg16 --epochs 1 --dataset cifar100  --batch-size 128 --msg True --deconv False --block-fc 0 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6213acd-8852-4a82-86e9-56336ffd6fd7",
   "metadata": {},
   "source": [
    "#### network deconvolution --- vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f5392a-ded9-4655-87a5-216150e85889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "| Preparing CIFAR-10 dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "2024-03-13 21:00:08.063912: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "==> Building model..\n",
      "************ Batch norm disabled when deconv is used. ************\n",
      "Namespace(msg=1, resume='', use_gpu=True, num_workers=16, result_path='checkpoints/cifar10,vgg16,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512/03-13-20.59', checkpoint_path='checkpoints/cifar10,vgg16,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512/03-13-20.59', checkpoint_epoch=-1, print_freq=20, seed=0, optimizer='SGD', lr=0.1, lr_scheduler='cosine', momentum=0.9, weight_decay=0.001, batch_size=128, epochs=1, milestone=0.4, multistep_gamma=0.1, arch='vgg16', dataset='cifar10', init='kaiming_1', save_plot=True, tensorboard=True, loss='CE', method=3, batchnorm=False, deconv=functools.partial(<class 'models.deconv.FastDeconv'>, bias=True, eps=1e-05, n_iter=5, block=64, sampling_stride=3), delinear=functools.partial(<class 'models.deconv.Delinear'>, block=512, eps=1e-05, n_iter=5), block_fc=512, block=64, deconv_iter=5, eps=1e-05, bias=True, stride=3, freeze=False, log_path='checkpoints/cifar10,vgg16,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512/03-13-20.59', channel_deconv=None, start_epoch=0, in_planes=3, input_size=32, classes=('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'), num_outputs=10, train_epoch_logger=<net_util.Logger object at 0x7fa15748fd90>, train_batch_logger=<net_util.Logger object at 0x7fa15748fdf0>, test_epoch_logger=<net_util.Logger object at 0x7fa15748fee0>, writer=<torch.utils.tensorboard.writer.SummaryWriter object at 0x7fa1574d4040>, criterion=CrossEntropyLoss(), logger_n_iter=0)\n",
      "14719818 trainable parameters in the network.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Current learning rate: 0.1\n",
      "training at epoch 1\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/oruma001/CS895/project/codebase/models/deconv.py:305: UserWarning: This overload of addmm is deprecated:\n",
      "\taddmm(Number beta, Tensor input, Number alpha, Tensor mat1, Tensor mat2, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddmm(Tensor input, Tensor mat1, Tensor mat2, *, Number beta, Number alpha, Tensor out) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  Cov = torch.addmm(self.eps, Id, 1. / X.shape[0], X.t(), X)\n",
      "Loss: 2.328 | top1: 7.031% ,top5: 49.219%\n",
      "Loss: 2.326 | top1: 8.984% ,top5: 51.172%\n",
      "Loss: 2.320 | top1: 9.896% ,top5: 50.521%\n",
      "Loss: 2.306 | top1: 10.352% ,top5: 52.734%\n",
      "Loss: 2.294 | top1: 11.719% ,top5: 55.156%\n",
      "Loss: 2.270 | top1: 14.062% ,top5: 58.724%\n",
      "Loss: 2.252 | top1: 14.732% ,top5: 60.379%\n",
      "Loss: 2.237 | top1: 15.527% ,top5: 62.207%\n",
      "Loss: 2.219 | top1: 17.014% ,top5: 63.976%\n",
      "Loss: 2.197 | top1: 17.812% ,top5: 66.406%\n",
      "Loss: 2.179 | top1: 19.176% ,top5: 67.969%\n",
      "Loss: 2.162 | top1: 20.638% ,top5: 68.880%\n",
      "Loss: 2.148 | top1: 21.514% ,top5: 69.712%\n",
      "Loss: 2.133 | top1: 22.210% ,top5: 70.592%\n",
      "Loss: 2.113 | top1: 23.021% ,top5: 71.719%\n",
      "Loss: 2.093 | top1: 23.877% ,top5: 72.900%\n",
      "Loss: 2.074 | top1: 25.138% ,top5: 73.529%\n",
      "Loss: 2.060 | top1: 25.694% ,top5: 74.089%\n",
      "Loss: 2.040 | top1: 26.686% ,top5: 75.082%\n",
      "Loss: 2.025 | top1: 27.344% ,top5: 75.820%\n",
      "Loss: 2.007 | top1: 28.274% ,top5: 76.562%\n",
      "Loss: 1.993 | top1: 28.729% ,top5: 77.237%\n",
      "Loss: 1.980 | top1: 29.246% ,top5: 77.649%\n",
      "Loss: 1.965 | top1: 29.720% ,top5: 78.223%\n",
      "Loss: 1.951 | top1: 30.375% ,top5: 78.750%\n",
      "Loss: 1.939 | top1: 30.980% ,top5: 79.117%\n",
      "Loss: 1.927 | top1: 31.424% ,top5: 79.485%\n",
      "Loss: 1.921 | top1: 31.920% ,top5: 79.520%\n",
      "Loss: 1.911 | top1: 32.139% ,top5: 79.849%\n",
      "Loss: 1.899 | top1: 32.604% ,top5: 80.286%\n",
      "Loss: 1.891 | top1: 32.863% ,top5: 80.595%\n",
      "Loss: 1.882 | top1: 33.203% ,top5: 80.811%\n",
      "Loss: 1.877 | top1: 33.357% ,top5: 81.084%\n",
      "Loss: 1.868 | top1: 33.709% ,top5: 81.319%\n",
      "Loss: 1.860 | top1: 33.951% ,top5: 81.607%\n",
      "Loss: 1.852 | top1: 34.201% ,top5: 81.923%\n",
      "Loss: 1.845 | top1: 34.502% ,top5: 82.031%\n",
      "Loss: 1.837 | top1: 34.786% ,top5: 82.299%\n",
      "Loss: 1.826 | top1: 35.357% ,top5: 82.572%\n",
      "Loss: 1.821 | top1: 35.430% ,top5: 82.793%\n",
      "Loss: 1.812 | top1: 35.709% ,top5: 83.060%\n",
      "Loss: 1.803 | top1: 35.956% ,top5: 83.315%\n",
      "Loss: 1.792 | top1: 36.265% ,top5: 83.539%\n",
      "Loss: 1.783 | top1: 36.577% ,top5: 83.718%\n",
      "Loss: 1.771 | top1: 37.014% ,top5: 83.941%\n",
      "Loss: 1.767 | top1: 37.177% ,top5: 84.052%\n",
      "Loss: 1.761 | top1: 37.267% ,top5: 84.159%\n",
      "Loss: 1.755 | top1: 37.370% ,top5: 84.310%\n",
      "Loss: 1.747 | top1: 37.612% ,top5: 84.550%\n",
      "Loss: 1.742 | top1: 37.734% ,top5: 84.672%\n",
      "Loss: 1.735 | top1: 37.990% ,top5: 84.804%\n",
      "Loss: 1.729 | top1: 38.161% ,top5: 84.991%\n",
      "Loss: 1.721 | top1: 38.429% ,top5: 85.156%\n",
      "Loss: 1.714 | top1: 38.773% ,top5: 85.315%\n",
      "Loss: 1.709 | top1: 38.977% ,top5: 85.412%\n",
      "Loss: 1.703 | top1: 39.146% ,top5: 85.519%\n",
      "Loss: 1.699 | top1: 39.296% ,top5: 85.554%\n",
      "Loss: 1.695 | top1: 39.507% ,top5: 85.601%\n",
      "Loss: 1.689 | top1: 39.764% ,top5: 85.699%\n",
      "Loss: 1.685 | top1: 39.831% ,top5: 85.833%\n",
      "Loss: 1.678 | top1: 40.023% ,top5: 85.976%\n",
      "Loss: 1.674 | top1: 40.197% ,top5: 86.051%\n",
      "Loss: 1.672 | top1: 40.352% ,top5: 86.136%\n",
      "Loss: 1.668 | top1: 40.479% ,top5: 86.218%\n",
      "Loss: 1.663 | top1: 40.697% ,top5: 86.298%\n",
      "Loss: 1.659 | top1: 40.838% ,top5: 86.387%\n",
      "Loss: 1.656 | top1: 40.951% ,top5: 86.427%\n",
      "Loss: 1.653 | top1: 41.108% ,top5: 86.477%\n",
      "Loss: 1.649 | top1: 41.214% ,top5: 86.504%\n",
      "Loss: 1.646 | top1: 41.350% ,top5: 86.618%\n",
      "Loss: 1.642 | top1: 41.527% ,top5: 86.730%\n",
      "Loss: 1.638 | top1: 41.678% ,top5: 86.882%\n",
      "Loss: 1.632 | top1: 41.845% ,top5: 86.986%\n",
      "Loss: 1.627 | top1: 41.997% ,top5: 87.078%\n",
      "Loss: 1.623 | top1: 42.167% ,top5: 87.188%\n",
      "Loss: 1.618 | top1: 42.362% ,top5: 87.315%\n",
      "Loss: 1.614 | top1: 42.532% ,top5: 87.388%\n",
      "Loss: 1.610 | top1: 42.668% ,top5: 87.460%\n",
      "Loss: 1.605 | top1: 42.840% ,top5: 87.579%\n",
      "Loss: 1.601 | top1: 42.979% ,top5: 87.666%\n",
      "Loss: 1.596 | top1: 43.152% ,top5: 87.751%\n",
      "Loss: 1.592 | top1: 43.302% ,top5: 87.824%\n",
      "Loss: 1.587 | top1: 43.468% ,top5: 87.895%\n",
      "Loss: 1.586 | top1: 43.536% ,top5: 87.919%\n",
      "Loss: 1.583 | top1: 43.649% ,top5: 87.950%\n",
      "Loss: 1.580 | top1: 43.723% ,top5: 88.009%\n",
      "Loss: 1.576 | top1: 43.831% ,top5: 88.084%\n",
      "Loss: 1.574 | top1: 43.936% ,top5: 88.104%\n",
      "Loss: 1.570 | top1: 44.066% ,top5: 88.158%\n",
      "Loss: 1.566 | top1: 44.262% ,top5: 88.255%\n",
      "Loss: 1.564 | top1: 44.325% ,top5: 88.307%\n",
      "Loss: 1.561 | top1: 44.463% ,top5: 88.366%\n",
      "Loss: 1.556 | top1: 44.624% ,top5: 88.424%\n",
      "Loss: 1.553 | top1: 44.722% ,top5: 88.497%\n",
      "Loss: 1.550 | top1: 44.794% ,top5: 88.561%\n",
      "Loss: 1.549 | top1: 44.808% ,top5: 88.647%\n",
      "Loss: 1.545 | top1: 44.926% ,top5: 88.700%\n",
      "Loss: 1.542 | top1: 45.081% ,top5: 88.736%\n",
      "Loss: 1.538 | top1: 45.178% ,top5: 88.810%\n",
      "Loss: 1.536 | top1: 45.250% ,top5: 88.883%\n",
      "Loss: 1.533 | top1: 45.367% ,top5: 88.931%\n",
      "Loss: 1.529 | top1: 45.473% ,top5: 88.978%\n",
      "Loss: 1.526 | top1: 45.608% ,top5: 89.009%\n",
      "Loss: 1.523 | top1: 45.733% ,top5: 89.100%\n",
      "Loss: 1.520 | top1: 45.871% ,top5: 89.167%\n",
      "Loss: 1.517 | top1: 45.976% ,top5: 89.217%\n",
      "Loss: 1.514 | top1: 46.072% ,top5: 89.267%\n",
      "Loss: 1.512 | top1: 46.166% ,top5: 89.301%\n",
      "Loss: 1.510 | top1: 46.259% ,top5: 89.328%\n",
      "Loss: 1.506 | top1: 46.420% ,top5: 89.389%\n",
      "Loss: 1.503 | top1: 46.523% ,top5: 89.457%\n",
      "Loss: 1.501 | top1: 46.610% ,top5: 89.509%\n",
      "Loss: 1.499 | top1: 46.633% ,top5: 89.574%\n",
      "Loss: 1.497 | top1: 46.724% ,top5: 89.576%\n",
      "Loss: 1.494 | top1: 46.882% ,top5: 89.620%\n",
      "Loss: 1.491 | top1: 47.037% ,top5: 89.648%\n",
      "Loss: 1.489 | top1: 47.122% ,top5: 89.697%\n",
      "Loss: 1.486 | top1: 47.226% ,top5: 89.751%\n",
      "Loss: 1.483 | top1: 47.354% ,top5: 89.804%\n",
      "Loss: 1.481 | top1: 47.409% ,top5: 89.837%\n",
      "Loss: 1.478 | top1: 47.501% ,top5: 89.889%\n",
      "Loss: 1.475 | top1: 47.624% ,top5: 89.927%\n",
      "Loss: 1.472 | top1: 47.758% ,top5: 89.983%\n",
      "Loss: 1.469 | top1: 47.858% ,top5: 90.026%\n",
      "Loss: 1.466 | top1: 47.969% ,top5: 90.094%\n",
      "Loss: 1.464 | top1: 48.065% ,top5: 90.129%\n",
      "Loss: 1.461 | top1: 48.185% ,top5: 90.164%\n",
      "Loss: 1.459 | top1: 48.260% ,top5: 90.210%\n",
      "Loss: 1.457 | top1: 48.341% ,top5: 90.274%\n",
      "Loss: 1.454 | top1: 48.438% ,top5: 90.343%\n",
      "Loss: 1.452 | top1: 48.497% ,top5: 90.380%\n",
      "Loss: 1.452 | top1: 48.562% ,top5: 90.376%\n",
      "Loss: 1.449 | top1: 48.684% ,top5: 90.437%\n",
      "Loss: 1.446 | top1: 48.811% ,top5: 90.462%\n",
      "Loss: 1.443 | top1: 48.941% ,top5: 90.521%\n",
      "Loss: 1.440 | top1: 49.029% ,top5: 90.568%\n",
      "Loss: 1.438 | top1: 49.110% ,top5: 90.602%\n",
      "Loss: 1.437 | top1: 49.134% ,top5: 90.608%\n",
      "Loss: 1.434 | top1: 49.241% ,top5: 90.631%\n",
      "Loss: 1.432 | top1: 49.347% ,top5: 90.664%\n",
      "Loss: 1.429 | top1: 49.463% ,top5: 90.691%\n",
      "Loss: 1.426 | top1: 49.582% ,top5: 90.735%\n",
      "Loss: 1.424 | top1: 49.661% ,top5: 90.778%\n",
      "Loss: 1.421 | top1: 49.750% ,top5: 90.815%\n",
      "Loss: 1.420 | top1: 49.801% ,top5: 90.862%\n",
      "Loss: 1.416 | top1: 49.946% ,top5: 90.914%\n",
      "Loss: 1.415 | top1: 49.973% ,top5: 90.939%\n",
      "Loss: 1.412 | top1: 50.058% ,top5: 90.989%\n",
      "Loss: 1.409 | top1: 50.173% ,top5: 91.018%\n",
      "Loss: 1.408 | top1: 50.198% ,top5: 91.062%\n",
      "Loss: 1.405 | top1: 50.290% ,top5: 91.096%\n",
      "Loss: 1.402 | top1: 50.401% ,top5: 91.129%\n",
      "Loss: 1.398 | top1: 50.516% ,top5: 91.176%\n",
      "Loss: 1.397 | top1: 50.578% ,top5: 91.193%\n",
      "Loss: 1.396 | top1: 50.635% ,top5: 91.230%\n",
      "Loss: 1.394 | top1: 50.711% ,top5: 91.271%\n",
      "Loss: 1.393 | top1: 50.727% ,top5: 91.307%\n",
      "Loss: 1.392 | top1: 50.781% ,top5: 91.317%\n",
      "Loss: 1.390 | top1: 50.865% ,top5: 91.323%\n",
      "Loss: 1.387 | top1: 50.952% ,top5: 91.353%\n",
      "Loss: 1.385 | top1: 51.019% ,top5: 91.392%\n",
      "Loss: 1.384 | top1: 51.100% ,top5: 91.416%\n",
      "Loss: 1.382 | top1: 51.189% ,top5: 91.459%\n",
      "Loss: 1.380 | top1: 51.258% ,top5: 91.478%\n",
      "Loss: 1.378 | top1: 51.345% ,top5: 91.515%\n",
      "Loss: 1.376 | top1: 51.431% ,top5: 91.538%\n",
      "Loss: 1.374 | top1: 51.497% ,top5: 91.561%\n",
      "Loss: 1.373 | top1: 51.521% ,top5: 91.592%\n",
      "Loss: 1.372 | top1: 51.599% ,top5: 91.619%\n",
      "Loss: 1.370 | top1: 51.668% ,top5: 91.631%\n",
      "Loss: 1.368 | top1: 51.718% ,top5: 91.667%\n",
      "Loss: 1.367 | top1: 51.762% ,top5: 91.692%\n",
      "Loss: 1.366 | top1: 51.752% ,top5: 91.722%\n",
      "Loss: 1.364 | top1: 51.823% ,top5: 91.743%\n",
      "Loss: 1.362 | top1: 51.906% ,top5: 91.759%\n",
      "Loss: 1.362 | top1: 51.940% ,top5: 91.775%\n",
      "Loss: 1.360 | top1: 52.013% ,top5: 91.795%\n",
      "Loss: 1.358 | top1: 52.076% ,top5: 91.828%\n",
      "Loss: 1.355 | top1: 52.160% ,top5: 91.860%\n",
      "Loss: 1.353 | top1: 52.248% ,top5: 91.888%\n",
      "Loss: 1.352 | top1: 52.322% ,top5: 91.898%\n",
      "Loss: 1.350 | top1: 52.348% ,top5: 91.921%\n",
      "Loss: 1.348 | top1: 52.438% ,top5: 91.944%\n",
      "Loss: 1.348 | top1: 52.446% ,top5: 91.958%\n",
      "Loss: 1.347 | top1: 52.517% ,top5: 91.981%\n",
      "Loss: 1.344 | top1: 52.592% ,top5: 92.015%\n",
      "Loss: 1.343 | top1: 52.632% ,top5: 92.045%\n",
      "Loss: 1.341 | top1: 52.693% ,top5: 92.079%\n",
      "Loss: 1.339 | top1: 52.774% ,top5: 92.113%\n",
      "Loss: 1.337 | top1: 52.862% ,top5: 92.134%\n",
      "Loss: 1.335 | top1: 52.925% ,top5: 92.155%\n",
      "Loss: 1.334 | top1: 52.999% ,top5: 92.175%\n",
      "Loss: 1.332 | top1: 53.068% ,top5: 92.204%\n",
      "Loss: 1.331 | top1: 53.117% ,top5: 92.228%\n",
      "Loss: 1.329 | top1: 53.197% ,top5: 92.256%\n",
      "Loss: 1.327 | top1: 53.276% ,top5: 92.279%\n",
      "Loss: 1.325 | top1: 53.359% ,top5: 92.287%\n",
      "Loss: 1.322 | top1: 53.441% ,top5: 92.310%\n",
      "Loss: 1.321 | top1: 53.494% ,top5: 92.325%\n",
      "Loss: 1.319 | top1: 53.543% ,top5: 92.348%\n",
      "Loss: 1.318 | top1: 53.595% ,top5: 92.370%\n",
      "Loss: 1.317 | top1: 53.639% ,top5: 92.404%\n",
      "Loss: 1.315 | top1: 53.698% ,top5: 92.434%\n",
      "Loss: 1.314 | top1: 53.730% ,top5: 92.456%\n",
      "Loss: 1.313 | top1: 53.773% ,top5: 92.462%\n",
      "Loss: 1.312 | top1: 53.815% ,top5: 92.476%\n",
      "Loss: 1.310 | top1: 53.872% ,top5: 92.505%\n",
      "Loss: 1.308 | top1: 53.948% ,top5: 92.529%\n",
      "Loss: 1.306 | top1: 54.011% ,top5: 92.561%\n",
      "Loss: 1.306 | top1: 54.040% ,top5: 92.582%\n",
      "Loss: 1.304 | top1: 54.084% ,top5: 92.591%\n",
      "Loss: 1.303 | top1: 54.149% ,top5: 92.615%\n",
      "Loss: 1.301 | top1: 54.192% ,top5: 92.642%\n",
      "Loss: 1.300 | top1: 54.260% ,top5: 92.662%\n",
      "Loss: 1.298 | top1: 54.306% ,top5: 92.685%\n",
      "Loss: 1.296 | top1: 54.387% ,top5: 92.697%\n",
      "Loss: 1.294 | top1: 54.471% ,top5: 92.717%\n",
      "Loss: 1.293 | top1: 54.519% ,top5: 92.732%\n",
      "Loss: 1.291 | top1: 54.584% ,top5: 92.755%\n",
      "Loss: 1.290 | top1: 54.599% ,top5: 92.777%\n",
      "Loss: 1.288 | top1: 54.670% ,top5: 92.788%\n",
      "Loss: 1.287 | top1: 54.716% ,top5: 92.810%\n",
      "Loss: 1.286 | top1: 54.754% ,top5: 92.825%\n",
      "Loss: 1.285 | top1: 54.782% ,top5: 92.850%\n",
      "Loss: 1.283 | top1: 54.833% ,top5: 92.872%\n",
      "Loss: 1.282 | top1: 54.885% ,top5: 92.886%\n",
      "Loss: 1.280 | top1: 54.935% ,top5: 92.910%\n",
      "Loss: 1.279 | top1: 54.989% ,top5: 92.924%\n",
      "Loss: 1.278 | top1: 55.035% ,top5: 92.928%\n",
      "Loss: 1.277 | top1: 55.048% ,top5: 92.945%\n",
      "Loss: 1.275 | top1: 55.103% ,top5: 92.959%\n",
      "Loss: 1.274 | top1: 55.166% ,top5: 92.975%\n",
      "Loss: 1.273 | top1: 55.197% ,top5: 92.999%\n",
      "Loss: 1.271 | top1: 55.282% ,top5: 93.022%\n",
      "Loss: 1.270 | top1: 55.309% ,top5: 93.042%\n",
      "Loss: 1.269 | top1: 55.313% ,top5: 93.068%\n",
      "Loss: 1.268 | top1: 55.373% ,top5: 93.084%\n",
      "Loss: 1.267 | top1: 55.390% ,top5: 93.084%\n",
      "Loss: 1.266 | top1: 55.416% ,top5: 93.090%\n",
      "Loss: 1.264 | top1: 55.485% ,top5: 93.118%\n",
      "Loss: 1.263 | top1: 55.543% ,top5: 93.137%\n",
      "Loss: 1.261 | top1: 55.604% ,top5: 93.150%\n",
      "Loss: 1.261 | top1: 55.617% ,top5: 93.168%\n",
      "Loss: 1.259 | top1: 55.677% ,top5: 93.190%\n",
      "Loss: 1.257 | top1: 55.730% ,top5: 93.211%\n",
      "Loss: 1.256 | top1: 55.793% ,top5: 93.232%\n",
      "Loss: 1.255 | top1: 55.807% ,top5: 93.253%\n",
      "Loss: 1.253 | top1: 55.881% ,top5: 93.262%\n",
      "Loss: 1.252 | top1: 55.933% ,top5: 93.279%\n",
      "Loss: 1.250 | top1: 55.969% ,top5: 93.300%\n",
      "Loss: 1.249 | top1: 56.017% ,top5: 93.324%\n",
      "Loss: 1.247 | top1: 56.061% ,top5: 93.341%\n",
      "Loss: 1.246 | top1: 56.089% ,top5: 93.367%\n",
      "Loss: 1.245 | top1: 56.152% ,top5: 93.387%\n",
      "Loss: 1.243 | top1: 56.201% ,top5: 93.410%\n",
      "Loss: 1.241 | top1: 56.265% ,top5: 93.433%\n",
      "Loss: 1.240 | top1: 56.323% ,top5: 93.449%\n",
      "Loss: 1.239 | top1: 56.353% ,top5: 93.453%\n",
      "Loss: 1.238 | top1: 56.413% ,top5: 93.476%\n",
      "Loss: 1.237 | top1: 56.427% ,top5: 93.492%\n",
      "Loss: 1.236 | top1: 56.472% ,top5: 93.499%\n",
      "Loss: 1.234 | top1: 56.533% ,top5: 93.520%\n",
      "Loss: 1.232 | top1: 56.568% ,top5: 93.539%\n",
      "Loss: 1.232 | top1: 56.590% ,top5: 93.549%\n",
      "Loss: 1.230 | top1: 56.657% ,top5: 93.567%\n",
      "Loss: 1.229 | top1: 56.708% ,top5: 93.588%\n",
      "Loss: 1.228 | top1: 56.750% ,top5: 93.601%\n",
      "Loss: 1.226 | top1: 56.798% ,top5: 93.622%\n",
      "Loss: 1.225 | top1: 56.842% ,top5: 93.634%\n",
      "Loss: 1.224 | top1: 56.869% ,top5: 93.649%\n",
      "Loss: 1.223 | top1: 56.913% ,top5: 93.652%\n",
      "Loss: 1.223 | top1: 56.939% ,top5: 93.664%\n",
      "Loss: 1.221 | top1: 56.974% ,top5: 93.673%\n",
      "Loss: 1.220 | top1: 57.026% ,top5: 93.684%\n",
      "Loss: 1.219 | top1: 57.060% ,top5: 93.690%\n",
      "Loss: 1.218 | top1: 57.094% ,top5: 93.705%\n",
      "Loss: 1.216 | top1: 57.147% ,top5: 93.719%\n",
      "Loss: 1.215 | top1: 57.194% ,top5: 93.733%\n",
      "Loss: 1.214 | top1: 57.241% ,top5: 93.744%\n",
      "Loss: 1.212 | top1: 57.288% ,top5: 93.764%\n",
      "Loss: 1.211 | top1: 57.320% ,top5: 93.781%\n",
      "Loss: 1.211 | top1: 57.333% ,top5: 93.783%\n",
      "Loss: 1.209 | top1: 57.393% ,top5: 93.802%\n",
      "Loss: 1.207 | top1: 57.469% ,top5: 93.824%\n",
      "Loss: 1.206 | top1: 57.505% ,top5: 93.843%\n",
      "Loss: 1.204 | top1: 57.572% ,top5: 93.859%\n",
      "Loss: 1.203 | top1: 57.608% ,top5: 93.872%\n",
      "Loss: 1.202 | top1: 57.625% ,top5: 93.883%\n",
      "Loss: 1.201 | top1: 57.669% ,top5: 93.891%\n",
      "Loss: 1.200 | top1: 57.702% ,top5: 93.898%\n",
      "Loss: 1.199 | top1: 57.735% ,top5: 93.914%\n",
      "Loss: 1.198 | top1: 57.788% ,top5: 93.932%\n",
      "Loss: 1.197 | top1: 57.820% ,top5: 93.947%\n",
      "Loss: 1.196 | top1: 57.855% ,top5: 93.963%\n",
      "Loss: 1.194 | top1: 57.908% ,top5: 93.978%\n",
      "Loss: 1.194 | top1: 57.923% ,top5: 93.988%\n",
      "Loss: 1.193 | top1: 57.973% ,top5: 93.987%\n",
      "Loss: 1.191 | top1: 58.038% ,top5: 93.999%\n",
      "Loss: 1.190 | top1: 58.076% ,top5: 94.011%\n",
      "Loss: 1.189 | top1: 58.107% ,top5: 94.016%\n",
      "Loss: 1.187 | top1: 58.163% ,top5: 94.028%\n",
      "Loss: 1.187 | top1: 58.188% ,top5: 94.035%\n",
      "Loss: 1.185 | top1: 58.251% ,top5: 94.044%\n",
      "Loss: 1.185 | top1: 58.285% ,top5: 94.053%\n",
      "Loss: 1.184 | top1: 58.322% ,top5: 94.060%\n",
      "Loss: 1.183 | top1: 58.351% ,top5: 94.074%\n",
      "Loss: 1.181 | top1: 58.403% ,top5: 94.088%\n",
      "Loss: 1.180 | top1: 58.457% ,top5: 94.103%\n",
      "Loss: 1.179 | top1: 58.503% ,top5: 94.114%\n",
      "Loss: 1.179 | top1: 58.523% ,top5: 94.131%\n",
      "Loss: 1.177 | top1: 58.574% ,top5: 94.147%\n",
      "Loss: 1.177 | top1: 58.586% ,top5: 94.161%\n",
      "Loss: 1.176 | top1: 58.614% ,top5: 94.167%\n",
      "Loss: 1.175 | top1: 58.644% ,top5: 94.180%\n",
      "Loss: 1.174 | top1: 58.681% ,top5: 94.189%\n",
      "Loss: 1.173 | top1: 58.717% ,top5: 94.202%\n",
      "Loss: 1.172 | top1: 58.747% ,top5: 94.211%\n",
      "Loss: 1.171 | top1: 58.778% ,top5: 94.214%\n",
      "Loss: 1.169 | top1: 58.861% ,top5: 94.230%\n",
      "Loss: 1.168 | top1: 58.911% ,top5: 94.236%\n",
      "Loss: 1.168 | top1: 58.922% ,top5: 94.237%\n",
      "Loss: 1.167 | top1: 58.963% ,top5: 94.245%\n",
      "Loss: 1.166 | top1: 58.998% ,top5: 94.256%\n",
      "Loss: 1.165 | top1: 59.035% ,top5: 94.264%\n",
      "Loss: 1.164 | top1: 59.060% ,top5: 94.272%\n",
      "Loss: 1.163 | top1: 59.104% ,top5: 94.277%\n",
      "Loss: 1.162 | top1: 59.134% ,top5: 94.283%\n",
      "Loss: 1.161 | top1: 59.165% ,top5: 94.293%\n",
      "Loss: 1.160 | top1: 59.204% ,top5: 94.299%\n",
      "Loss: 1.159 | top1: 59.254% ,top5: 94.316%\n",
      "Loss: 1.158 | top1: 59.309% ,top5: 94.328%\n",
      "Loss: 1.157 | top1: 59.340% ,top5: 94.334%\n",
      "Loss: 1.156 | top1: 59.382% ,top5: 94.339%\n",
      "Loss: 1.155 | top1: 59.398% ,top5: 94.349%\n",
      "Loss: 1.154 | top1: 59.445% ,top5: 94.356%\n",
      "Loss: 1.153 | top1: 59.491% ,top5: 94.368%\n",
      "Loss: 1.152 | top1: 59.530% ,top5: 94.374%\n",
      "Loss: 1.151 | top1: 59.569% ,top5: 94.381%\n",
      "Loss: 1.151 | top1: 59.605% ,top5: 94.395%\n",
      "Loss: 1.149 | top1: 59.642% ,top5: 94.407%\n",
      "Loss: 1.149 | top1: 59.659% ,top5: 94.419%\n",
      "Loss: 1.148 | top1: 59.686% ,top5: 94.428%\n",
      "Loss: 1.147 | top1: 59.712% ,top5: 94.440%\n",
      "Loss: 1.146 | top1: 59.741% ,top5: 94.445%\n",
      "Loss: 1.145 | top1: 59.783% ,top5: 94.454%\n",
      "Loss: 1.144 | top1: 59.804% ,top5: 94.468%\n",
      "Loss: 1.143 | top1: 59.846% ,top5: 94.477%\n",
      "Loss: 1.142 | top1: 59.887% ,top5: 94.477%\n",
      "Loss: 1.141 | top1: 59.930% ,top5: 94.482%\n",
      "Loss: 1.141 | top1: 59.944% ,top5: 94.491%\n",
      "Loss: 1.140 | top1: 59.978% ,top5: 94.496%\n",
      "Loss: 1.139 | top1: 60.014% ,top5: 94.507%\n",
      "Loss: 1.138 | top1: 60.035% ,top5: 94.516%\n",
      "Loss: 1.137 | top1: 60.066% ,top5: 94.527%\n",
      "Loss: 1.136 | top1: 60.110% ,top5: 94.540%\n",
      "Loss: 1.135 | top1: 60.150% ,top5: 94.549%\n",
      "Loss: 1.134 | top1: 60.174% ,top5: 94.558%\n",
      "Loss: 1.134 | top1: 60.209% ,top5: 94.564%\n",
      "Loss: 1.133 | top1: 60.243% ,top5: 94.577%\n",
      "Loss: 1.132 | top1: 60.265% ,top5: 94.588%\n",
      "Loss: 1.131 | top1: 60.327% ,top5: 94.598%\n",
      "Loss: 1.130 | top1: 60.344% ,top5: 94.611%\n",
      "Loss: 1.129 | top1: 60.369% ,top5: 94.624%\n",
      "Loss: 1.128 | top1: 60.407% ,top5: 94.636%\n",
      "Loss: 1.127 | top1: 60.430% ,top5: 94.645%\n",
      "Loss: 1.127 | top1: 60.459% ,top5: 94.657%\n",
      "Loss: 1.126 | top1: 60.490% ,top5: 94.670%\n",
      "Loss: 1.125 | top1: 60.524% ,top5: 94.682%\n",
      "Loss: 1.124 | top1: 60.563% ,top5: 94.692%\n",
      "Loss: 1.123 | top1: 60.587% ,top5: 94.694%\n",
      "Loss: 1.122 | top1: 60.628% ,top5: 94.704%\n",
      "Loss: 1.121 | top1: 60.671% ,top5: 94.710%\n",
      "Loss: 1.121 | top1: 60.695% ,top5: 94.713%\n",
      "Loss: 1.120 | top1: 60.737% ,top5: 94.723%\n",
      "Loss: 1.119 | top1: 60.773% ,top5: 94.737%\n",
      "Loss: 1.118 | top1: 60.802% ,top5: 94.747%\n",
      "Loss: 1.117 | top1: 60.830% ,top5: 94.757%\n",
      "Loss: 1.116 | top1: 60.873% ,top5: 94.769%\n",
      "Loss: 1.115 | top1: 60.905% ,top5: 94.781%\n",
      "Loss: 1.115 | top1: 60.911% ,top5: 94.788%\n",
      "Loss: 1.114 | top1: 60.940% ,top5: 94.796%\n",
      "Loss: 1.113 | top1: 60.982% ,top5: 94.807%\n",
      "Loss: 1.113 | top1: 61.013% ,top5: 94.813%\n",
      "Loss: 1.112 | top1: 61.035% ,top5: 94.818%\n",
      "Loss: 1.111 | top1: 61.053% ,top5: 94.825%\n",
      "Loss: 1.111 | top1: 61.061% ,top5: 94.829%\n",
      "Loss: 1.110 | top1: 61.097% ,top5: 94.842%\n",
      "Loss: 1.110 | top1: 61.131% ,top5: 94.847%\n",
      "Loss: 1.109 | top1: 61.164% ,top5: 94.855%\n",
      "Loss: 1.108 | top1: 61.200% ,top5: 94.864%\n",
      "Loss: 1.107 | top1: 61.214% ,top5: 94.872%\n",
      "Loss: 1.107 | top1: 61.214%, top5: 94.872% elasped time: 125 seconds.\n",
      "Testing at epoch 1\n",
      "Loss: 0.678 | top1: 71.094% ,top5: 97.656%\n",
      "Loss: 0.659 | top1: 74.219% ,top5: 98.047%\n",
      "Loss: 0.677 | top1: 73.438% ,top5: 98.438%\n",
      "Loss: 0.705 | top1: 73.242% ,top5: 98.438%\n",
      "Loss: 0.733 | top1: 72.031% ,top5: 98.438%\n",
      "Loss: 0.748 | top1: 71.615% ,top5: 98.307%\n",
      "Loss: 0.749 | top1: 71.763% ,top5: 98.326%\n",
      "Loss: 0.751 | top1: 72.266% ,top5: 98.340%\n",
      "Loss: 0.761 | top1: 72.049% ,top5: 98.351%\n",
      "Loss: 0.755 | top1: 72.500% ,top5: 98.438%\n",
      "Loss: 0.740 | top1: 73.366% ,top5: 98.509%\n",
      "Loss: 0.731 | top1: 74.023% ,top5: 98.568%\n",
      "Loss: 0.726 | top1: 73.978% ,top5: 98.558%\n",
      "Loss: 0.738 | top1: 73.717% ,top5: 98.270%\n",
      "Loss: 0.740 | top1: 73.750% ,top5: 98.229%\n",
      "Loss: 0.746 | top1: 73.779% ,top5: 98.242%\n",
      "Loss: 0.748 | top1: 73.621% ,top5: 98.300%\n",
      "Loss: 0.743 | top1: 73.785% ,top5: 98.394%\n",
      "Loss: 0.743 | top1: 73.931% ,top5: 98.273%\n",
      "Loss: 0.738 | top1: 74.336% ,top5: 98.203%\n",
      "Loss: 0.729 | top1: 74.591% ,top5: 98.289%\n",
      "Loss: 0.729 | top1: 74.751% ,top5: 98.295%\n",
      "Loss: 0.722 | top1: 74.932% ,top5: 98.370%\n",
      "Loss: 0.723 | top1: 74.935% ,top5: 98.372%\n",
      "Loss: 0.723 | top1: 75.000% ,top5: 98.375%\n",
      "Loss: 0.723 | top1: 74.880% ,top5: 98.377%\n",
      "Loss: 0.725 | top1: 74.682% ,top5: 98.409%\n",
      "Loss: 0.726 | top1: 74.581% ,top5: 98.410%\n",
      "Loss: 0.725 | top1: 74.596% ,top5: 98.411%\n",
      "Loss: 0.726 | top1: 74.531% ,top5: 98.438%\n",
      "Loss: 0.722 | top1: 74.773% ,top5: 98.488%\n",
      "Loss: 0.722 | top1: 74.683% ,top5: 98.511%\n",
      "Loss: 0.729 | top1: 74.455% ,top5: 98.414%\n",
      "Loss: 0.726 | top1: 74.540% ,top5: 98.415%\n",
      "Loss: 0.724 | top1: 74.688% ,top5: 98.415%\n",
      "Loss: 0.721 | top1: 74.848% ,top5: 98.438%\n",
      "Loss: 0.720 | top1: 74.768% ,top5: 98.459%\n",
      "Loss: 0.720 | top1: 74.794% ,top5: 98.458%\n",
      "Loss: 0.722 | top1: 74.679% ,top5: 98.478%\n",
      "Loss: 0.722 | top1: 74.707% ,top5: 98.477%\n",
      "Loss: 0.723 | top1: 74.752% ,top5: 98.495%\n",
      "Loss: 0.727 | top1: 74.628% ,top5: 98.438%\n",
      "Loss: 0.728 | top1: 74.491% ,top5: 98.383%\n",
      "Loss: 0.733 | top1: 74.254% ,top5: 98.349%\n",
      "Loss: 0.734 | top1: 74.201% ,top5: 98.368%\n",
      "Loss: 0.738 | top1: 73.998% ,top5: 98.336%\n",
      "Loss: 0.739 | top1: 73.936% ,top5: 98.371%\n",
      "Loss: 0.741 | top1: 73.910% ,top5: 98.324%\n",
      "Loss: 0.742 | top1: 73.948% ,top5: 98.310%\n",
      "Loss: 0.742 | top1: 73.938% ,top5: 98.328%\n",
      "Loss: 0.740 | top1: 73.989% ,top5: 98.315%\n",
      "Loss: 0.743 | top1: 73.933% ,top5: 98.302%\n",
      "Loss: 0.742 | top1: 74.042% ,top5: 98.275%\n",
      "Loss: 0.744 | top1: 74.016% ,top5: 98.249%\n",
      "Loss: 0.744 | top1: 73.977% ,top5: 98.253%\n",
      "Loss: 0.740 | top1: 74.093% ,top5: 98.284%\n",
      "Loss: 0.739 | top1: 74.109% ,top5: 98.300%\n",
      "Loss: 0.738 | top1: 74.165% ,top5: 98.316%\n",
      "Loss: 0.735 | top1: 74.272% ,top5: 98.332%\n",
      "Loss: 0.737 | top1: 74.232% ,top5: 98.307%\n",
      "Loss: 0.737 | top1: 74.232% ,top5: 98.322%\n",
      "Loss: 0.737 | top1: 74.257% ,top5: 98.311%\n",
      "Loss: 0.736 | top1: 74.380% ,top5: 98.313%\n",
      "Loss: 0.736 | top1: 74.341% ,top5: 98.328%\n",
      "Loss: 0.734 | top1: 74.411% ,top5: 98.341%\n",
      "Loss: 0.733 | top1: 74.396% ,top5: 98.343%\n",
      "Loss: 0.735 | top1: 74.300% ,top5: 98.321%\n",
      "Loss: 0.734 | top1: 74.311% ,top5: 98.334%\n",
      "Loss: 0.732 | top1: 74.377% ,top5: 98.336%\n",
      "Loss: 0.731 | top1: 74.453% ,top5: 98.348%\n",
      "Loss: 0.730 | top1: 74.505% ,top5: 98.338%\n",
      "Loss: 0.731 | top1: 74.468% ,top5: 98.329%\n",
      "Loss: 0.732 | top1: 74.390% ,top5: 98.309%\n",
      "Loss: 0.731 | top1: 74.398% ,top5: 98.279%\n",
      "Loss: 0.732 | top1: 74.385% ,top5: 98.271%\n",
      "Loss: 0.732 | top1: 74.373% ,top5: 98.273%\n",
      "Loss: 0.732 | top1: 74.401% ,top5: 98.275%\n",
      "Loss: 0.733 | top1: 74.399% ,top5: 98.267%\n",
      "Loss: 0.732 | top1: 74.410% ,top5: 98.260%\n",
      "Loss: 0.732 | top1: 74.410%, top5: 98.260%, elasped time:   9 seconds. Best Acc: 74.410%\n",
      "Training finished successfully. Model size:  14719818\n",
      "Best acc:  74.41\n"
     ]
    }
   ],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch vgg16 --epochs 1 --dataset cifar10  --batch-size 128 --msg True --deconv True --block-fc 512 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02001197-76b0-4b80-8ba3-2da2d1b86064",
   "metadata": {},
   "source": [
    "# ResNet-18 --- resnet18d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ad3c7-61b1-4125-9518-7b88c30f41c7",
   "metadata": {},
   "source": [
    "#### batch normalization --- resnet18d\n",
    "\n",
    "issue resolved -- ```No module named 'torchvision.models.utils'```\n",
    "\n",
    "https://stackoverflow.com/questions/70998767/no-module-named-torchvision-models-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79720e1e-ea95-464f-9ff6-f9a4e1fecf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch resnet18d --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv False --block-fc 0 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f25576-e835-4474-9f74-43c8031e3307",
   "metadata": {},
   "source": [
    "#### network deconvolution --- resnet18d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451b4cd-0512-436d-bc9e-b8ea29a11526",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch resnet18d --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv True --block-fc 512 --wd .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad46bd38-ea19-44ff-8b1d-d3a7e1e94b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=0 python main.py --lr .1 --optimizer SGD --arch resnet --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv True --block-fc 512 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba9ad3-0dc5-4f06-a78e-a4434b5ef98c",
   "metadata": {},
   "source": [
    "# Preact-18 --- preact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1e987-70d2-4c82-9c28-ed5cb61fb237",
   "metadata": {},
   "source": [
    "#### batch normalization --- preact\n",
    "\n",
    "```Traceback (most recent call last):```\r\n",
    "  File \"/home/oruma001/CS895/project/codebase/main.py\", line 271, in <module>\r\n",
    "    net = PreActResNet18(num_classes=args.num_outputs,deconv=args.deconv,delinear=args.delinear,channel_deconv=args.channel_deconv)\r\n",
    "NameError: name 'PreActResNet18' is not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec032472-a283-4d73-bc2e-02bead5ff995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "| Preparing CIFAR-10 dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "2024-03-14 20:28:07.261707: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "==> Building model..\n",
      "Namespace(msg=1, resume='', use_gpu=True, num_workers=16, result_path='checkpoints/cifar10,preact,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-14-20.28', checkpoint_path='checkpoints/cifar10,preact,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-14-20.28', checkpoint_epoch=-1, print_freq=20, seed=0, optimizer='SGD', lr=0.1, lr_scheduler='cosine', momentum=0.9, weight_decay=0.001, batch_size=128, epochs=1, milestone=0.4, multistep_gamma=0.1, arch='preact', dataset='cifar10', init='kaiming_1', save_plot=True, tensorboard=True, loss='CE', method=3, batchnorm=True, deconv=None, delinear=None, block_fc=0, block=64, deconv_iter=5, eps=1e-05, bias=True, stride=3, freeze=False, log_path='checkpoints/cifar10,preact,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-14-20.28', channel_deconv=None, start_epoch=0, in_planes=3, input_size=32, classes=('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'), num_outputs=10, train_epoch_logger=<net_util.Logger object at 0x7fa17e73be50>, train_batch_logger=<net_util.Logger object at 0x7fa17e73beb0>, test_epoch_logger=<net_util.Logger object at 0x7fa17e73bfa0>, writer=<torch.utils.tensorboard.writer.SummaryWriter object at 0x7fa17e7840d0>, criterion=CrossEntropyLoss(), logger_n_iter=0)\n",
      "21279434 trainable parameters in the network.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Current learning rate: 0.1\n",
      "training at epoch 1\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Loss: 2.282 | top1: 7.812% ,top5: 57.031%\n",
      "Loss: 2.906 | top1: 7.422% ,top5: 51.172%\n",
      "Loss: 3.286 | top1: 8.333% ,top5: 54.688%\n",
      "Loss: 4.610 | top1: 9.180% ,top5: 54.102%\n",
      "Loss: 5.070 | top1: 11.094% ,top5: 55.000%\n",
      "Loss: 6.209 | top1: 11.068% ,top5: 54.557%\n",
      "Loss: 8.653 | top1: 10.714% ,top5: 53.125%\n",
      "Loss: 8.917 | top1: 10.547% ,top5: 51.855%\n",
      "Loss: 10.357 | top1: 10.764% ,top5: 51.562%\n",
      "Loss: 11.221 | top1: 10.859% ,top5: 51.953%\n",
      "Loss: 16.753 | top1: 10.724% ,top5: 51.491%\n",
      "Loss: 17.119 | top1: 10.612% ,top5: 51.367%\n",
      "Loss: 17.466 | top1: 10.697% ,top5: 51.262%\n",
      "Loss: 17.122 | top1: 10.714% ,top5: 51.228%\n",
      "Loss: 18.438 | top1: 10.781% ,top5: 50.885%\n",
      "Loss: 19.495 | top1: 10.742% ,top5: 50.684%\n",
      "Loss: 19.300 | top1: 10.662% ,top5: 50.643%\n",
      "Loss: 19.478 | top1: 10.590% ,top5: 50.825%\n",
      "Loss: 22.389 | top1: 10.609% ,top5: 51.110%\n",
      "Loss: 23.074 | top1: 10.859% ,top5: 51.055%\n",
      "Loss: 23.961 | top1: 10.789% ,top5: 51.004%\n",
      "Loss: 25.180 | top1: 10.724% ,top5: 50.994%\n",
      "Loss: 24.845 | top1: 10.632% ,top5: 50.713%\n",
      "Loss: 24.687 | top1: 10.645% ,top5: 50.456%\n",
      "Loss: 24.086 | top1: 10.531% ,top5: 50.500%\n",
      "Loss: 23.484 | top1: 10.367% ,top5: 50.331%\n",
      "Loss: 22.818 | top1: 10.446% ,top5: 50.579%\n",
      "Loss: 22.249 | top1: 10.352% ,top5: 50.725%\n",
      "Loss: 21.665 | top1: 10.291% ,top5: 50.889%\n",
      "Loss: 21.592 | top1: 10.260% ,top5: 50.755%\n",
      "Loss: 21.834 | top1: 10.181% ,top5: 50.504%\n",
      "Loss: 21.759 | top1: 10.303% ,top5: 50.610%\n",
      "Loss: 21.355 | top1: 10.511% ,top5: 50.994%\n",
      "Loss: 21.645 | top1: 10.593% ,top5: 51.057%\n",
      "Loss: 21.545 | top1: 10.580% ,top5: 51.161%\n",
      "Loss: 21.216 | top1: 10.460% ,top5: 51.020%\n",
      "Loss: 20.838 | top1: 10.346% ,top5: 50.823%\n",
      "Loss: 20.409 | top1: 10.424% ,top5: 50.781%\n",
      "Loss: 20.169 | top1: 10.457% ,top5: 50.901%\n",
      "Loss: 19.793 | top1: 10.371% ,top5: 50.762%\n",
      "Loss: 19.435 | top1: 10.385% ,top5: 50.610%\n",
      "Loss: 19.099 | top1: 10.305% ,top5: 50.781%\n",
      "Loss: 18.719 | top1: 10.392% ,top5: 50.799%\n",
      "Loss: 18.349 | top1: 10.405% ,top5: 50.835%\n",
      "Loss: 18.110 | top1: 10.469% ,top5: 50.920%\n",
      "Loss: 17.808 | top1: 10.496% ,top5: 51.036%\n",
      "Loss: 17.640 | top1: 10.505% ,top5: 51.164%\n",
      "Loss: 17.372 | top1: 10.531% ,top5: 51.139%\n",
      "Loss: 17.214 | top1: 10.555% ,top5: 51.244%\n",
      "Loss: 16.968 | top1: 10.484% ,top5: 51.141%\n",
      "Loss: 16.693 | top1: 10.555% ,top5: 51.180%\n",
      "Loss: 16.631 | top1: 10.532% ,top5: 51.127%\n",
      "Loss: 16.533 | top1: 10.451% ,top5: 51.032%\n",
      "Loss: 16.312 | top1: 10.503% ,top5: 51.100%\n",
      "Loss: 16.098 | top1: 10.455% ,top5: 51.065%\n",
      "Loss: 15.869 | top1: 10.449% ,top5: 51.018%\n",
      "Loss: 15.632 | top1: 10.471% ,top5: 50.959%\n",
      "Loss: 15.464 | top1: 10.453% ,top5: 50.889%\n",
      "Loss: 15.243 | top1: 10.448% ,top5: 50.861%\n",
      "Loss: 15.064 | top1: 10.508% ,top5: 50.794%\n",
      "Loss: 14.867 | top1: 10.540% ,top5: 50.730%\n",
      "Loss: 14.676 | top1: 10.559% ,top5: 50.794%\n",
      "Loss: 14.479 | top1: 10.503% ,top5: 50.756%\n",
      "Loss: 14.290 | top1: 10.486% ,top5: 50.720%\n",
      "Loss: 14.109 | top1: 10.541% ,top5: 50.745%\n",
      "Loss: 13.942 | top1: 10.559% ,top5: 50.746%\n",
      "Loss: 13.776 | top1: 10.553% ,top5: 50.665%\n",
      "Loss: 13.613 | top1: 10.593% ,top5: 50.574%\n",
      "Loss: 13.457 | top1: 10.609% ,top5: 50.645%\n",
      "Loss: 13.300 | top1: 10.636% ,top5: 50.703%\n",
      "Loss: 13.148 | top1: 10.618% ,top5: 50.715%\n",
      "Loss: 12.998 | top1: 10.699% ,top5: 50.673%\n",
      "Loss: 12.851 | top1: 10.691% ,top5: 50.621%\n",
      "Loss: 12.710 | top1: 10.695% ,top5: 50.644%\n",
      "Loss: 12.577 | top1: 10.708% ,top5: 50.573%\n",
      "Loss: 12.458 | top1: 10.670% ,top5: 50.555%\n",
      "Loss: 12.336 | top1: 10.633% ,top5: 50.457%\n",
      "Loss: 12.210 | top1: 10.657% ,top5: 50.461%\n",
      "Loss: 12.098 | top1: 10.740% ,top5: 50.465%\n",
      "Loss: 11.985 | top1: 10.742% ,top5: 50.391%\n",
      "Loss: 11.868 | top1: 10.725% ,top5: 50.444%\n",
      "Loss: 11.757 | top1: 10.776% ,top5: 50.562%\n",
      "Loss: 11.645 | top1: 10.721% ,top5: 50.480%\n",
      "Loss: 11.536 | top1: 10.677% ,top5: 50.400%\n",
      "Loss: 11.436 | top1: 10.634% ,top5: 50.441%\n",
      "Loss: 11.340 | top1: 10.583% ,top5: 50.454%\n",
      "Loss: 11.255 | top1: 10.596% ,top5: 50.431%\n",
      "Loss: 11.190 | top1: 10.574% ,top5: 50.408%\n",
      "Loss: 11.095 | top1: 10.542% ,top5: 50.386%\n",
      "Loss: 11.012 | top1: 10.521% ,top5: 50.321%\n",
      "Loss: 10.950 | top1: 10.482% ,top5: 50.292%\n",
      "Loss: 10.939 | top1: 10.445% ,top5: 50.246%\n",
      "Loss: 10.911 | top1: 10.467% ,top5: 50.218%\n",
      "Loss: 10.844 | top1: 10.464% ,top5: 50.183%\n",
      "Loss: 10.776 | top1: 10.461% ,top5: 50.164%\n",
      "Loss: 10.688 | top1: 10.449% ,top5: 50.146%\n",
      "Loss: 10.617 | top1: 10.438% ,top5: 50.161%\n",
      "Loss: 10.562 | top1: 10.403% ,top5: 50.120%\n",
      "Loss: 10.505 | top1: 10.401% ,top5: 50.071%\n",
      "Loss: 10.440 | top1: 10.445% ,top5: 50.086%\n",
      "Loss: 10.366 | top1: 10.450% ,top5: 50.093%\n",
      "Loss: 10.287 | top1: 10.478% ,top5: 50.092%\n",
      "Loss: 10.220 | top1: 10.490% ,top5: 50.129%\n",
      "Loss: 10.163 | top1: 10.494% ,top5: 50.120%\n",
      "Loss: 10.095 | top1: 10.469% ,top5: 50.022%\n",
      "Loss: 10.026 | top1: 10.495% ,top5: 50.007%\n",
      "Loss: 9.968 | top1: 10.499% ,top5: 50.066%\n",
      "Loss: 9.915 | top1: 10.467% ,top5: 50.007%\n",
      "Loss: 9.854 | top1: 10.493% ,top5: 49.950%\n",
      "Loss: 9.792 | top1: 10.490% ,top5: 49.950%\n",
      "Loss: 9.732 | top1: 10.473% ,top5: 50.014%\n",
      "Loss: 9.667 | top1: 10.456% ,top5: 49.986%\n",
      "Loss: 9.605 | top1: 10.454% ,top5: 50.007%\n",
      "Loss: 9.546 | top1: 10.458% ,top5: 50.014%\n",
      "Loss: 9.489 | top1: 10.496% ,top5: 49.980%\n",
      "Loss: 9.432 | top1: 10.486% ,top5: 50.027%\n",
      "Loss: 9.381 | top1: 10.483% ,top5: 50.047%\n",
      "Loss: 9.330 | top1: 10.474% ,top5: 49.980%\n",
      "Loss: 9.277 | top1: 10.458% ,top5: 49.934%\n",
      "Loss: 9.227 | top1: 10.501% ,top5: 49.915%\n",
      "Loss: 9.172 | top1: 10.492% ,top5: 49.890%\n",
      "Loss: 9.119 | top1: 10.476% ,top5: 49.898%\n",
      "Loss: 9.069 | top1: 10.461% ,top5: 49.943%\n",
      "Loss: 9.018 | top1: 10.465% ,top5: 50.000%\n",
      "Loss: 8.977 | top1: 10.456% ,top5: 50.006%\n",
      "Loss: 8.925 | top1: 10.448% ,top5: 50.025%\n",
      "Loss: 8.874 | top1: 10.470% ,top5: 50.068%\n",
      "Loss: 8.824 | top1: 10.461% ,top5: 50.043%\n",
      "Loss: 8.776 | top1: 10.447% ,top5: 50.018%\n",
      "Loss: 8.726 | top1: 10.451% ,top5: 49.976%\n",
      "Loss: 8.678 | top1: 10.478% ,top5: 49.964%\n",
      "Loss: 8.633 | top1: 10.464% ,top5: 49.970%\n",
      "Loss: 8.587 | top1: 10.444% ,top5: 49.965%\n",
      "Loss: 8.541 | top1: 10.465% ,top5: 49.994%\n",
      "Loss: 8.497 | top1: 10.451% ,top5: 50.017%\n",
      "Loss: 8.452 | top1: 10.449% ,top5: 50.023%\n",
      "Loss: 8.409 | top1: 10.470% ,top5: 50.074%\n",
      "Loss: 8.366 | top1: 10.473% ,top5: 50.062%\n",
      "Loss: 8.327 | top1: 10.482% ,top5: 50.129%\n",
      "Loss: 8.285 | top1: 10.480% ,top5: 50.167%\n",
      "Loss: 8.244 | top1: 10.483% ,top5: 50.155%\n",
      "Loss: 8.204 | top1: 10.508% ,top5: 50.193%\n",
      "Loss: 8.170 | top1: 10.522% ,top5: 50.235%\n",
      "Loss: 8.130 | top1: 10.514% ,top5: 50.222%\n",
      "Loss: 8.095 | top1: 10.517% ,top5: 50.286%\n",
      "Loss: 8.059 | top1: 10.499% ,top5: 50.289%\n",
      "Loss: 8.024 | top1: 10.480% ,top5: 50.276%\n",
      "Loss: 7.992 | top1: 10.494% ,top5: 50.306%\n",
      "Loss: 7.955 | top1: 10.481% ,top5: 50.262%\n",
      "Loss: 7.917 | top1: 10.516% ,top5: 50.255%\n",
      "Loss: 7.885 | top1: 10.539% ,top5: 50.238%\n",
      "Loss: 7.851 | top1: 10.578% ,top5: 50.236%\n",
      "Loss: 7.817 | top1: 10.575% ,top5: 50.199%\n",
      "Loss: 7.782 | top1: 10.547% ,top5: 50.228%\n",
      "Loss: 7.747 | top1: 10.544% ,top5: 50.212%\n",
      "Loss: 7.714 | top1: 10.527% ,top5: 50.165%\n",
      "Loss: 7.680 | top1: 10.524% ,top5: 50.194%\n",
      "Loss: 7.646 | top1: 10.497% ,top5: 50.163%\n",
      "Loss: 7.612 | top1: 10.500% ,top5: 50.162%\n",
      "Loss: 7.580 | top1: 10.508% ,top5: 50.220%\n",
      "Loss: 7.547 | top1: 10.467% ,top5: 50.233%\n",
      "Loss: 7.516 | top1: 10.455% ,top5: 50.241%\n",
      "Loss: 7.485 | top1: 10.439% ,top5: 50.206%\n",
      "Loss: 7.455 | top1: 10.418% ,top5: 50.171%\n",
      "Loss: 7.423 | top1: 10.440% ,top5: 50.147%\n",
      "Loss: 7.392 | top1: 10.443% ,top5: 50.174%\n",
      "Loss: 7.362 | top1: 10.428% ,top5: 50.173%\n",
      "Loss: 7.334 | top1: 10.417% ,top5: 50.195%\n",
      "Loss: 7.305 | top1: 10.378% ,top5: 50.148%\n",
      "Loss: 7.277 | top1: 10.381% ,top5: 50.152%\n",
      "Loss: 7.249 | top1: 10.380% ,top5: 50.132%\n",
      "Loss: 7.221 | top1: 10.392% ,top5: 50.132%\n",
      "Loss: 7.193 | top1: 10.405% ,top5: 50.135%\n",
      "Loss: 7.166 | top1: 10.412% ,top5: 50.121%\n",
      "Loss: 7.138 | top1: 10.415% ,top5: 50.138%\n",
      "Loss: 7.112 | top1: 10.414% ,top5: 50.115%\n",
      "Loss: 7.085 | top1: 10.399% ,top5: 50.115%\n",
      "Loss: 7.059 | top1: 10.406% ,top5: 50.119%\n",
      "Loss: 7.034 | top1: 10.405% ,top5: 50.127%\n",
      "Loss: 7.008 | top1: 10.412% ,top5: 50.156%\n",
      "Loss: 6.983 | top1: 10.415% ,top5: 50.155%\n",
      "Loss: 6.958 | top1: 10.410% ,top5: 50.124%\n",
      "Loss: 6.933 | top1: 10.417% ,top5: 50.137%\n",
      "Loss: 6.908 | top1: 10.415% ,top5: 50.149%\n",
      "Loss: 6.883 | top1: 10.427% ,top5: 50.165%\n",
      "Loss: 6.859 | top1: 10.421% ,top5: 50.193%\n",
      "Loss: 6.835 | top1: 10.445% ,top5: 50.213%\n",
      "Loss: 6.811 | top1: 10.435% ,top5: 50.208%\n",
      "Loss: 6.787 | top1: 10.446% ,top5: 50.244%\n",
      "Loss: 6.764 | top1: 10.473% ,top5: 50.189%\n",
      "Loss: 6.740 | top1: 10.488% ,top5: 50.180%\n",
      "Loss: 6.718 | top1: 10.486% ,top5: 50.175%\n",
      "Loss: 6.695 | top1: 10.504% ,top5: 50.182%\n",
      "Loss: 6.672 | top1: 10.507% ,top5: 50.246%\n",
      "Loss: 6.650 | top1: 10.501% ,top5: 50.236%\n",
      "Loss: 6.628 | top1: 10.511% ,top5: 50.219%\n",
      "Loss: 6.606 | top1: 10.497% ,top5: 50.226%\n",
      "Loss: 6.585 | top1: 10.472% ,top5: 50.197%\n",
      "Loss: 6.563 | top1: 10.459% ,top5: 50.192%\n",
      "Loss: 6.542 | top1: 10.465% ,top5: 50.195%\n",
      "Loss: 6.522 | top1: 10.463% ,top5: 50.194%\n",
      "Loss: 6.501 | top1: 10.450% ,top5: 50.147%\n",
      "Loss: 6.480 | top1: 10.437% ,top5: 50.158%\n",
      "Loss: 6.460 | top1: 10.447% ,top5: 50.180%\n",
      "Loss: 6.440 | top1: 10.438% ,top5: 50.191%\n",
      "Loss: 6.420 | top1: 10.471% ,top5: 50.167%\n",
      "Loss: 6.400 | top1: 10.473% ,top5: 50.151%\n",
      "Loss: 6.381 | top1: 10.479% ,top5: 50.165%\n",
      "Loss: 6.362 | top1: 10.470% ,top5: 50.209%\n",
      "Loss: 6.343 | top1: 10.450% ,top5: 50.197%\n",
      "Loss: 6.324 | top1: 10.445% ,top5: 50.152%\n",
      "Loss: 6.305 | top1: 10.433% ,top5: 50.155%\n",
      "Loss: 6.286 | top1: 10.450% ,top5: 50.187%\n",
      "Loss: 6.268 | top1: 10.459% ,top5: 50.183%\n",
      "Loss: 6.250 | top1: 10.440% ,top5: 50.142%\n",
      "Loss: 6.231 | top1: 10.428% ,top5: 50.156%\n",
      "Loss: 6.213 | top1: 10.448% ,top5: 50.166%\n",
      "Loss: 6.196 | top1: 10.443% ,top5: 50.197%\n",
      "Loss: 6.178 | top1: 10.431% ,top5: 50.186%\n",
      "Loss: 6.160 | top1: 10.419% ,top5: 50.153%\n",
      "Loss: 6.143 | top1: 10.425% ,top5: 50.159%\n",
      "Loss: 6.125 | top1: 10.410% ,top5: 50.155%\n",
      "Loss: 6.108 | top1: 10.405% ,top5: 50.179%\n",
      "Loss: 6.091 | top1: 10.407% ,top5: 50.164%\n",
      "Loss: 6.075 | top1: 10.396% ,top5: 50.160%\n",
      "Loss: 6.058 | top1: 10.381% ,top5: 50.159%\n",
      "Loss: 6.041 | top1: 10.394% ,top5: 50.169%\n",
      "Loss: 6.025 | top1: 10.376% ,top5: 50.137%\n",
      "Loss: 6.009 | top1: 10.364% ,top5: 50.119%\n",
      "Loss: 5.993 | top1: 10.363% ,top5: 50.095%\n",
      "Loss: 5.977 | top1: 10.366% ,top5: 50.101%\n",
      "Loss: 5.961 | top1: 10.379% ,top5: 50.118%\n",
      "Loss: 5.946 | top1: 10.374% ,top5: 50.107%\n",
      "Loss: 5.930 | top1: 10.387% ,top5: 50.117%\n",
      "Loss: 5.915 | top1: 10.392% ,top5: 50.113%\n",
      "Loss: 5.900 | top1: 10.421% ,top5: 50.079%\n",
      "Loss: 5.885 | top1: 10.410% ,top5: 50.053%\n",
      "Loss: 5.870 | top1: 10.409% ,top5: 50.036%\n",
      "Loss: 5.855 | top1: 10.408% ,top5: 50.042%\n",
      "Loss: 5.840 | top1: 10.407% ,top5: 50.033%\n",
      "Loss: 5.825 | top1: 10.416% ,top5: 50.000%\n",
      "Loss: 5.811 | top1: 10.402% ,top5: 49.987%\n",
      "Loss: 5.797 | top1: 10.397% ,top5: 49.974%\n",
      "Loss: 5.782 | top1: 10.403% ,top5: 49.965%\n",
      "Loss: 5.768 | top1: 10.392% ,top5: 49.952%\n",
      "Loss: 5.754 | top1: 10.407% ,top5: 49.994%\n",
      "Loss: 5.741 | top1: 10.409% ,top5: 50.038%\n",
      "Loss: 5.727 | top1: 10.408% ,top5: 50.019%\n",
      "Loss: 5.713 | top1: 10.410% ,top5: 50.044%\n",
      "Loss: 5.700 | top1: 10.428% ,top5: 50.081%\n",
      "Loss: 5.686 | top1: 10.430% ,top5: 50.096%\n",
      "Loss: 5.673 | top1: 10.417% ,top5: 50.084%\n",
      "Loss: 5.659 | top1: 10.409% ,top5: 50.083%\n",
      "Loss: 5.646 | top1: 10.408% ,top5: 50.095%\n",
      "Loss: 5.633 | top1: 10.407% ,top5: 50.064%\n",
      "Loss: 5.620 | top1: 10.410% ,top5: 50.055%\n",
      "Loss: 5.608 | top1: 10.406% ,top5: 50.049%\n",
      "Loss: 5.595 | top1: 10.429% ,top5: 50.082%\n",
      "Loss: 5.582 | top1: 10.413% ,top5: 50.054%\n",
      "Loss: 5.570 | top1: 10.391% ,top5: 50.051%\n",
      "Loss: 5.557 | top1: 10.396% ,top5: 50.096%\n",
      "Loss: 5.545 | top1: 10.410% ,top5: 50.128%\n",
      "Loss: 5.532 | top1: 10.412% ,top5: 50.154%\n",
      "Loss: 5.520 | top1: 10.411% ,top5: 50.127%\n",
      "Loss: 5.508 | top1: 10.419% ,top5: 50.147%\n",
      "Loss: 5.496 | top1: 10.441% ,top5: 50.173%\n",
      "Loss: 5.484 | top1: 10.437% ,top5: 50.149%\n",
      "Loss: 5.472 | top1: 10.433% ,top5: 50.155%\n",
      "Loss: 5.460 | top1: 10.423% ,top5: 50.160%\n",
      "Loss: 5.449 | top1: 10.428% ,top5: 50.148%\n",
      "Loss: 5.437 | top1: 10.436% ,top5: 50.147%\n",
      "Loss: 5.426 | top1: 10.455% ,top5: 50.161%\n",
      "Loss: 5.414 | top1: 10.448% ,top5: 50.152%\n",
      "Loss: 5.403 | top1: 10.436% ,top5: 50.148%\n",
      "Loss: 5.391 | top1: 10.443% ,top5: 50.139%\n",
      "Loss: 5.380 | top1: 10.442% ,top5: 50.116%\n",
      "Loss: 5.369 | top1: 10.447% ,top5: 50.116%\n",
      "Loss: 5.358 | top1: 10.437% ,top5: 50.104%\n",
      "Loss: 5.347 | top1: 10.433% ,top5: 50.092%\n",
      "Loss: 5.336 | top1: 10.452% ,top5: 50.098%\n",
      "Loss: 5.326 | top1: 10.445% ,top5: 50.072%\n",
      "Loss: 5.315 | top1: 10.450% ,top5: 50.066%\n",
      "Loss: 5.304 | top1: 10.460% ,top5: 50.083%\n",
      "Loss: 5.294 | top1: 10.440% ,top5: 50.083%\n",
      "Loss: 5.283 | top1: 10.430% ,top5: 50.071%\n",
      "Loss: 5.273 | top1: 10.443% ,top5: 50.074%\n",
      "Loss: 5.263 | top1: 10.445% ,top5: 50.073%\n",
      "Loss: 5.252 | top1: 10.452% ,top5: 50.081%\n",
      "Loss: 5.242 | top1: 10.456% ,top5: 50.073%\n",
      "Loss: 5.232 | top1: 10.453% ,top5: 50.092%\n",
      "Loss: 5.222 | top1: 10.465% ,top5: 50.115%\n",
      "Loss: 5.212 | top1: 10.480% ,top5: 50.112%\n",
      "Loss: 5.202 | top1: 10.484% ,top5: 50.117%\n",
      "Loss: 5.192 | top1: 10.496% ,top5: 50.149%\n",
      "Loss: 5.182 | top1: 10.516% ,top5: 50.164%\n",
      "Loss: 5.173 | top1: 10.518% ,top5: 50.153%\n",
      "Loss: 5.163 | top1: 10.498% ,top5: 50.139%\n",
      "Loss: 5.153 | top1: 10.518% ,top5: 50.139%\n",
      "Loss: 5.144 | top1: 10.519% ,top5: 50.144%\n",
      "Loss: 5.134 | top1: 10.516% ,top5: 50.146%\n",
      "Loss: 5.125 | top1: 10.517% ,top5: 50.148%\n",
      "Loss: 5.116 | top1: 10.524% ,top5: 50.155%\n",
      "Loss: 5.106 | top1: 10.512% ,top5: 50.147%\n",
      "Loss: 5.097 | top1: 10.498% ,top5: 50.144%\n",
      "Loss: 5.088 | top1: 10.512% ,top5: 50.151%\n",
      "Loss: 5.079 | top1: 10.519% ,top5: 50.133%\n",
      "Loss: 5.070 | top1: 10.533% ,top5: 50.143%\n",
      "Loss: 5.061 | top1: 10.524% ,top5: 50.127%\n",
      "Loss: 5.052 | top1: 10.523% ,top5: 50.116%\n",
      "Loss: 5.043 | top1: 10.509% ,top5: 50.121%\n",
      "Loss: 5.034 | top1: 10.505% ,top5: 50.113%\n",
      "Loss: 5.026 | top1: 10.502% ,top5: 50.113%\n",
      "Loss: 5.017 | top1: 10.506% ,top5: 50.107%\n",
      "Loss: 5.008 | top1: 10.497% ,top5: 50.114%\n",
      "Loss: 5.000 | top1: 10.491% ,top5: 50.099%\n",
      "Loss: 4.991 | top1: 10.480% ,top5: 50.087%\n",
      "Loss: 4.983 | top1: 10.472% ,top5: 50.086%\n",
      "Loss: 4.974 | top1: 10.473% ,top5: 50.093%\n",
      "Loss: 4.966 | top1: 10.492% ,top5: 50.118%\n",
      "Loss: 4.958 | top1: 10.498% ,top5: 50.117%\n",
      "Loss: 4.949 | top1: 10.499% ,top5: 50.083%\n",
      "Loss: 4.941 | top1: 10.496% ,top5: 50.080%\n",
      "Loss: 4.933 | top1: 10.490% ,top5: 50.089%\n",
      "Loss: 4.925 | top1: 10.491% ,top5: 50.089%\n",
      "Loss: 4.917 | top1: 10.488% ,top5: 50.072%\n",
      "Loss: 4.909 | top1: 10.487% ,top5: 50.058%\n",
      "Loss: 4.901 | top1: 10.488% ,top5: 50.079%\n",
      "Loss: 4.893 | top1: 10.475% ,top5: 50.055%\n",
      "Loss: 4.885 | top1: 10.472% ,top5: 50.043%\n",
      "Loss: 4.877 | top1: 10.471% ,top5: 50.028%\n",
      "Loss: 4.869 | top1: 10.463% ,top5: 50.019%\n",
      "Loss: 4.862 | top1: 10.462% ,top5: 50.012%\n",
      "Loss: 4.854 | top1: 10.475% ,top5: 50.012%\n",
      "Loss: 4.846 | top1: 10.453% ,top5: 49.988%\n",
      "Loss: 4.839 | top1: 10.443% ,top5: 50.000%\n",
      "Loss: 4.831 | top1: 10.435% ,top5: 49.995%\n",
      "Loss: 4.824 | top1: 10.427% ,top5: 49.956%\n",
      "Loss: 4.816 | top1: 10.422% ,top5: 49.951%\n",
      "Loss: 4.809 | top1: 10.417% ,top5: 49.952%\n",
      "Loss: 4.801 | top1: 10.425% ,top5: 49.961%\n",
      "Loss: 4.794 | top1: 10.420% ,top5: 49.966%\n",
      "Loss: 4.787 | top1: 10.417% ,top5: 49.952%\n",
      "Loss: 4.780 | top1: 10.414% ,top5: 49.945%\n",
      "Loss: 4.772 | top1: 10.415% ,top5: 49.941%\n",
      "Loss: 4.765 | top1: 10.414% ,top5: 49.952%\n",
      "Loss: 4.758 | top1: 10.425% ,top5: 49.948%\n",
      "Loss: 4.751 | top1: 10.431% ,top5: 49.941%\n",
      "Loss: 4.744 | top1: 10.417% ,top5: 49.946%\n",
      "Loss: 4.737 | top1: 10.423% ,top5: 49.935%\n",
      "Loss: 4.730 | top1: 10.422% ,top5: 49.942%\n",
      "Loss: 4.723 | top1: 10.421% ,top5: 49.938%\n",
      "Loss: 4.716 | top1: 10.423% ,top5: 49.929%\n",
      "Loss: 4.709 | top1: 10.428% ,top5: 49.951%\n",
      "Loss: 4.703 | top1: 10.425% ,top5: 49.967%\n",
      "Loss: 4.696 | top1: 10.427% ,top5: 49.956%\n",
      "Loss: 4.689 | top1: 10.431% ,top5: 49.952%\n",
      "Loss: 4.682 | top1: 10.428% ,top5: 49.974%\n",
      "Loss: 4.676 | top1: 10.431% ,top5: 49.998%\n",
      "Loss: 4.669 | top1: 10.435% ,top5: 49.983%\n",
      "Loss: 4.663 | top1: 10.421% ,top5: 49.970%\n",
      "Loss: 4.656 | top1: 10.412% ,top5: 49.965%\n",
      "Loss: 4.650 | top1: 10.409% ,top5: 49.961%\n",
      "Loss: 4.643 | top1: 10.408% ,top5: 49.968%\n",
      "Loss: 4.637 | top1: 10.401% ,top5: 49.951%\n",
      "Loss: 4.630 | top1: 10.407% ,top5: 49.970%\n",
      "Loss: 4.624 | top1: 10.415% ,top5: 49.996%\n",
      "Loss: 4.618 | top1: 10.412% ,top5: 50.019%\n",
      "Loss: 4.611 | top1: 10.409% ,top5: 50.011%\n",
      "Loss: 4.605 | top1: 10.412% ,top5: 50.030%\n",
      "Loss: 4.599 | top1: 10.414% ,top5: 50.042%\n",
      "Loss: 4.593 | top1: 10.409% ,top5: 50.044%\n",
      "Loss: 4.586 | top1: 10.408% ,top5: 50.050%\n",
      "Loss: 4.580 | top1: 10.403% ,top5: 50.029%\n",
      "Loss: 4.574 | top1: 10.405% ,top5: 50.025%\n",
      "Loss: 4.568 | top1: 10.400% ,top5: 50.033%\n",
      "Loss: 4.562 | top1: 10.397% ,top5: 50.035%\n",
      "Loss: 4.556 | top1: 10.399% ,top5: 50.052%\n",
      "Loss: 4.550 | top1: 10.408% ,top5: 50.064%\n",
      "Loss: 4.544 | top1: 10.406% ,top5: 50.049%\n",
      "Loss: 4.539 | top1: 10.405% ,top5: 50.047%\n",
      "Loss: 4.533 | top1: 10.408% ,top5: 50.051%\n",
      "Loss: 4.527 | top1: 10.406% ,top5: 50.041%\n",
      "Loss: 4.521 | top1: 10.407% ,top5: 50.051%\n",
      "Loss: 4.515 | top1: 10.406% ,top5: 50.049%\n",
      "Loss: 4.509 | top1: 10.396% ,top5: 50.055%\n",
      "Loss: 4.504 | top1: 10.387% ,top5: 50.059%\n",
      "Loss: 4.498 | top1: 10.392% ,top5: 50.057%\n",
      "Loss: 4.492 | top1: 10.394% ,top5: 50.048%\n",
      "Loss: 4.487 | top1: 10.395% ,top5: 50.044%\n",
      "Loss: 4.481 | top1: 10.393% ,top5: 50.050%\n",
      "Loss: 4.478 | top1: 10.394% ,top5: 50.052%\n",
      "Loss: 4.478 | top1: 10.394%, top5: 50.052% elasped time:  25 seconds.\n",
      "Testing at epoch 1\n",
      "Loss: 2.299 | top1: 10.156% ,top5: 52.344%\n",
      "Loss: 2.301 | top1: 9.375% ,top5: 50.391%\n",
      "Loss: 2.302 | top1: 9.115% ,top5: 49.740%\n",
      "Loss: 2.301 | top1: 10.156% ,top5: 50.781%\n",
      "Loss: 2.301 | top1: 9.688% ,top5: 51.719%\n",
      "Loss: 2.301 | top1: 11.068% ,top5: 52.214%\n",
      "Loss: 2.302 | top1: 9.821% ,top5: 51.451%\n",
      "Loss: 2.301 | top1: 10.352% ,top5: 50.977%\n",
      "Loss: 2.302 | top1: 10.156% ,top5: 49.913%\n",
      "Loss: 2.302 | top1: 10.156% ,top5: 50.391%\n",
      "Loss: 2.303 | top1: 10.298% ,top5: 49.929%\n",
      "Loss: 2.301 | top1: 10.286% ,top5: 49.740%\n",
      "Loss: 2.301 | top1: 10.096% ,top5: 49.940%\n",
      "Loss: 2.301 | top1: 10.045% ,top5: 50.391%\n",
      "Loss: 2.301 | top1: 10.052% ,top5: 50.365%\n",
      "Loss: 2.301 | top1: 10.352% ,top5: 50.244%\n",
      "Loss: 2.301 | top1: 10.202% ,top5: 50.276%\n",
      "Loss: 2.301 | top1: 10.113% ,top5: 50.000%\n",
      "Loss: 2.300 | top1: 10.074% ,top5: 50.082%\n",
      "Loss: 2.300 | top1: 10.156% ,top5: 50.000%\n",
      "Loss: 2.300 | top1: 10.342% ,top5: 50.223%\n",
      "Loss: 2.300 | top1: 10.156% ,top5: 49.787%\n",
      "Loss: 2.300 | top1: 10.054% ,top5: 49.728%\n",
      "Loss: 2.300 | top1: 10.156% ,top5: 49.512%\n",
      "Loss: 2.301 | top1: 9.938% ,top5: 49.438%\n",
      "Loss: 2.300 | top1: 10.066% ,top5: 49.790%\n",
      "Loss: 2.300 | top1: 10.127% ,top5: 49.740%\n",
      "Loss: 2.301 | top1: 9.961% ,top5: 49.916%\n",
      "Loss: 2.300 | top1: 10.210% ,top5: 50.242%\n",
      "Loss: 2.301 | top1: 10.208% ,top5: 50.130%\n",
      "Loss: 2.301 | top1: 10.358% ,top5: 50.328%\n",
      "Loss: 2.301 | top1: 10.303% ,top5: 50.073%\n",
      "Loss: 2.301 | top1: 10.417% ,top5: 50.047%\n",
      "Loss: 2.302 | top1: 10.340% ,top5: 49.908%\n",
      "Loss: 2.303 | top1: 10.379% ,top5: 50.022%\n",
      "Loss: 2.303 | top1: 10.156% ,top5: 49.957%\n",
      "Loss: 2.303 | top1: 10.093% ,top5: 49.894%\n",
      "Loss: 2.303 | top1: 10.074% ,top5: 49.836%\n",
      "Loss: 2.303 | top1: 10.016% ,top5: 49.860%\n",
      "Loss: 2.303 | top1: 9.980% ,top5: 49.805%\n",
      "Loss: 2.303 | top1: 9.928% ,top5: 49.829%\n",
      "Loss: 2.304 | top1: 9.914% ,top5: 49.684%\n",
      "Loss: 2.305 | top1: 9.902% ,top5: 49.727%\n",
      "Loss: 2.305 | top1: 9.979% ,top5: 49.734%\n",
      "Loss: 2.305 | top1: 9.896% ,top5: 49.757%\n",
      "Loss: 2.305 | top1: 9.935% ,top5: 49.728%\n",
      "Loss: 2.304 | top1: 10.007% ,top5: 49.801%\n",
      "Loss: 2.304 | top1: 9.993% ,top5: 49.951%\n",
      "Loss: 2.304 | top1: 9.901% ,top5: 49.888%\n",
      "Loss: 2.304 | top1: 10.000% ,top5: 50.000%\n",
      "Loss: 2.304 | top1: 9.972% ,top5: 50.000%\n",
      "Loss: 2.304 | top1: 10.021% ,top5: 50.135%\n",
      "Loss: 2.304 | top1: 10.009% ,top5: 50.118%\n",
      "Loss: 2.304 | top1: 10.055% ,top5: 50.145%\n",
      "Loss: 2.304 | top1: 10.085% ,top5: 50.142%\n",
      "Loss: 2.304 | top1: 10.142% ,top5: 50.153%\n",
      "Loss: 2.304 | top1: 10.129% ,top5: 50.137%\n",
      "Loss: 2.304 | top1: 10.129% ,top5: 50.094%\n",
      "Loss: 2.304 | top1: 10.090% ,top5: 50.066%\n",
      "Loss: 2.304 | top1: 10.104% ,top5: 50.091%\n",
      "Loss: 2.304 | top1: 10.041% ,top5: 50.128%\n",
      "Loss: 2.304 | top1: 10.030% ,top5: 50.101%\n",
      "Loss: 2.304 | top1: 10.032% ,top5: 50.273%\n",
      "Loss: 2.304 | top1: 10.071% ,top5: 50.195%\n",
      "Loss: 2.304 | top1: 10.120% ,top5: 50.361%\n",
      "Loss: 2.304 | top1: 10.121% ,top5: 50.391%\n",
      "Loss: 2.304 | top1: 10.110% ,top5: 50.396%\n",
      "Loss: 2.304 | top1: 10.064% ,top5: 50.345%\n",
      "Loss: 2.304 | top1: 10.054% ,top5: 50.249%\n",
      "Loss: 2.304 | top1: 10.045% ,top5: 50.335%\n",
      "Loss: 2.304 | top1: 10.046% ,top5: 50.396%\n",
      "Loss: 2.304 | top1: 10.080% ,top5: 50.391%\n",
      "Loss: 2.304 | top1: 10.049% ,top5: 50.375%\n",
      "Loss: 2.304 | top1: 10.082% ,top5: 50.454%\n",
      "Loss: 2.304 | top1: 10.073% ,top5: 50.448%\n",
      "Loss: 2.304 | top1: 10.074% ,top5: 50.370%\n",
      "Loss: 2.304 | top1: 10.095% ,top5: 50.375%\n",
      "Loss: 2.304 | top1: 10.126% ,top5: 50.381%\n",
      "Loss: 2.304 | top1: 10.130% ,top5: 50.370%\n",
      "Loss: 2.304 | top1: 10.130%, top5: 50.370%, elasped time:   3 seconds. Best Acc: 10.130%\n",
      "Training finished successfully. Model size:  21279434\n",
      "Best acc:  10.13\n"
     ]
    }
   ],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch preact --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv False --block-fc 0 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e447b-9e3a-493f-8b97-d1c88606717f",
   "metadata": {},
   "source": [
    "#### network deconvolution --- preact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00847006-0fa7-47f4-ab49-3c9b3ff737d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "| Preparing CIFAR-10 dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "2024-03-14 20:29:57.273898: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "==> Building model..\n",
      "************ Batch norm disabled when deconv is used. ************\n",
      "Namespace(msg=1, resume='', use_gpu=True, num_workers=16, result_path='checkpoints/cifar10,preact,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512/03-14-20.29', checkpoint_path='checkpoints/cifar10,preact,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512/03-14-20.29', checkpoint_epoch=-1, print_freq=20, seed=0, optimizer='SGD', lr=0.1, lr_scheduler='cosine', momentum=0.9, weight_decay=0.001, batch_size=128, epochs=1, milestone=0.4, multistep_gamma=0.1, arch='preact', dataset='cifar10', init='kaiming_1', save_plot=True, tensorboard=True, loss='CE', method=3, batchnorm=False, deconv=functools.partial(<class 'models.deconv.FastDeconv'>, bias=True, eps=1e-05, n_iter=5, block=64, sampling_stride=3), delinear=functools.partial(<class 'models.deconv.Delinear'>, block=512, eps=1e-05, n_iter=5), block_fc=512, block=64, deconv_iter=5, eps=1e-05, bias=True, stride=3, freeze=False, log_path='checkpoints/cifar10,preact,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512/03-14-20.29', channel_deconv=None, start_epoch=0, in_planes=3, input_size=32, classes=('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'), num_outputs=10, train_epoch_logger=<net_util.Logger object at 0x7fd82a36be50>, train_batch_logger=<net_util.Logger object at 0x7fd82a36beb0>, test_epoch_logger=<net_util.Logger object at 0x7fd82a36bfa0>, writer=<torch.utils.tensorboard.writer.SummaryWriter object at 0x7fd82a3b40d0>, criterion=CrossEntropyLoss(), logger_n_iter=0)\n",
      "11169162 trainable parameters in the network.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Current learning rate: 0.1\n",
      "training at epoch 1\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/oruma001/CS895/project/codebase/models/deconv.py:305: UserWarning: This overload of addmm is deprecated:\n",
      "\taddmm(Number beta, Tensor input, Number alpha, Tensor mat1, Tensor mat2, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddmm(Tensor input, Tensor mat1, Tensor mat2, *, Number beta, Number alpha, Tensor out) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  Cov = torch.addmm(self.eps, Id, 1. / X.shape[0], X.t(), X)\n",
      "Loss: 2.331 | top1: 10.938% ,top5: 48.438%\n",
      "Loss: 2.289 | top1: 12.891% ,top5: 53.516%\n",
      "Loss: 2.277 | top1: 12.240% ,top5: 55.990%\n",
      "Loss: 2.256 | top1: 13.867% ,top5: 58.789%\n",
      "Loss: 2.240 | top1: 15.781% ,top5: 59.844%\n",
      "Loss: 2.233 | top1: 15.755% ,top5: 60.417%\n",
      "Loss: 2.226 | top1: 16.629% ,top5: 61.719%\n",
      "Loss: 2.227 | top1: 16.211% ,top5: 62.305%\n",
      "Loss: 2.219 | top1: 17.448% ,top5: 63.281%\n",
      "Loss: 2.213 | top1: 17.578% ,top5: 63.828%\n",
      "Loss: 2.198 | top1: 18.395% ,top5: 64.773%\n",
      "Loss: 2.190 | top1: 18.945% ,top5: 65.430%\n",
      "Loss: 2.183 | top1: 19.171% ,top5: 66.226%\n",
      "Loss: 2.173 | top1: 19.531% ,top5: 66.853%\n",
      "Loss: 2.167 | top1: 19.844% ,top5: 67.448%\n",
      "Loss: 2.159 | top1: 20.605% ,top5: 67.822%\n",
      "Loss: 2.149 | top1: 21.140% ,top5: 68.520%\n",
      "Loss: 2.139 | top1: 21.354% ,top5: 69.184%\n",
      "Loss: 2.128 | top1: 21.875% ,top5: 69.778%\n",
      "Loss: 2.122 | top1: 21.914% ,top5: 70.078%\n",
      "Loss: 2.115 | top1: 22.247% ,top5: 70.759%\n",
      "Loss: 2.096 | top1: 23.224% ,top5: 71.555%\n",
      "Loss: 2.079 | top1: 24.117% ,top5: 72.215%\n",
      "Loss: 2.067 | top1: 24.674% ,top5: 72.721%\n",
      "Loss: 2.057 | top1: 24.906% ,top5: 73.250%\n",
      "Loss: 2.047 | top1: 25.180% ,top5: 73.798%\n",
      "Loss: 2.038 | top1: 25.781% ,top5: 74.248%\n",
      "Loss: 2.026 | top1: 26.032% ,top5: 74.833%\n",
      "Loss: 2.020 | top1: 26.401% ,top5: 75.108%\n",
      "Loss: 2.012 | top1: 26.510% ,top5: 75.443%\n",
      "Loss: 2.005 | top1: 26.815% ,top5: 76.033%\n",
      "Loss: 1.995 | top1: 27.222% ,top5: 76.343%\n",
      "Loss: 1.987 | top1: 27.462% ,top5: 76.634%\n",
      "Loss: 1.979 | top1: 27.826% ,top5: 76.999%\n",
      "Loss: 1.971 | top1: 28.103% ,top5: 77.277%\n",
      "Loss: 1.960 | top1: 28.407% ,top5: 77.669%\n",
      "Loss: 1.952 | top1: 28.822% ,top5: 77.893%\n",
      "Loss: 1.950 | top1: 28.988% ,top5: 78.022%\n",
      "Loss: 1.943 | top1: 29.127% ,top5: 78.345%\n",
      "Loss: 1.934 | top1: 29.453% ,top5: 78.691%\n",
      "Loss: 1.926 | top1: 29.802% ,top5: 78.982%\n",
      "Loss: 1.917 | top1: 30.208% ,top5: 79.222%\n",
      "Loss: 1.909 | top1: 30.414% ,top5: 79.506%\n",
      "Loss: 1.903 | top1: 30.682% ,top5: 79.759%\n",
      "Loss: 1.898 | top1: 30.885% ,top5: 79.896%\n",
      "Loss: 1.890 | top1: 31.250% ,top5: 80.146%\n",
      "Loss: 1.881 | top1: 31.533% ,top5: 80.319%\n",
      "Loss: 1.875 | top1: 31.803% ,top5: 80.501%\n",
      "Loss: 1.869 | top1: 31.983% ,top5: 80.676%\n",
      "Loss: 1.863 | top1: 32.109% ,top5: 80.938%\n",
      "Loss: 1.857 | top1: 32.368% ,top5: 81.158%\n",
      "Loss: 1.854 | top1: 32.497% ,top5: 81.265%\n",
      "Loss: 1.849 | top1: 32.754% ,top5: 81.338%\n",
      "Loss: 1.842 | top1: 32.928% ,top5: 81.481%\n",
      "Loss: 1.836 | top1: 33.125% ,top5: 81.619%\n",
      "Loss: 1.830 | top1: 33.357% ,top5: 81.822%\n",
      "Loss: 1.821 | top1: 33.758% ,top5: 82.045%\n",
      "Loss: 1.817 | top1: 33.850% ,top5: 82.260%\n",
      "Loss: 1.814 | top1: 33.938% ,top5: 82.336%\n",
      "Loss: 1.808 | top1: 34.128% ,top5: 82.500%\n",
      "Loss: 1.803 | top1: 34.247% ,top5: 82.659%\n",
      "Loss: 1.800 | top1: 34.362% ,top5: 82.812%\n",
      "Loss: 1.796 | top1: 34.474% ,top5: 82.924%\n",
      "Loss: 1.792 | top1: 34.558% ,top5: 83.057%\n",
      "Loss: 1.790 | top1: 34.603% ,top5: 83.161%\n",
      "Loss: 1.783 | top1: 34.801% ,top5: 83.310%\n",
      "Loss: 1.777 | top1: 35.075% ,top5: 83.489%\n",
      "Loss: 1.772 | top1: 35.260% ,top5: 83.628%\n",
      "Loss: 1.768 | top1: 35.417% ,top5: 83.741%\n",
      "Loss: 1.763 | top1: 35.647% ,top5: 83.873%\n",
      "Loss: 1.761 | top1: 35.772% ,top5: 83.957%\n",
      "Loss: 1.754 | top1: 36.013% ,top5: 84.125%\n",
      "Loss: 1.747 | top1: 36.269% ,top5: 84.289%\n",
      "Loss: 1.741 | top1: 36.529% ,top5: 84.407%\n",
      "Loss: 1.736 | top1: 36.677% ,top5: 84.531%\n",
      "Loss: 1.731 | top1: 36.852% ,top5: 84.642%\n",
      "Loss: 1.727 | top1: 36.952% ,top5: 84.750%\n",
      "Loss: 1.722 | top1: 37.179% ,top5: 84.856%\n",
      "Loss: 1.718 | top1: 37.431% ,top5: 84.949%\n",
      "Loss: 1.713 | top1: 37.627% ,top5: 85.049%\n",
      "Loss: 1.710 | top1: 37.770% ,top5: 85.118%\n",
      "Loss: 1.705 | top1: 37.995% ,top5: 85.232%\n",
      "Loss: 1.699 | top1: 38.225% ,top5: 85.354%\n",
      "Loss: 1.694 | top1: 38.411% ,top5: 85.472%\n",
      "Loss: 1.690 | top1: 38.566% ,top5: 85.588%\n",
      "Loss: 1.687 | top1: 38.654% ,top5: 85.710%\n",
      "Loss: 1.683 | top1: 38.883% ,top5: 85.767%\n",
      "Loss: 1.679 | top1: 39.080% ,top5: 85.840%\n",
      "Loss: 1.677 | top1: 39.177% ,top5: 85.929%\n",
      "Loss: 1.674 | top1: 39.297% ,top5: 86.024%\n",
      "Loss: 1.669 | top1: 39.432% ,top5: 86.169%\n",
      "Loss: 1.665 | top1: 39.555% ,top5: 86.269%\n",
      "Loss: 1.661 | top1: 39.693% ,top5: 86.366%\n",
      "Loss: 1.658 | top1: 39.794% ,top5: 86.428%\n",
      "Loss: 1.654 | top1: 39.926% ,top5: 86.530%\n",
      "Loss: 1.649 | top1: 40.096% ,top5: 86.637%\n",
      "Loss: 1.645 | top1: 40.279% ,top5: 86.719%\n",
      "Loss: 1.640 | top1: 40.458% ,top5: 86.830%\n",
      "Loss: 1.636 | top1: 40.601% ,top5: 86.916%\n",
      "Loss: 1.632 | top1: 40.711% ,top5: 87.000%\n",
      "Loss: 1.628 | top1: 40.811% ,top5: 87.036%\n",
      "Loss: 1.627 | top1: 40.878% ,top5: 87.109%\n",
      "Loss: 1.624 | top1: 41.042% ,top5: 87.189%\n",
      "Loss: 1.621 | top1: 41.181% ,top5: 87.275%\n",
      "Loss: 1.619 | top1: 41.272% ,top5: 87.292%\n",
      "Loss: 1.617 | top1: 41.340% ,top5: 87.338%\n",
      "Loss: 1.614 | top1: 41.472% ,top5: 87.376%\n",
      "Loss: 1.611 | top1: 41.616% ,top5: 87.442%\n",
      "Loss: 1.606 | top1: 41.779% ,top5: 87.522%\n",
      "Loss: 1.603 | top1: 41.868% ,top5: 87.592%\n",
      "Loss: 1.601 | top1: 41.934% ,top5: 87.627%\n",
      "Loss: 1.599 | top1: 42.041% ,top5: 87.695%\n",
      "Loss: 1.597 | top1: 42.077% ,top5: 87.749%\n",
      "Loss: 1.595 | top1: 42.181% ,top5: 87.802%\n",
      "Loss: 1.592 | top1: 42.262% ,top5: 87.887%\n",
      "Loss: 1.590 | top1: 42.356% ,top5: 87.938%\n",
      "Loss: 1.588 | top1: 42.468% ,top5: 88.021%\n",
      "Loss: 1.585 | top1: 42.525% ,top5: 88.089%\n",
      "Loss: 1.584 | top1: 42.568% ,top5: 88.111%\n",
      "Loss: 1.581 | top1: 42.656% ,top5: 88.184%\n",
      "Loss: 1.577 | top1: 42.788% ,top5: 88.230%\n",
      "Loss: 1.574 | top1: 42.943% ,top5: 88.300%\n",
      "Loss: 1.571 | top1: 43.064% ,top5: 88.351%\n",
      "Loss: 1.568 | top1: 43.214% ,top5: 88.426%\n",
      "Loss: 1.565 | top1: 43.312% ,top5: 88.469%\n",
      "Loss: 1.562 | top1: 43.428% ,top5: 88.529%\n",
      "Loss: 1.558 | top1: 43.541% ,top5: 88.589%\n",
      "Loss: 1.556 | top1: 43.640% ,top5: 88.660%\n",
      "Loss: 1.555 | top1: 43.720% ,top5: 88.669%\n",
      "Loss: 1.553 | top1: 43.786% ,top5: 88.714%\n",
      "Loss: 1.550 | top1: 43.887% ,top5: 88.740%\n",
      "Loss: 1.548 | top1: 43.951% ,top5: 88.772%\n",
      "Loss: 1.545 | top1: 44.067% ,top5: 88.833%\n",
      "Loss: 1.543 | top1: 44.176% ,top5: 88.864%\n",
      "Loss: 1.540 | top1: 44.288% ,top5: 88.924%\n",
      "Loss: 1.537 | top1: 44.382% ,top5: 88.953%\n",
      "Loss: 1.535 | top1: 44.480% ,top5: 88.977%\n",
      "Loss: 1.533 | top1: 44.548% ,top5: 89.046%\n",
      "Loss: 1.531 | top1: 44.655% ,top5: 89.102%\n",
      "Loss: 1.528 | top1: 44.749% ,top5: 89.146%\n",
      "Loss: 1.526 | top1: 44.853% ,top5: 89.207%\n",
      "Loss: 1.524 | top1: 44.916% ,top5: 89.239%\n",
      "Loss: 1.521 | top1: 45.034% ,top5: 89.270%\n",
      "Loss: 1.519 | top1: 45.117% ,top5: 89.296%\n",
      "Loss: 1.516 | top1: 45.259% ,top5: 89.348%\n",
      "Loss: 1.512 | top1: 45.409% ,top5: 89.389%\n",
      "Loss: 1.511 | top1: 45.467% ,top5: 89.403%\n",
      "Loss: 1.509 | top1: 45.566% ,top5: 89.421%\n",
      "Loss: 1.506 | top1: 45.680% ,top5: 89.451%\n",
      "Loss: 1.504 | top1: 45.724% ,top5: 89.500%\n",
      "Loss: 1.500 | top1: 45.866% ,top5: 89.570%\n",
      "Loss: 1.498 | top1: 45.950% ,top5: 89.623%\n",
      "Loss: 1.496 | top1: 46.012% ,top5: 89.660%\n",
      "Loss: 1.493 | top1: 46.119% ,top5: 89.712%\n",
      "Loss: 1.491 | top1: 46.200% ,top5: 89.753%\n",
      "Loss: 1.489 | top1: 46.274% ,top5: 89.809%\n",
      "Loss: 1.487 | top1: 46.318% ,top5: 89.869%\n",
      "Loss: 1.485 | top1: 46.351% ,top5: 89.908%\n",
      "Loss: 1.484 | top1: 46.403% ,top5: 89.932%\n",
      "Loss: 1.482 | top1: 46.514% ,top5: 89.961%\n",
      "Loss: 1.480 | top1: 46.579% ,top5: 90.014%\n",
      "Loss: 1.478 | top1: 46.634% ,top5: 90.056%\n",
      "Loss: 1.475 | top1: 46.741% ,top5: 90.093%\n",
      "Loss: 1.473 | top1: 46.851% ,top5: 90.134%\n",
      "Loss: 1.470 | top1: 46.918% ,top5: 90.170%\n",
      "Loss: 1.468 | top1: 46.993% ,top5: 90.187%\n",
      "Loss: 1.466 | top1: 47.114% ,top5: 90.223%\n",
      "Loss: 1.463 | top1: 47.201% ,top5: 90.253%\n",
      "Loss: 1.461 | top1: 47.277% ,top5: 90.292%\n",
      "Loss: 1.459 | top1: 47.381% ,top5: 90.331%\n",
      "Loss: 1.457 | top1: 47.437% ,top5: 90.369%\n",
      "Loss: 1.456 | top1: 47.452% ,top5: 90.389%\n",
      "Loss: 1.453 | top1: 47.548% ,top5: 90.426%\n",
      "Loss: 1.452 | top1: 47.602% ,top5: 90.450%\n",
      "Loss: 1.449 | top1: 47.688% ,top5: 90.496%\n",
      "Loss: 1.448 | top1: 47.741% ,top5: 90.536%\n",
      "Loss: 1.445 | top1: 47.846% ,top5: 90.576%\n",
      "Loss: 1.443 | top1: 47.950% ,top5: 90.621%\n",
      "Loss: 1.440 | top1: 47.988% ,top5: 90.660%\n",
      "Loss: 1.439 | top1: 48.043% ,top5: 90.690%\n",
      "Loss: 1.436 | top1: 48.105% ,top5: 90.737%\n",
      "Loss: 1.435 | top1: 48.146% ,top5: 90.771%\n",
      "Loss: 1.432 | top1: 48.245% ,top5: 90.800%\n",
      "Loss: 1.430 | top1: 48.344% ,top5: 90.812%\n",
      "Loss: 1.428 | top1: 48.425% ,top5: 90.857%\n",
      "Loss: 1.426 | top1: 48.484% ,top5: 90.890%\n",
      "Loss: 1.424 | top1: 48.596% ,top5: 90.905%\n",
      "Loss: 1.423 | top1: 48.649% ,top5: 90.933%\n",
      "Loss: 1.420 | top1: 48.723% ,top5: 90.976%\n",
      "Loss: 1.418 | top1: 48.812% ,top5: 91.012%\n",
      "Loss: 1.417 | top1: 48.863% ,top5: 91.026%\n",
      "Loss: 1.415 | top1: 48.954% ,top5: 91.052%\n",
      "Loss: 1.412 | top1: 49.061% ,top5: 91.086%\n",
      "Loss: 1.411 | top1: 49.118% ,top5: 91.120%\n",
      "Loss: 1.408 | top1: 49.199% ,top5: 91.154%\n",
      "Loss: 1.407 | top1: 49.267% ,top5: 91.167%\n",
      "Loss: 1.406 | top1: 49.322% ,top5: 91.196%\n",
      "Loss: 1.404 | top1: 49.369% ,top5: 91.221%\n",
      "Loss: 1.403 | top1: 49.454% ,top5: 91.261%\n",
      "Loss: 1.400 | top1: 49.512% ,top5: 91.301%\n",
      "Loss: 1.399 | top1: 49.569% ,top5: 91.329%\n",
      "Loss: 1.396 | top1: 49.687% ,top5: 91.360%\n",
      "Loss: 1.395 | top1: 49.731% ,top5: 91.395%\n",
      "Loss: 1.393 | top1: 49.812% ,top5: 91.429%\n",
      "Loss: 1.391 | top1: 49.893% ,top5: 91.456%\n",
      "Loss: 1.389 | top1: 49.970% ,top5: 91.478%\n",
      "Loss: 1.388 | top1: 50.008% ,top5: 91.504%\n",
      "Loss: 1.385 | top1: 50.094% ,top5: 91.523%\n",
      "Loss: 1.384 | top1: 50.150% ,top5: 91.533%\n",
      "Loss: 1.382 | top1: 50.242% ,top5: 91.566%\n",
      "Loss: 1.380 | top1: 50.304% ,top5: 91.591%\n",
      "Loss: 1.379 | top1: 50.361% ,top5: 91.627%\n",
      "Loss: 1.376 | top1: 50.447% ,top5: 91.659%\n",
      "Loss: 1.375 | top1: 50.504% ,top5: 91.687%\n",
      "Loss: 1.373 | top1: 50.545% ,top5: 91.711%\n",
      "Loss: 1.372 | top1: 50.597% ,top5: 91.732%\n",
      "Loss: 1.371 | top1: 50.612% ,top5: 91.755%\n",
      "Loss: 1.370 | top1: 50.645% ,top5: 91.772%\n",
      "Loss: 1.368 | top1: 50.706% ,top5: 91.799%\n",
      "Loss: 1.367 | top1: 50.756% ,top5: 91.822%\n",
      "Loss: 1.365 | top1: 50.802% ,top5: 91.848%\n",
      "Loss: 1.364 | top1: 50.866% ,top5: 91.871%\n",
      "Loss: 1.362 | top1: 50.935% ,top5: 91.897%\n",
      "Loss: 1.360 | top1: 50.994% ,top5: 91.912%\n",
      "Loss: 1.359 | top1: 51.069% ,top5: 91.938%\n",
      "Loss: 1.357 | top1: 51.123% ,top5: 91.959%\n",
      "Loss: 1.356 | top1: 51.191% ,top5: 91.971%\n",
      "Loss: 1.354 | top1: 51.237% ,top5: 91.985%\n",
      "Loss: 1.353 | top1: 51.286% ,top5: 92.003%\n",
      "Loss: 1.352 | top1: 51.315% ,top5: 92.021%\n",
      "Loss: 1.350 | top1: 51.390% ,top5: 92.042%\n",
      "Loss: 1.349 | top1: 51.441% ,top5: 92.066%\n",
      "Loss: 1.347 | top1: 51.505% ,top5: 92.090%\n",
      "Loss: 1.346 | top1: 51.552% ,top5: 92.121%\n",
      "Loss: 1.345 | top1: 51.606% ,top5: 92.151%\n",
      "Loss: 1.343 | top1: 51.668% ,top5: 92.174%\n",
      "Loss: 1.341 | top1: 51.734% ,top5: 92.201%\n",
      "Loss: 1.339 | top1: 51.799% ,top5: 92.227%\n",
      "Loss: 1.338 | top1: 51.850% ,top5: 92.250%\n",
      "Loss: 1.337 | top1: 51.901% ,top5: 92.269%\n",
      "Loss: 1.335 | top1: 51.977% ,top5: 92.288%\n",
      "Loss: 1.334 | top1: 52.027% ,top5: 92.310%\n",
      "Loss: 1.332 | top1: 52.093% ,top5: 92.329%\n",
      "Loss: 1.330 | top1: 52.136% ,top5: 92.341%\n",
      "Loss: 1.329 | top1: 52.191% ,top5: 92.366%\n",
      "Loss: 1.328 | top1: 52.242% ,top5: 92.384%\n",
      "Loss: 1.326 | top1: 52.312% ,top5: 92.415%\n",
      "Loss: 1.324 | top1: 52.360% ,top5: 92.433%\n",
      "Loss: 1.323 | top1: 52.397% ,top5: 92.451%\n",
      "Loss: 1.322 | top1: 52.462% ,top5: 92.472%\n",
      "Loss: 1.320 | top1: 52.537% ,top5: 92.499%\n",
      "Loss: 1.318 | top1: 52.592% ,top5: 92.522%\n",
      "Loss: 1.317 | top1: 52.656% ,top5: 92.540%\n",
      "Loss: 1.316 | top1: 52.704% ,top5: 92.560%\n",
      "Loss: 1.314 | top1: 52.751% ,top5: 92.570%\n",
      "Loss: 1.313 | top1: 52.768% ,top5: 92.584%\n",
      "Loss: 1.311 | top1: 52.848% ,top5: 92.607%\n",
      "Loss: 1.309 | top1: 52.931% ,top5: 92.627%\n",
      "Loss: 1.308 | top1: 52.983% ,top5: 92.637%\n",
      "Loss: 1.306 | top1: 53.035% ,top5: 92.662%\n",
      "Loss: 1.305 | top1: 53.089% ,top5: 92.687%\n",
      "Loss: 1.303 | top1: 53.161% ,top5: 92.712%\n",
      "Loss: 1.301 | top1: 53.217% ,top5: 92.728%\n",
      "Loss: 1.300 | top1: 53.255% ,top5: 92.744%\n",
      "Loss: 1.299 | top1: 53.302% ,top5: 92.765%\n",
      "Loss: 1.298 | top1: 53.354% ,top5: 92.775%\n",
      "Loss: 1.296 | top1: 53.423% ,top5: 92.790%\n",
      "Loss: 1.295 | top1: 53.507% ,top5: 92.808%\n",
      "Loss: 1.293 | top1: 53.555% ,top5: 92.832%\n",
      "Loss: 1.292 | top1: 53.628% ,top5: 92.847%\n",
      "Loss: 1.290 | top1: 53.679% ,top5: 92.856%\n",
      "Loss: 1.289 | top1: 53.697% ,top5: 92.877%\n",
      "Loss: 1.288 | top1: 53.740% ,top5: 92.891%\n",
      "Loss: 1.286 | top1: 53.795% ,top5: 92.912%\n",
      "Loss: 1.285 | top1: 53.832% ,top5: 92.932%\n",
      "Loss: 1.284 | top1: 53.884% ,top5: 92.940%\n",
      "Loss: 1.282 | top1: 53.937% ,top5: 92.960%\n",
      "Loss: 1.281 | top1: 53.999% ,top5: 92.969%\n",
      "Loss: 1.280 | top1: 54.052% ,top5: 92.980%\n",
      "Loss: 1.279 | top1: 54.121% ,top5: 92.994%\n",
      "Loss: 1.278 | top1: 54.162% ,top5: 93.005%\n",
      "Loss: 1.276 | top1: 54.197% ,top5: 93.027%\n",
      "Loss: 1.275 | top1: 54.246% ,top5: 93.041%\n",
      "Loss: 1.273 | top1: 54.300% ,top5: 93.060%\n",
      "Loss: 1.272 | top1: 54.380% ,top5: 93.081%\n",
      "Loss: 1.270 | top1: 54.420% ,top5: 93.089%\n",
      "Loss: 1.269 | top1: 54.481% ,top5: 93.105%\n",
      "Loss: 1.268 | top1: 54.506% ,top5: 93.115%\n",
      "Loss: 1.267 | top1: 54.552% ,top5: 93.128%\n",
      "Loss: 1.265 | top1: 54.617% ,top5: 93.138%\n",
      "Loss: 1.264 | top1: 54.677% ,top5: 93.157%\n",
      "Loss: 1.263 | top1: 54.725% ,top5: 93.175%\n",
      "Loss: 1.261 | top1: 54.775% ,top5: 93.185%\n",
      "Loss: 1.260 | top1: 54.810% ,top5: 93.203%\n",
      "Loss: 1.258 | top1: 54.883% ,top5: 93.212%\n",
      "Loss: 1.257 | top1: 54.914% ,top5: 93.227%\n",
      "Loss: 1.256 | top1: 54.953% ,top5: 93.242%\n",
      "Loss: 1.255 | top1: 54.992% ,top5: 93.260%\n",
      "Loss: 1.254 | top1: 55.025% ,top5: 93.269%\n",
      "Loss: 1.253 | top1: 55.068% ,top5: 93.286%\n",
      "Loss: 1.252 | top1: 55.103% ,top5: 93.298%\n",
      "Loss: 1.251 | top1: 55.132% ,top5: 93.313%\n",
      "Loss: 1.250 | top1: 55.183% ,top5: 93.327%\n",
      "Loss: 1.248 | top1: 55.235% ,top5: 93.347%\n",
      "Loss: 1.247 | top1: 55.302% ,top5: 93.361%\n",
      "Loss: 1.246 | top1: 55.354% ,top5: 93.375%\n",
      "Loss: 1.244 | top1: 55.431% ,top5: 93.386%\n",
      "Loss: 1.242 | top1: 55.492% ,top5: 93.405%\n",
      "Loss: 1.241 | top1: 55.529% ,top5: 93.419%\n",
      "Loss: 1.240 | top1: 55.580% ,top5: 93.435%\n",
      "Loss: 1.239 | top1: 55.597% ,top5: 93.454%\n",
      "Loss: 1.239 | top1: 55.629% ,top5: 93.467%\n",
      "Loss: 1.238 | top1: 55.678% ,top5: 93.478%\n",
      "Loss: 1.236 | top1: 55.728% ,top5: 93.494%\n",
      "Loss: 1.235 | top1: 55.756% ,top5: 93.502%\n",
      "Loss: 1.234 | top1: 55.807% ,top5: 93.515%\n",
      "Loss: 1.233 | top1: 55.861% ,top5: 93.528%\n",
      "Loss: 1.232 | top1: 55.896% ,top5: 93.536%\n",
      "Loss: 1.231 | top1: 55.944% ,top5: 93.544%\n",
      "Loss: 1.230 | top1: 55.974% ,top5: 93.550%\n",
      "Loss: 1.228 | top1: 56.043% ,top5: 93.567%\n",
      "Loss: 1.228 | top1: 56.068% ,top5: 93.575%\n",
      "Loss: 1.227 | top1: 56.105% ,top5: 93.590%\n",
      "Loss: 1.226 | top1: 56.149% ,top5: 93.601%\n",
      "Loss: 1.225 | top1: 56.173% ,top5: 93.618%\n",
      "Loss: 1.224 | top1: 56.209% ,top5: 93.630%\n",
      "Loss: 1.223 | top1: 56.255% ,top5: 93.647%\n",
      "Loss: 1.222 | top1: 56.300% ,top5: 93.657%\n",
      "Loss: 1.221 | top1: 56.333% ,top5: 93.660%\n",
      "Loss: 1.220 | top1: 56.383% ,top5: 93.677%\n",
      "Loss: 1.219 | top1: 56.436% ,top5: 93.693%\n",
      "Loss: 1.218 | top1: 56.464% ,top5: 93.703%\n",
      "Loss: 1.217 | top1: 56.508% ,top5: 93.715%\n",
      "Loss: 1.215 | top1: 56.570% ,top5: 93.731%\n",
      "Loss: 1.214 | top1: 56.616% ,top5: 93.745%\n",
      "Loss: 1.213 | top1: 56.641% ,top5: 93.750%\n",
      "Loss: 1.212 | top1: 56.711% ,top5: 93.766%\n",
      "Loss: 1.211 | top1: 56.728% ,top5: 93.775%\n",
      "Loss: 1.210 | top1: 56.771% ,top5: 93.789%\n",
      "Loss: 1.209 | top1: 56.797% ,top5: 93.798%\n",
      "Loss: 1.208 | top1: 56.837% ,top5: 93.807%\n",
      "Loss: 1.207 | top1: 56.869% ,top5: 93.816%\n",
      "Loss: 1.206 | top1: 56.901% ,top5: 93.832%\n",
      "Loss: 1.206 | top1: 56.929% ,top5: 93.843%\n",
      "Loss: 1.204 | top1: 56.954% ,top5: 93.856%\n",
      "Loss: 1.204 | top1: 56.986% ,top5: 93.865%\n",
      "Loss: 1.203 | top1: 57.018% ,top5: 93.872%\n",
      "Loss: 1.202 | top1: 57.031% ,top5: 93.880%\n",
      "Loss: 1.201 | top1: 57.065% ,top5: 93.893%\n",
      "Loss: 1.200 | top1: 57.103% ,top5: 93.902%\n",
      "Loss: 1.200 | top1: 57.129% ,top5: 93.904%\n",
      "Loss: 1.199 | top1: 57.156% ,top5: 93.916%\n",
      "Loss: 1.198 | top1: 57.191% ,top5: 93.929%\n",
      "Loss: 1.197 | top1: 57.230% ,top5: 93.929%\n",
      "Loss: 1.196 | top1: 57.282% ,top5: 93.944%\n",
      "Loss: 1.195 | top1: 57.330% ,top5: 93.954%\n",
      "Loss: 1.194 | top1: 57.375% ,top5: 93.962%\n",
      "Loss: 1.193 | top1: 57.411% ,top5: 93.973%\n",
      "Loss: 1.192 | top1: 57.447% ,top5: 93.985%\n",
      "Loss: 1.191 | top1: 57.470% ,top5: 93.997%\n",
      "Loss: 1.190 | top1: 57.512% ,top5: 94.008%\n",
      "Loss: 1.189 | top1: 57.547% ,top5: 94.018%\n",
      "Loss: 1.188 | top1: 57.569% ,top5: 94.030%\n",
      "Loss: 1.187 | top1: 57.609% ,top5: 94.038%\n",
      "Loss: 1.186 | top1: 57.667% ,top5: 94.054%\n",
      "Loss: 1.185 | top1: 57.699% ,top5: 94.064%\n",
      "Loss: 1.184 | top1: 57.729% ,top5: 94.076%\n",
      "Loss: 1.183 | top1: 57.762% ,top5: 94.083%\n",
      "Loss: 1.182 | top1: 57.796% ,top5: 94.097%\n",
      "Loss: 1.182 | top1: 57.802% ,top5: 94.109%\n",
      "Loss: 1.180 | top1: 57.848% ,top5: 94.119%\n",
      "Loss: 1.179 | top1: 57.890% ,top5: 94.126%\n",
      "Loss: 1.178 | top1: 57.919% ,top5: 94.133%\n",
      "Loss: 1.177 | top1: 57.955% ,top5: 94.145%\n",
      "Loss: 1.177 | top1: 57.977% ,top5: 94.152%\n",
      "Loss: 1.176 | top1: 58.020% ,top5: 94.161%\n",
      "Loss: 1.175 | top1: 58.059% ,top5: 94.171%\n",
      "Loss: 1.174 | top1: 58.075% ,top5: 94.178%\n",
      "Loss: 1.173 | top1: 58.116% ,top5: 94.187%\n",
      "Loss: 1.172 | top1: 58.156% ,top5: 94.196%\n",
      "Loss: 1.171 | top1: 58.214% ,top5: 94.205%\n",
      "Loss: 1.170 | top1: 58.250% ,top5: 94.214%\n",
      "Loss: 1.170 | top1: 58.269% ,top5: 94.223%\n",
      "Loss: 1.169 | top1: 58.317% ,top5: 94.234%\n",
      "Loss: 1.168 | top1: 58.336% ,top5: 94.243%\n",
      "Loss: 1.167 | top1: 58.367% ,top5: 94.250%\n",
      "Loss: 1.167 | top1: 58.396% ,top5: 94.261%\n",
      "Loss: 1.166 | top1: 58.425% ,top5: 94.272%\n",
      "Loss: 1.165 | top1: 58.451% ,top5: 94.282%\n",
      "Loss: 1.164 | top1: 58.482% ,top5: 94.295%\n",
      "Loss: 1.164 | top1: 58.502% ,top5: 94.292%\n",
      "Loss: 1.164 | top1: 58.502%, top5: 94.292% elasped time:  55 seconds.\n",
      "Testing at epoch 1\n",
      "Loss: 0.796 | top1: 69.531% ,top5: 97.656%\n",
      "Loss: 0.735 | top1: 72.656% ,top5: 97.656%\n",
      "Loss: 0.761 | top1: 72.917% ,top5: 97.656%\n",
      "Loss: 0.736 | top1: 74.023% ,top5: 97.852%\n",
      "Loss: 0.752 | top1: 73.594% ,top5: 97.656%\n",
      "Loss: 0.766 | top1: 72.656% ,top5: 97.917%\n",
      "Loss: 0.790 | top1: 71.875% ,top5: 97.656%\n",
      "Loss: 0.789 | top1: 71.973% ,top5: 97.656%\n",
      "Loss: 0.785 | top1: 72.396% ,top5: 97.656%\n",
      "Loss: 0.788 | top1: 72.500% ,top5: 97.656%\n",
      "Loss: 0.808 | top1: 71.520% ,top5: 97.727%\n",
      "Loss: 0.809 | top1: 71.484% ,top5: 97.852%\n",
      "Loss: 0.801 | top1: 71.875% ,top5: 97.837%\n",
      "Loss: 0.809 | top1: 71.373% ,top5: 97.879%\n",
      "Loss: 0.810 | top1: 71.458% ,top5: 97.812%\n",
      "Loss: 0.807 | top1: 71.387% ,top5: 97.803%\n",
      "Loss: 0.794 | top1: 71.967% ,top5: 97.840%\n",
      "Loss: 0.790 | top1: 72.135% ,top5: 97.917%\n",
      "Loss: 0.784 | top1: 72.451% ,top5: 97.862%\n",
      "Loss: 0.776 | top1: 72.734% ,top5: 97.969%\n",
      "Loss: 0.772 | top1: 72.879% ,top5: 97.879%\n",
      "Loss: 0.773 | top1: 72.940% ,top5: 97.976%\n",
      "Loss: 0.769 | top1: 72.928% ,top5: 98.064%\n",
      "Loss: 0.769 | top1: 72.624% ,top5: 98.079%\n",
      "Loss: 0.768 | top1: 72.406% ,top5: 98.094%\n",
      "Loss: 0.767 | top1: 72.446% ,top5: 98.107%\n",
      "Loss: 0.764 | top1: 72.483% ,top5: 98.090%\n",
      "Loss: 0.767 | top1: 72.321% ,top5: 98.103%\n",
      "Loss: 0.777 | top1: 71.956% ,top5: 98.033%\n",
      "Loss: 0.778 | top1: 71.875% ,top5: 98.021%\n",
      "Loss: 0.775 | top1: 72.077% ,top5: 97.984%\n",
      "Loss: 0.776 | top1: 72.095% ,top5: 97.974%\n",
      "Loss: 0.780 | top1: 71.922% ,top5: 97.988%\n",
      "Loss: 0.778 | top1: 72.013% ,top5: 98.047%\n",
      "Loss: 0.779 | top1: 72.054% ,top5: 98.058%\n",
      "Loss: 0.779 | top1: 72.027% ,top5: 98.090%\n",
      "Loss: 0.778 | top1: 72.086% ,top5: 98.100%\n",
      "Loss: 0.775 | top1: 72.183% ,top5: 98.129%\n",
      "Loss: 0.773 | top1: 72.236% ,top5: 98.177%\n",
      "Loss: 0.775 | top1: 72.227% ,top5: 98.145%\n",
      "Loss: 0.774 | top1: 72.332% ,top5: 98.152%\n",
      "Loss: 0.772 | top1: 72.433% ,top5: 98.158%\n",
      "Loss: 0.770 | top1: 72.475% ,top5: 98.147%\n",
      "Loss: 0.767 | top1: 72.621% ,top5: 98.171%\n",
      "Loss: 0.767 | top1: 72.639% ,top5: 98.160%\n",
      "Loss: 0.769 | top1: 72.554% ,top5: 98.149%\n",
      "Loss: 0.770 | top1: 72.540% ,top5: 98.105%\n",
      "Loss: 0.771 | top1: 72.445% ,top5: 98.112%\n",
      "Loss: 0.772 | top1: 72.497% ,top5: 98.151%\n",
      "Loss: 0.772 | top1: 72.406% ,top5: 98.172%\n",
      "Loss: 0.772 | top1: 72.396% ,top5: 98.162%\n",
      "Loss: 0.773 | top1: 72.371% ,top5: 98.182%\n",
      "Loss: 0.776 | top1: 72.347% ,top5: 98.172%\n",
      "Loss: 0.774 | top1: 72.367% ,top5: 98.206%\n",
      "Loss: 0.773 | top1: 72.401% ,top5: 98.168%\n",
      "Loss: 0.773 | top1: 72.349% ,top5: 98.186%\n",
      "Loss: 0.774 | top1: 72.327% ,top5: 98.163%\n",
      "Loss: 0.775 | top1: 72.198% ,top5: 98.155%\n",
      "Loss: 0.776 | top1: 72.180% ,top5: 98.133%\n",
      "Loss: 0.775 | top1: 72.266% ,top5: 98.138%\n",
      "Loss: 0.775 | top1: 72.285% ,top5: 98.143%\n",
      "Loss: 0.775 | top1: 72.329% ,top5: 98.110%\n",
      "Loss: 0.775 | top1: 72.359% ,top5: 98.127%\n",
      "Loss: 0.773 | top1: 72.473% ,top5: 98.145%\n",
      "Loss: 0.773 | top1: 72.500% ,top5: 98.149%\n",
      "Loss: 0.771 | top1: 72.562% ,top5: 98.177%\n",
      "Loss: 0.774 | top1: 72.505% ,top5: 98.169%\n",
      "Loss: 0.772 | top1: 72.530% ,top5: 98.196%\n",
      "Loss: 0.770 | top1: 72.577% ,top5: 98.200%\n",
      "Loss: 0.771 | top1: 72.567% ,top5: 98.214%\n",
      "Loss: 0.773 | top1: 72.469% ,top5: 98.195%\n",
      "Loss: 0.771 | top1: 72.483% ,top5: 98.210%\n",
      "Loss: 0.769 | top1: 72.539% ,top5: 98.234%\n",
      "Loss: 0.769 | top1: 72.572% ,top5: 98.237%\n",
      "Loss: 0.772 | top1: 72.458% ,top5: 98.229%\n",
      "Loss: 0.773 | top1: 72.410% ,top5: 98.232%\n",
      "Loss: 0.775 | top1: 72.332% ,top5: 98.224%\n",
      "Loss: 0.774 | top1: 72.386% ,top5: 98.227%\n",
      "Loss: 0.773 | top1: 72.400% ,top5: 98.230%\n",
      "Loss: 0.773 | top1: 72.400%, top5: 98.230%, elasped time:   3 seconds. Best Acc: 72.400%\n",
      "Training finished successfully. Model size:  11169162\n",
      "Best acc:  72.4\n"
     ]
    }
   ],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch preact --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv True --block-fc 512 --wd .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa4ee5-c9f9-4a1a-a634-09273f54be6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ca681f8-f259-4446-a9f7-9cd449964fb5",
   "metadata": {},
   "source": [
    "# DenseNet-121 --- densenet121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fbed0d-7234-4353-bdd3-e0e4e9b9f1f6",
   "metadata": {},
   "source": [
    "#### batch normalization --- densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5204f210-052e-4444-92e7-0062238fa6aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "| Preparing CIFAR-10 dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "2024-03-14 21:18:48.529854: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "==> Building model..\n",
      "Namespace(msg=1, resume='', use_gpu=True, num_workers=16, result_path='checkpoints/cifar10,densenet121,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-14-21.18', checkpoint_path='checkpoints/cifar10,densenet121,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-14-21.18', checkpoint_epoch=-1, print_freq=20, seed=0, optimizer='SGD', lr=0.1, lr_scheduler='cosine', momentum=0.9, weight_decay=0.001, batch_size=128, epochs=1, milestone=0.4, multistep_gamma=0.1, arch='densenet121', dataset='cifar10', init='kaiming_1', save_plot=True, tensorboard=True, loss='CE', method=3, batchnorm=True, deconv=None, delinear=None, block_fc=0, block=64, deconv_iter=5, eps=1e-05, bias=True, stride=3, freeze=False, log_path='checkpoints/cifar10,densenet121,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-14-21.18', channel_deconv=None, start_epoch=0, in_planes=3, input_size=32, classes=('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'), num_outputs=10, train_epoch_logger=<net_util.Logger object at 0x7f1ab580be80>, train_batch_logger=<net_util.Logger object at 0x7f1ab580beb0>, test_epoch_logger=<net_util.Logger object at 0x7f1ab580bfa0>, writer=<torch.utils.tensorboard.writer.SummaryWriter object at 0x7f1ab58540d0>, criterion=CrossEntropyLoss(), logger_n_iter=0)\n",
      "6956298 trainable parameters in the network.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Current learning rate: 0.1\n",
      "training at epoch 1\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Loss: 2.280 | top1: 14.844% ,top5: 61.719%\n",
      "Loss: 2.322 | top1: 16.016% ,top5: 54.297%\n",
      "Loss: 2.336 | top1: 14.583% ,top5: 55.469%\n",
      "Loss: 2.336 | top1: 15.234% ,top5: 58.984%\n",
      "Loss: 2.365 | top1: 14.375% ,top5: 58.750%\n",
      "Loss: 2.345 | top1: 14.583% ,top5: 60.938%\n",
      "Loss: 2.351 | top1: 15.513% ,top5: 62.946%\n",
      "Loss: 2.345 | top1: 16.113% ,top5: 64.160%\n",
      "Loss: 2.327 | top1: 16.319% ,top5: 65.538%\n",
      "Loss: 2.329 | top1: 17.266% ,top5: 67.109%\n",
      "Loss: 2.354 | top1: 18.324% ,top5: 68.182%\n",
      "Loss: 2.369 | top1: 18.164% ,top5: 68.880%\n",
      "Loss: 2.391 | top1: 18.269% ,top5: 69.291%\n",
      "Loss: 2.380 | top1: 18.359% ,top5: 69.866%\n",
      "Loss: 2.362 | top1: 18.698% ,top5: 70.885%\n",
      "Loss: 2.353 | top1: 18.750% ,top5: 71.338%\n",
      "Loss: 2.351 | top1: 18.796% ,top5: 71.645%\n",
      "Loss: 2.344 | top1: 19.227% ,top5: 71.615%\n",
      "Loss: 2.345 | top1: 19.161% ,top5: 71.957%\n",
      "Loss: 2.344 | top1: 19.258% ,top5: 72.500%\n",
      "Loss: 2.335 | top1: 19.531% ,top5: 72.917%\n",
      "Loss: 2.327 | top1: 19.886% ,top5: 73.082%\n",
      "Loss: 2.317 | top1: 19.803% ,top5: 73.268%\n",
      "Loss: 2.302 | top1: 19.922% ,top5: 73.828%\n",
      "Loss: 2.286 | top1: 20.344% ,top5: 74.500%\n",
      "Loss: 2.273 | top1: 20.523% ,top5: 74.940%\n",
      "Loss: 2.260 | top1: 20.775% ,top5: 75.260%\n",
      "Loss: 2.246 | top1: 20.954% ,top5: 75.558%\n",
      "Loss: 2.246 | top1: 21.255% ,top5: 75.889%\n",
      "Loss: 2.238 | top1: 21.615% ,top5: 75.938%\n",
      "Loss: 2.229 | top1: 21.724% ,top5: 76.159%\n",
      "Loss: 2.224 | top1: 22.046% ,top5: 76.367%\n",
      "Loss: 2.217 | top1: 22.112% ,top5: 76.444%\n",
      "Loss: 2.204 | top1: 22.381% ,top5: 76.723%\n",
      "Loss: 2.195 | top1: 22.589% ,top5: 76.897%\n",
      "Loss: 2.186 | top1: 22.852% ,top5: 76.997%\n",
      "Loss: 2.177 | top1: 23.353% ,top5: 77.323%\n",
      "Loss: 2.169 | top1: 23.643% ,top5: 77.549%\n",
      "Loss: 2.160 | top1: 23.978% ,top5: 77.825%\n",
      "Loss: 2.152 | top1: 24.199% ,top5: 77.969%\n",
      "Loss: 2.145 | top1: 24.409% ,top5: 78.125%\n",
      "Loss: 2.135 | top1: 24.535% ,top5: 78.385%\n",
      "Loss: 2.133 | top1: 24.509% ,top5: 78.361%\n",
      "Loss: 2.130 | top1: 24.538% ,top5: 78.427%\n",
      "Loss: 2.122 | top1: 24.653% ,top5: 78.715%\n",
      "Loss: 2.112 | top1: 24.881% ,top5: 78.923%\n",
      "Loss: 2.110 | top1: 25.083% ,top5: 78.923%\n",
      "Loss: 2.104 | top1: 25.098% ,top5: 79.085%\n",
      "Loss: 2.104 | top1: 25.112% ,top5: 79.161%\n",
      "Loss: 2.099 | top1: 25.250% ,top5: 79.250%\n",
      "Loss: 2.096 | top1: 25.230% ,top5: 79.228%\n",
      "Loss: 2.091 | top1: 25.316% ,top5: 79.372%\n",
      "Loss: 2.087 | top1: 25.398% ,top5: 79.466%\n",
      "Loss: 2.085 | top1: 25.492% ,top5: 79.586%\n",
      "Loss: 2.079 | top1: 25.639% ,top5: 79.688%\n",
      "Loss: 2.076 | top1: 25.684% ,top5: 79.813%\n",
      "Loss: 2.071 | top1: 25.781% ,top5: 79.962%\n",
      "Loss: 2.066 | top1: 25.902% ,top5: 80.011%\n",
      "Loss: 2.067 | top1: 26.046% ,top5: 80.058%\n",
      "Loss: 2.060 | top1: 26.133% ,top5: 80.247%\n",
      "Loss: 2.061 | top1: 26.178% ,top5: 80.277%\n",
      "Loss: 2.056 | top1: 26.386% ,top5: 80.406%\n",
      "Loss: 2.052 | top1: 26.438% ,top5: 80.469%\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oruma001/CS895/project/codebase/main.py\", line 422, in <module>\n",
      "    train_net(net,args)\n",
      "  File \"/home/oruma001/CS895/project/codebase/net_util.py\", line 117, in train_net\n",
      "    optimizer.step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 68, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 140, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 23, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/sgd.py\", line 151, in step\n",
      "    sgd(params_with_grad,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/sgd.py\", line 202, in sgd\n",
      "    func(params,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/sgd.py\", line 238, in _single_tensor_sgd\n",
      "    buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch densenet121 --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv False --block-fc 0 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e04873-c477-4f11-8619-94bd5f63341e",
   "metadata": {},
   "source": [
    "#### network deconvolution --- densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d3fb4-171e-4655-9446-8f17a4ad3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch densenet121 --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv True --block-fc 512 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d961a-3dc3-4eec-a287-5a0bc75086b1",
   "metadata": {},
   "source": [
    "# ResNext-29 --- resnext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fbc155-7eb4-42d3-ba88-34b1bbfcb3e4",
   "metadata": {},
   "source": [
    "#### batch normalization --- resnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f12ab91-c7f1-4bfb-819a-dbe46cdb15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch resnext --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv False --block-fc 0 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd52fe-8f5a-4c22-82b7-6cb2d7559507",
   "metadata": {},
   "source": [
    "#### network deconvolution --- resnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7a0e9-bb84-4192-a59f-149102cad2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch resnext --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv True --block-fc 512 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf33ada-6102-460c-9f82-f26c4c3be5d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MobileNet v2 --- mobilev2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715511a4-b2a5-49d0-98ae-9f7571b860ba",
   "metadata": {},
   "source": [
    "#### batch normalization --- mobilev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67511ff3-cda4-4535-a822-d12422c32866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "| Preparing CIFAR-10 dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "2024-03-14 23:43:15.074610: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "==> Building model..\n",
      "Namespace(msg=1, resume='', use_gpu=True, num_workers=16, result_path='checkpoints/cifar10,mobilev2,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-14-23.43', checkpoint_path='checkpoints/cifar10,mobilev2,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-14-23.43', checkpoint_epoch=-1, print_freq=20, seed=0, optimizer='SGD', lr=0.1, lr_scheduler='cosine', momentum=0.9, weight_decay=0.001, batch_size=128, epochs=1, milestone=0.4, multistep_gamma=0.1, arch='mobilev2', dataset='cifar10', init='kaiming_1', save_plot=True, tensorboard=True, loss='CE', method=3, batchnorm=True, deconv=None, delinear=None, block_fc=0, block=64, deconv_iter=5, eps=1e-05, bias=True, stride=3, freeze=False, log_path='checkpoints/cifar10,mobilev2,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-14-23.43', channel_deconv=None, start_epoch=0, in_planes=3, input_size=32, classes=('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'), num_outputs=10, train_epoch_logger=<net_util.Logger object at 0x7f828fa47e20>, train_batch_logger=<net_util.Logger object at 0x7f828fa47e50>, test_epoch_logger=<net_util.Logger object at 0x7f828fa47f40>, writer=<torch.utils.tensorboard.writer.SummaryWriter object at 0x7f828fa90070>, criterion=CrossEntropyLoss(), logger_n_iter=0)\n",
      "2296922 trainable parameters in the network.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Current learning rate: 0.1\n",
      "training at epoch 1\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Loss: 2.287 | top1: 13.281% ,top5: 59.375%\n",
      "Loss: 2.326 | top1: 12.891% ,top5: 55.078%\n",
      "Loss: 2.364 | top1: 12.240% ,top5: 54.167%\n",
      "Loss: 2.377 | top1: 11.133% ,top5: 54.688%\n",
      "Loss: 2.387 | top1: 12.188% ,top5: 56.094%\n",
      "Loss: 2.425 | top1: 12.370% ,top5: 58.073%\n",
      "Loss: 2.441 | top1: 12.277% ,top5: 58.036%\n",
      "Loss: 2.429 | top1: 12.305% ,top5: 59.961%\n",
      "Loss: 2.477 | top1: 12.413% ,top5: 59.115%\n",
      "Loss: 2.473 | top1: 13.125% ,top5: 59.766%\n",
      "Loss: 2.518 | top1: 13.565% ,top5: 59.659%\n",
      "Loss: 2.558 | top1: 13.997% ,top5: 59.375%\n",
      "Loss: 2.553 | top1: 14.483% ,top5: 59.495%\n",
      "Loss: 2.573 | top1: 14.900% ,top5: 59.821%\n",
      "Loss: 2.607 | top1: 15.417% ,top5: 59.531%\n",
      "Loss: 2.615 | top1: 15.381% ,top5: 60.010%\n",
      "Loss: 2.610 | top1: 15.487% ,top5: 60.846%\n",
      "Loss: 2.631 | top1: 15.712% ,top5: 61.024%\n",
      "Loss: 2.628 | top1: 16.036% ,top5: 61.472%\n",
      "Loss: 2.635 | top1: 16.055% ,top5: 61.719%\n",
      "Loss: 2.632 | top1: 16.146% ,top5: 62.426%\n",
      "Loss: 2.650 | top1: 16.335% ,top5: 62.855%\n",
      "Loss: 2.657 | top1: 16.406% ,top5: 62.840%\n",
      "Loss: 2.655 | top1: 16.602% ,top5: 63.216%\n",
      "Loss: 2.649 | top1: 16.656% ,top5: 63.719%\n",
      "Loss: 2.649 | top1: 16.917% ,top5: 63.882%\n",
      "Loss: 2.656 | top1: 17.101% ,top5: 64.062%\n",
      "Loss: 2.650 | top1: 17.076% ,top5: 64.118%\n",
      "Loss: 2.653 | top1: 17.241% ,top5: 64.494%\n",
      "Loss: 2.656 | top1: 17.240% ,top5: 64.948%\n",
      "Loss: 2.640 | top1: 17.515% ,top5: 65.323%\n",
      "Loss: 2.623 | top1: 17.578% ,top5: 65.845%\n",
      "Loss: 2.611 | top1: 17.661% ,top5: 66.146%\n",
      "Loss: 2.602 | top1: 17.624% ,top5: 66.452%\n",
      "Loss: 2.591 | top1: 17.946% ,top5: 66.741%\n",
      "Loss: 2.580 | top1: 17.990% ,top5: 67.036%\n",
      "Loss: 2.572 | top1: 18.011% ,top5: 67.441%\n",
      "Loss: 2.558 | top1: 18.133% ,top5: 67.825%\n",
      "Loss: 2.553 | top1: 18.229% ,top5: 68.029%\n",
      "Loss: 2.545 | top1: 18.457% ,top5: 68.262%\n",
      "Loss: 2.537 | top1: 18.502% ,top5: 68.559%\n",
      "Loss: 2.525 | top1: 18.471% ,top5: 68.899%\n",
      "Loss: 2.519 | top1: 18.423% ,top5: 68.950%\n",
      "Loss: 2.505 | top1: 18.537% ,top5: 69.318%\n",
      "Loss: 2.493 | top1: 18.507% ,top5: 69.566%\n",
      "Loss: 2.484 | top1: 18.444% ,top5: 69.803%\n",
      "Loss: 2.471 | top1: 18.650% ,top5: 70.096%\n",
      "Loss: 2.460 | top1: 18.815% ,top5: 70.247%\n",
      "Loss: 2.450 | top1: 19.005% ,top5: 70.392%\n",
      "Loss: 2.439 | top1: 19.203% ,top5: 70.594%\n",
      "Loss: 2.429 | top1: 19.317% ,top5: 70.772%\n",
      "Loss: 2.420 | top1: 19.381% ,top5: 70.898%\n",
      "Loss: 2.410 | top1: 19.590% ,top5: 71.050%\n",
      "Loss: 2.404 | top1: 19.690% ,top5: 71.137%\n",
      "Loss: 2.395 | top1: 19.759% ,top5: 71.392%\n",
      "Loss: 2.386 | top1: 19.713% ,top5: 71.624%\n",
      "Loss: 2.379 | top1: 19.696% ,top5: 71.711%\n",
      "Loss: 2.370 | top1: 19.720% ,top5: 71.983%\n",
      "Loss: 2.362 | top1: 19.849% ,top5: 72.180%\n",
      "Loss: 2.353 | top1: 20.013% ,top5: 72.409%\n",
      "Loss: 2.345 | top1: 20.095% ,top5: 72.567%\n",
      "Loss: 2.338 | top1: 20.098% ,top5: 72.757%\n",
      "Loss: 2.332 | top1: 20.102% ,top5: 72.855%\n",
      "Loss: 2.325 | top1: 20.215% ,top5: 73.022%\n",
      "Loss: 2.320 | top1: 20.240% ,top5: 73.137%\n",
      "Loss: 2.313 | top1: 20.265% ,top5: 73.295%\n",
      "Loss: 2.307 | top1: 20.394% ,top5: 73.496%\n",
      "Loss: 2.302 | top1: 20.554% ,top5: 73.564%\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch mobilev2 --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv False --block-fc 0 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186548c-271d-45e9-884c-e7c63d0a1c3e",
   "metadata": {},
   "source": [
    "#### network deconvolution --- mobilev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db48a65-14df-445b-897f-813c8efb11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch mobilev2 --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv True --block-fc 512 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586bd1c-385b-4cb5-8ca5-33c96613564d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DPN-92 --- dpn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76da62d-7bdc-40b0-a1f3-cddae66bd0af",
   "metadata": {},
   "source": [
    "#### batch normalization --- dpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f9fb19-23a1-4be4-827a-2a68fc3f3377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "| Preparing CIFAR-10 dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "2024-03-15 00:41:32.501297: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "==> Building model..\n",
      "Namespace(msg=1, resume='', use_gpu=True, num_workers=16, result_path='checkpoints/cifar10,dpn,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-15-00.41', checkpoint_path='checkpoints/cifar10,dpn,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-15-00.41', checkpoint_epoch=-1, print_freq=20, seed=0, optimizer='SGD', lr=0.1, lr_scheduler='cosine', momentum=0.9, weight_decay=0.001, batch_size=128, epochs=1, milestone=0.4, multistep_gamma=0.1, arch='dpn', dataset='cifar10', init='kaiming_1', save_plot=True, tensorboard=True, loss='CE', method=3, batchnorm=True, deconv=None, delinear=None, block_fc=0, block=64, deconv_iter=5, eps=1e-05, bias=True, stride=3, freeze=False, log_path='checkpoints/cifar10,dpn,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-15-00.41', channel_deconv=None, start_epoch=0, in_planes=3, input_size=32, classes=('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'), num_outputs=10, train_epoch_logger=<net_util.Logger object at 0x7f0d6cb080a0>, train_batch_logger=<net_util.Logger object at 0x7f0d6cb08100>, test_epoch_logger=<net_util.Logger object at 0x7f0d6cb081f0>, writer=<torch.utils.tensorboard.writer.SummaryWriter object at 0x7f0d6cb082e0>, criterion=CrossEntropyLoss(), logger_n_iter=0)\n",
      "34236634 trainable parameters in the network.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Current learning rate: 0.1\n",
      "training at epoch 1\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Loss: 2.376 | top1: 14.844% ,top5: 60.156%\n",
      "Loss: 7.026 | top1: 10.547% ,top5: 53.125%\n",
      "Loss: 12.837 | top1: 11.198% ,top5: 53.125%\n",
      "Loss: 16.446 | top1: 11.133% ,top5: 52.344%\n",
      "Loss: 17.107 | top1: 10.469% ,top5: 50.312%\n",
      "Loss: 16.295 | top1: 10.807% ,top5: 49.870%\n",
      "Loss: 14.926 | top1: 11.049% ,top5: 50.112%\n",
      "Loss: 16.174 | top1: 10.840% ,top5: 50.098%\n",
      "Loss: 18.455 | top1: 10.677% ,top5: 50.347%\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oruma001/CS895/project/codebase/main.py\", line 425, in <module>\n",
      "    train_net(net,args)\n",
      "  File \"/home/oruma001/CS895/project/codebase/net_util.py\", line 106, in train_net\n",
      "    losses.update(loss.item(), inputs.size(0))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch dpn --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv False --block-fc 0 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef0a4d5-7586-4f41-a766-af2a1c1c2a3f",
   "metadata": {},
   "source": [
    "#### network deconvolution --- dpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6391fa4-1212-4967-b3cf-68251f20307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch dpn --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv True --block-fc 512 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228ce48-88ac-483d-81e5-3e6bb1aea968",
   "metadata": {},
   "source": [
    "# PNASNetA --- pnasnetA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2cac59-70c2-475f-b147-a7dd0ef6d21a",
   "metadata": {},
   "source": [
    "#### batch normalization --- pnasnetA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea18eba-a09b-447e-8882-4ae0a59e14a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch pnasnetA --epochs 1 --dataset cifar10  --batch-size 128 --msg True --deconv False --block-fc 0 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77fb0de-93c6-409a-ac9f-e76079550d7c",
   "metadata": {},
   "source": [
    "#### network deconvolution --- pnasnetA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a6fa75-26e7-4df5-8d85-498057579c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch pnasnetA --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv True --block-fc 512 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4bdbd4-6ea6-4a38-80f2-dfcd50c2fb86",
   "metadata": {},
   "source": [
    "# SENet-18 --- senet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8dee0-0693-48c1-ad94-573c5b4e4481",
   "metadata": {},
   "source": [
    "#### batch normalization --- senet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8935dbe6-23ee-4651-95ea-9695c8b5c9d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "| Preparing CIFAR-10 dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "2024-03-15 18:36:45.236938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "==> Building model..\n",
      "Namespace(msg=1, resume='', use_gpu=True, num_workers=16, result_path='checkpoints/cifar10,senet,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-15-18.36', checkpoint_path='checkpoints/cifar10,senet,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-15-18.36', checkpoint_epoch=-1, print_freq=20, seed=0, optimizer='SGD', lr=0.1, lr_scheduler='cosine', momentum=0.9, weight_decay=0.001, batch_size=128, epochs=1, milestone=0.4, multistep_gamma=0.1, arch='senet', dataset='cifar10', init='kaiming_1', save_plot=True, tensorboard=True, loss='CE', method=3, batchnorm=True, deconv=None, delinear=None, block_fc=0, block=64, deconv_iter=5, eps=1e-05, bias=True, stride=3, freeze=False, log_path='checkpoints/cifar10,senet,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-15-18.36', channel_deconv=None, start_epoch=0, in_planes=3, input_size=32, classes=('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'), num_outputs=10, train_epoch_logger=<net_util.Logger object at 0x7fb00107bf10>, train_batch_logger=<net_util.Logger object at 0x7fb00111c0d0>, test_epoch_logger=<net_util.Logger object at 0x7fb00111c1c0>, writer=<torch.utils.tensorboard.writer.SummaryWriter object at 0x7fb00111c2b0>, criterion=CrossEntropyLoss(), logger_n_iter=0)\n",
      "11260354 trainable parameters in the network.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Current learning rate: 0.1\n",
      "training at epoch 1\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "Loss: 2.315 | top1: 10.156% ,top5: 53.125%\n",
      "Loss: 2.287 | top1: 11.328% ,top5: 56.250%\n",
      "Loss: 2.281 | top1: 11.719% ,top5: 58.333%\n",
      "Loss: 2.279 | top1: 12.109% ,top5: 60.352%\n",
      "Loss: 2.253 | top1: 13.750% ,top5: 62.500%\n",
      "Loss: 2.230 | top1: 15.495% ,top5: 64.193%\n",
      "Loss: 2.210 | top1: 17.299% ,top5: 65.848%\n",
      "Loss: 2.222 | top1: 17.285% ,top5: 66.699%\n",
      "Loss: 2.211 | top1: 17.795% ,top5: 67.274%\n",
      "Loss: 2.215 | top1: 17.734% ,top5: 68.438%\n",
      "Loss: 2.216 | top1: 17.045% ,top5: 69.034%\n",
      "Loss: 2.234 | top1: 17.122% ,top5: 69.010%\n",
      "Loss: 2.214 | top1: 18.329% ,top5: 69.471%\n",
      "Loss: 2.209 | top1: 18.583% ,top5: 69.531%\n",
      "Loss: 2.193 | top1: 19.479% ,top5: 70.000%\n",
      "Loss: 2.175 | top1: 20.117% ,top5: 70.752%\n",
      "Loss: 2.181 | top1: 19.991% ,top5: 70.772%\n",
      "Loss: 2.180 | top1: 19.705% ,top5: 71.181%\n",
      "Loss: 2.177 | top1: 19.531% ,top5: 71.423%\n",
      "Loss: 2.185 | top1: 19.219% ,top5: 71.602%\n",
      "Loss: 2.185 | top1: 19.085% ,top5: 71.615%\n",
      "Loss: 2.174 | top1: 19.425% ,top5: 72.195%\n",
      "Loss: 2.173 | top1: 19.667% ,top5: 72.045%\n",
      "Loss: 2.172 | top1: 19.564% ,top5: 72.135%\n",
      "Loss: 2.162 | top1: 19.844% ,top5: 72.531%\n",
      "Loss: 2.156 | top1: 20.343% ,top5: 72.837%\n",
      "Loss: 2.152 | top1: 20.631% ,top5: 72.917%\n",
      "Loss: 2.149 | top1: 20.647% ,top5: 73.242%\n",
      "Loss: 2.139 | top1: 20.878% ,top5: 73.438%\n",
      "Loss: 2.135 | top1: 20.938% ,top5: 73.672%\n",
      "Loss: 2.142 | top1: 20.691% ,top5: 73.488%\n",
      "Loss: 2.140 | top1: 20.776% ,top5: 73.535%\n",
      "Loss: 2.134 | top1: 20.928% ,top5: 73.627%\n",
      "Loss: 2.128 | top1: 21.163% ,top5: 73.851%\n",
      "Loss: 2.122 | top1: 21.295% ,top5: 74.129%\n",
      "Loss: 2.120 | top1: 21.354% ,top5: 74.197%\n",
      "Loss: 2.121 | top1: 21.453% ,top5: 74.177%\n",
      "Loss: 2.114 | top1: 21.669% ,top5: 74.404%\n",
      "Loss: 2.108 | top1: 21.875% ,top5: 74.740%\n",
      "Loss: 2.107 | top1: 21.973% ,top5: 74.785%\n",
      "Loss: 2.104 | top1: 22.008% ,top5: 74.924%\n",
      "Loss: 2.097 | top1: 22.098% ,top5: 75.149%\n",
      "Loss: 2.094 | top1: 22.202% ,top5: 75.309%\n",
      "Loss: 2.093 | top1: 22.159% ,top5: 75.355%\n",
      "Loss: 2.089 | top1: 22.274% ,top5: 75.521%\n",
      "Loss: 2.082 | top1: 22.503% ,top5: 75.747%\n",
      "Loss: 2.080 | top1: 22.557% ,top5: 75.914%\n",
      "Loss: 2.070 | top1: 22.835% ,top5: 76.204%\n",
      "Loss: 2.066 | top1: 22.927% ,top5: 76.339%\n",
      "Loss: 2.062 | top1: 23.078% ,top5: 76.469%\n",
      "Loss: 2.054 | top1: 23.300% ,top5: 76.700%\n",
      "Loss: 2.050 | top1: 23.483% ,top5: 76.863%\n",
      "Loss: 2.046 | top1: 23.629% ,top5: 77.005%\n",
      "Loss: 2.041 | top1: 23.828% ,top5: 77.257%\n",
      "Loss: 2.038 | top1: 23.849% ,top5: 77.372%\n",
      "Loss: 2.035 | top1: 23.912% ,top5: 77.483%\n",
      "Loss: 2.032 | top1: 23.986% ,top5: 77.645%\n",
      "Loss: 2.028 | top1: 24.151% ,top5: 77.788%\n",
      "Loss: 2.023 | top1: 24.285% ,top5: 77.940%\n",
      "Loss: 2.020 | top1: 24.388% ,top5: 78.008%\n",
      "Loss: 2.017 | top1: 24.488% ,top5: 78.189%\n",
      "Loss: 2.014 | top1: 24.660% ,top5: 78.352%\n",
      "Loss: 2.015 | top1: 24.566% ,top5: 78.423%\n",
      "Loss: 2.012 | top1: 24.683% ,top5: 78.516%\n",
      "Loss: 2.008 | top1: 24.748% ,top5: 78.690%\n",
      "Loss: 2.005 | top1: 24.787% ,top5: 78.800%\n",
      "Loss: 2.001 | top1: 24.848% ,top5: 78.871%\n",
      "Loss: 1.999 | top1: 24.931% ,top5: 78.929%\n",
      "Loss: 1.996 | top1: 25.079% ,top5: 79.031%\n",
      "Loss: 1.992 | top1: 25.212% ,top5: 79.074%\n",
      "Loss: 1.989 | top1: 25.231% ,top5: 79.170%\n",
      "Loss: 1.987 | top1: 25.336% ,top5: 79.253%\n",
      "Loss: 1.984 | top1: 25.492% ,top5: 79.409%\n",
      "Loss: 1.981 | top1: 25.591% ,top5: 79.508%\n",
      "Loss: 1.977 | top1: 25.802% ,top5: 79.594%\n",
      "Loss: 1.973 | top1: 25.935% ,top5: 79.667%\n",
      "Loss: 1.971 | top1: 26.096% ,top5: 79.667%\n",
      "Loss: 1.969 | top1: 26.162% ,top5: 79.798%\n",
      "Loss: 1.966 | top1: 26.246% ,top5: 79.925%\n",
      "Loss: 1.965 | top1: 26.250% ,top5: 80.039%\n",
      "Loss: 1.961 | top1: 26.292% ,top5: 80.131%\n",
      "Loss: 1.958 | top1: 26.401% ,top5: 80.231%\n",
      "Loss: 1.955 | top1: 26.506% ,top5: 80.328%\n",
      "Loss: 1.954 | top1: 26.600% ,top5: 80.404%\n",
      "Loss: 1.949 | top1: 26.783% ,top5: 80.551%\n",
      "Loss: 1.946 | top1: 26.871% ,top5: 80.623%\n",
      "Loss: 1.944 | top1: 26.922% ,top5: 80.747%\n",
      "Loss: 1.940 | top1: 27.051% ,top5: 80.806%\n",
      "Loss: 1.939 | top1: 27.045% ,top5: 80.873%\n",
      "Loss: 1.935 | top1: 27.144% ,top5: 80.990%\n",
      "Loss: 1.934 | top1: 27.172% ,top5: 81.104%\n",
      "Loss: 1.931 | top1: 27.250% ,top5: 81.165%\n",
      "Loss: 1.928 | top1: 27.403% ,top5: 81.258%\n",
      "Loss: 1.927 | top1: 27.502% ,top5: 81.292%\n",
      "Loss: 1.925 | top1: 27.590% ,top5: 81.373%\n",
      "Loss: 1.924 | top1: 27.620% ,top5: 81.494%\n",
      "Loss: 1.923 | top1: 27.714% ,top5: 81.556%\n",
      "Loss: 1.921 | top1: 27.790% ,top5: 81.617%\n",
      "Loss: 1.919 | top1: 27.825% ,top5: 81.684%\n",
      "Loss: 1.916 | top1: 27.953% ,top5: 81.695%\n",
      "Loss: 1.914 | top1: 27.970% ,top5: 81.722%\n",
      "Loss: 1.912 | top1: 28.041% ,top5: 81.817%\n",
      "Loss: 1.910 | top1: 28.095% ,top5: 81.872%\n",
      "Loss: 1.909 | top1: 28.185% ,top5: 81.949%\n",
      "Loss: 1.907 | top1: 28.199% ,top5: 82.024%\n",
      "Loss: 1.905 | top1: 28.243% ,top5: 82.068%\n",
      "Loss: 1.902 | top1: 28.344% ,top5: 82.163%\n",
      "Loss: 1.900 | top1: 28.458% ,top5: 82.234%\n",
      "Loss: 1.899 | top1: 28.541% ,top5: 82.304%\n",
      "Loss: 1.897 | top1: 28.537% ,top5: 82.393%\n",
      "Loss: 1.894 | top1: 28.604% ,top5: 82.475%\n",
      "Loss: 1.893 | top1: 28.613% ,top5: 82.506%\n",
      "Loss: 1.890 | top1: 28.706% ,top5: 82.605%\n",
      "Loss: 1.889 | top1: 28.749% ,top5: 82.648%\n",
      "Loss: 1.888 | top1: 28.804% ,top5: 82.683%\n",
      "Loss: 1.886 | top1: 28.852% ,top5: 82.765%\n",
      "Loss: 1.884 | top1: 28.913% ,top5: 82.812%\n",
      "Loss: 1.882 | top1: 29.012% ,top5: 82.846%\n",
      "Loss: 1.880 | top1: 29.110% ,top5: 82.858%\n",
      "Loss: 1.878 | top1: 29.186% ,top5: 82.923%\n",
      "Loss: 1.877 | top1: 29.261% ,top5: 82.948%\n",
      "Loss: 1.875 | top1: 29.355% ,top5: 82.998%\n",
      "Loss: 1.872 | top1: 29.433% ,top5: 83.060%\n",
      "Loss: 1.869 | top1: 29.568% ,top5: 83.128%\n",
      "Loss: 1.868 | top1: 29.631% ,top5: 83.175%\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oruma001/CS895/project/codebase/main.py\", line 425, in <module>\n",
      "    train_net(net,args)\n",
      "  File \"/home/oruma001/CS895/project/codebase/net_util.py\", line 91, in train_net\n",
      "    outputs = net(inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 169, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/oruma001/CS895/project/codebase/models/senet.py\", line 160, in forward\n",
      "    out = self.layer2(out)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n",
      "    input = module(input)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/oruma001/CS895/project/codebase/models/senet.py\", line 111, in forward\n",
      "    w = F.relu(self.fc1(w))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch senet --epochs 1 --dataset cifar10  --batch-size 128 --msg True --deconv False --block-fc 0 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc60ef-9373-4105-93ab-828063f176ec",
   "metadata": {},
   "source": [
    "#### network deconvolution --- senet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e9a87-d58e-4729-b5b5-68197752509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch senet --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv True --block-fc 512 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410856f2-83fc-4c36-9c85-7c35a6491c89",
   "metadata": {},
   "source": [
    "# EfficientNet --- efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab6782-03a4-4bdc-b46b-ed3e48a47960",
   "metadata": {},
   "source": [
    "#### batch normalization --- efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c378288b-e2a0-4198-bce7-0243c73bd9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "| Preparing CIFAR-10 dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "2024-03-15 18:38:18.327308: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "==> Building model..\n",
      "Namespace(msg=1, resume='', use_gpu=True, num_workers=16, result_path='checkpoints/cifar10,efficient,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-15-18.38', checkpoint_path='checkpoints/cifar10,efficient,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-15-18.38', checkpoint_epoch=-1, print_freq=20, seed=0, optimizer='SGD', lr=0.1, lr_scheduler='cosine', momentum=0.9, weight_decay=0.001, batch_size=128, epochs=1, milestone=0.4, multistep_gamma=0.1, arch='efficient', dataset='cifar10', init='kaiming_1', save_plot=True, tensorboard=True, loss='CE', method=3, batchnorm=True, deconv=None, delinear=None, block_fc=0, block=64, deconv_iter=5, eps=1e-05, bias=True, stride=3, freeze=False, log_path='checkpoints/cifar10,efficient,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-15-18.38', channel_deconv=None, start_epoch=0, in_planes=3, input_size=32, classes=('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'), num_outputs=10, train_epoch_logger=<net_util.Logger object at 0x7f8fcb5a7f10>, train_batch_logger=<net_util.Logger object at 0x7f8fcb6600d0>, test_epoch_logger=<net_util.Logger object at 0x7f8fcb6601c0>, writer=<torch.utils.tensorboard.writer.SummaryWriter object at 0x7f8fcb6602b0>, criterion=CrossEntropyLoss(), logger_n_iter=0)\n",
      "2912089 trainable parameters in the network.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Current learning rate: 0.1\n",
      "training at epoch 1\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Loss: 2.605 | top1: 11.719% ,top5: 52.344%\n",
      "Loss: 2.650 | top1: 12.109% ,top5: 53.125%\n",
      "Loss: 2.813 | top1: 11.719% ,top5: 54.948%\n",
      "Loss: 2.967 | top1: 12.109% ,top5: 55.273%\n",
      "Loss: 3.238 | top1: 11.719% ,top5: 56.562%\n",
      "Loss: 3.242 | top1: 12.109% ,top5: 57.812%\n",
      "Loss: 3.424 | top1: 12.723% ,top5: 57.701%\n",
      "Loss: 3.538 | top1: 13.086% ,top5: 56.738%\n",
      "Loss: 3.451 | top1: 13.281% ,top5: 57.378%\n",
      "Loss: 3.584 | top1: 13.984% ,top5: 57.969%\n",
      "Loss: 3.518 | top1: 13.991% ,top5: 57.386%\n",
      "Loss: 3.438 | top1: 13.932% ,top5: 57.161%\n",
      "Loss: 3.428 | top1: 13.762% ,top5: 56.611%\n",
      "Loss: 3.499 | top1: 14.007% ,top5: 56.529%\n",
      "Loss: 3.471 | top1: 14.115% ,top5: 57.188%\n",
      "Loss: 3.422 | top1: 14.404% ,top5: 57.666%\n",
      "Loss: 3.403 | top1: 14.614% ,top5: 57.904%\n",
      "Loss: 3.408 | top1: 14.627% ,top5: 58.116%\n",
      "Loss: 3.420 | top1: 14.515% ,top5: 58.306%\n",
      "Loss: 3.442 | top1: 14.531% ,top5: 58.984%\n",
      "Loss: 3.403 | top1: 14.844% ,top5: 59.338%\n",
      "Loss: 3.363 | top1: 14.915% ,top5: 59.730%\n",
      "Loss: 3.382 | top1: 14.946% ,top5: 60.156%\n",
      "Loss: 3.333 | top1: 15.299% ,top5: 60.775%\n",
      "Loss: 3.362 | top1: 15.344% ,top5: 60.875%\n",
      "Loss: 3.433 | top1: 15.355% ,top5: 60.968%\n",
      "Loss: 3.529 | top1: 15.365% ,top5: 60.938%\n",
      "Loss: 3.564 | top1: 15.179% ,top5: 60.965%\n",
      "Loss: 3.536 | top1: 15.275% ,top5: 60.938%\n",
      "Loss: 3.517 | top1: 15.339% ,top5: 61.224%\n",
      "Loss: 3.590 | top1: 15.071% ,top5: 61.013%\n",
      "Loss: 3.622 | top1: 14.941% ,top5: 60.645%\n",
      "Loss: 3.741 | top1: 14.796% ,top5: 60.346%\n",
      "Loss: 3.797 | top1: 14.545% ,top5: 60.087%\n",
      "Loss: 3.935 | top1: 14.330% ,top5: 59.799%\n",
      "Loss: 3.930 | top1: 14.193% ,top5: 59.462%\n",
      "Loss: 3.950 | top1: 14.210% ,top5: 59.333%\n",
      "Loss: 3.989 | top1: 14.062% ,top5: 58.923%\n",
      "Loss: 3.993 | top1: 14.083% ,top5: 58.574%\n",
      "Loss: 3.978 | top1: 14.004% ,top5: 58.691%\n",
      "Loss: 3.966 | top1: 14.043% ,top5: 58.727%\n",
      "Loss: 3.955 | top1: 14.081% ,top5: 58.612%\n",
      "Loss: 3.934 | top1: 14.008% ,top5: 58.394%\n",
      "Loss: 3.973 | top1: 13.991% ,top5: 58.239%\n",
      "Loss: 4.050 | top1: 13.906% ,top5: 58.021%\n",
      "Loss: 4.077 | top1: 13.842% ,top5: 57.846%\n",
      "Loss: 4.077 | top1: 13.797% ,top5: 57.746%\n",
      "Loss: 4.077 | top1: 13.688% ,top5: 57.633%\n",
      "Loss: 4.087 | top1: 13.648% ,top5: 57.510%\n",
      "Loss: 4.084 | top1: 13.609% ,top5: 57.375%\n",
      "Loss: 4.068 | top1: 13.526% ,top5: 57.230%\n",
      "Loss: 4.049 | top1: 13.477% ,top5: 57.106%\n",
      "Loss: 4.026 | top1: 13.399% ,top5: 56.840%\n",
      "Loss: 4.019 | top1: 13.267% ,top5: 56.597%\n",
      "Loss: 4.011 | top1: 13.210% ,top5: 56.449%\n",
      "Loss: 3.999 | top1: 13.142% ,top5: 56.403%\n",
      "Loss: 3.980 | top1: 13.144% ,top5: 56.401%\n",
      "Loss: 3.962 | top1: 13.093% ,top5: 56.385%\n",
      "Loss: 3.955 | top1: 12.963% ,top5: 56.144%\n",
      "Loss: 3.937 | top1: 12.956% ,top5: 56.172%\n",
      "Loss: 3.925 | top1: 12.923% ,top5: 56.148%\n",
      "Loss: 3.913 | top1: 12.916% ,top5: 56.237%\n",
      "Loss: 3.903 | top1: 12.860% ,top5: 56.250%\n",
      "Loss: 3.880 | top1: 12.817% ,top5: 56.238%\n",
      "Loss: 3.869 | top1: 12.788% ,top5: 56.058%\n",
      "Loss: 3.848 | top1: 12.867% ,top5: 56.143%\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oruma001/CS895/project/codebase/main.py\", line 425, in <module>\n",
      "    train_net(net,args)\n",
      "  File \"/home/oruma001/CS895/project/codebase/net_util.py\", line 115, in train_net\n",
      "    loss.backward()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch efficient --epochs 1 --dataset cifar10  --batch-size 128 --msg True --deconv False --block-fc 0 --wd .001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5e746-4297-4588-a6d5-6262d88c4126",
   "metadata": {},
   "source": [
    "#### network deconvolution --- efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ef131-1f20-4ee6-8104-47d64903516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch efficient --epochs 20 --dataset cifar10  --batch-size 128 --msg True --deconv True --block-fc 512 --wd .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6752884e-479e-4c43-9289-9f93acf0d352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faf2eada-f758-4667-a51a-354c321dc63c",
   "metadata": {},
   "source": [
    "Attempt 1\n",
    "\n",
    "sbatch --export=ALL,architecture='vgg16',epochs=100 -o vgg16_100_att1_BN.txt single_experiment.sh\n",
    "sbatch --export=ALL,architecture='vgg16',epochs=100 -o vgg16_100_att1_dconv.txt single_experiment_net_deconv.sh\n",
    "\n",
    "\n",
    "sbatch --export=ALL,architecture='resnet18d',epochs=100 -o resnet18d_100_att1_BN.txt single_experiment.sh\n",
    "sbatch --export=ALL,architecture='resnet18d',epochs=100 -o resnet18d_100_att1_dconv.txt single_experiment_net_deconv.sh\n",
    "\n",
    "\n",
    "sbatch --export=ALL,architecture='preact',epochs=100 -o preact_100_att1_BN.txt single_experiment.sh\n",
    "sbatch --export=ALL,architecture='preact',epochs=100 -o preact_100_att1_dconv.txt single_experiment_net_deconv.sh\n",
    "\n",
    "\n",
    "sbatch --export=ALL,architecture='densenet121',epochs=100 -o densenet121_100_att1_BN.txt single_experiment.sh\n",
    "sbatch --export=ALL,architecture='densenet121',epochs=100 -o densenet121_100_att1_dconv.txt single_experiment_net_deconv.sh\n",
    "\n",
    "\n",
    "sbatch --export=ALL,architecture='resnext',epochs=100 -o resnext_100_att1_BN.txt single_experiment.sh\n",
    "sbatch --export=ALL,architecture='resnext',epochs=100 -o resnext_100_att1_dconv.txt single_experiment_net_deconv.sh\n",
    "\n",
    "\n",
    "sbatch --export=ALL,architecture='mobilev2',epochs=100 -o mobilev2_100_att1_BN.txt single_experiment.sh\n",
    "sbatch --export=ALL,architecture='mobilev2',epochs=100 -o mobilev2_100_att1_dconv.txt single_experiment_net_deconv.sh\n",
    "\n",
    "\n",
    "sbatch --export=ALL,architecture='dpn',epochs=100 -o dpn_100_att1_BN.txt single_experiment.sh\n",
    "sbatch --export=ALL,architecture='dpn',epochs=100 -o dpn_100_att1_dconv.txt single_experiment_net_deconv.sh\n",
    "\n",
    "\n",
    "sbatch --export=ALL,architecture='pnasnetA',epochs=100 -o pnasnetA_100_att1_BN.txt single_experiment.sh\n",
    "sbatch --export=ALL,architecture='pnasnetA',epochs=100 -o pnasnetA_100_att1_dconv.txt single_experiment_net_deconv.sh\n",
    "\n",
    "\n",
    "sbatch --export=ALL,architecture='senet',epochs=100 -o senet_100_att1_BN.txt single_experiment.sh\n",
    "sbatch --export=ALL,architecture='senet',epochs=100 -o senet_100_att1_dconv.txt single_experiment_net_deconv.sh\n",
    "\n",
    "\n",
    "sbatch --export=ALL,architecture='efficient',epochs=100 -o efficient_100_att1_BN.txt single_experiment.sh\n",
    "sbatch --export=ALL,architecture='efficient',epochs=100 -o efficient_100_att1_dconv.txt single_experiment_net_deconv.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d952a8-0d6d-47bf-810f-13f27640d12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar10,densenet121,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,densenet121,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,densenet121,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,densenet121,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,dpn,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,dpn,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,dpn,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,dpn,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,dpn,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,efficient,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,efficient,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,efficient,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,efficient,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,mobilev2,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,mobilev2,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,mobilev2,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,mobilev2,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,mobilev2,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,pnasnetA,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,pnasnetA,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,pnasnetA,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,pnasnetA,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,preact,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,preact,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,preact,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,preact,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,resnet,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,resnet,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,resnet18d,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,resnet18d,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,resnet18d,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,resnext,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,resnext,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,resnext,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,resnext,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,resnext,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,senet,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,senet,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,senet,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,senet,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,vgg16,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,vgg16,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,vgg16,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,vgg16,ep.100,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n",
      "cifar10,vgg16,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0\n",
      "cifar10,vgg16,ep.20,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.1,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.512\n"
     ]
    }
   ],
   "source": [
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e989bb1-c8eb-4b81-a514-236d9d626aae",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac19782b-1789-40ae-8c1d-8accbaaa95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# file_list = glob.glob('checkpoints/data?.txt')\n",
    "file_list_ = glob.glob('checkpoints/*ep.20*')\n",
    "file_list = sorted(file_list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83aa5aa0-8f68-4604-abdb-28e74a15729e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 densenet121---BN\n",
      "Attempt 1\n",
      "densenet121 - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 98.32 - time: 1174.68\n",
      "densenet121 - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 93.1 - time: 111.86\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "densenet121 - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 98.108 - time: 1132.57\n",
      "densenet121 - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 93.02 - time: 91.34\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "densenet121 - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 98.304 - time: 1162.58\n",
      "densenet121 - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 93.21 - time: 97.98\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2 densenet121---deconv.1\n",
      "Attempt 1\n",
      "densenet121 - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 99.402 - time: 5028.65\n",
      "densenet121 - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 94.96 - time: 132.67\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "densenet121 - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 99.42 - time: 4630.60\n",
      "densenet121 - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 94.89 - time: 125.93\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "densenet121 - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 99.416 - time: 4718.71\n",
      "densenet121 - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 94.75 - time: 126.13\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3 dpn---BN\n",
      "Attempt 1\n",
      "dpn - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 95.506 - time: 2750.28\n",
      "dpn - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 91.59 - time: 179.18\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "dpn - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 96.052 - time: 2780.88\n",
      "dpn - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 92.19 - time: 178.25\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "dpn - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 95.35 - time: 2772.89\n",
      "dpn - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 91.59 - time: 181.87\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4 dpn---deconv.1\n",
      "Attempt 1\n",
      "dpn - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 98.586 - time: 6567.51\n",
      "dpn - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 93.7 - time: 196.55\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "dpn - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 98.656 - time: 6534.73\n",
      "dpn - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 93.47 - time: 176.97\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "dpn - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 98.584 - time: 6397.82\n",
      "dpn - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 93.47 - time: 173.28\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 efficient---BN\n",
      "Attempt 1\n",
      "efficient - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 86.844 - time: 539.96\n",
      "efficient - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 85.52 - time: 48.22\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "efficient - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 85.498 - time: 541.57\n",
      "efficient - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 84.18 - time: 50.30\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "efficient - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 86.004 - time: 569.22\n",
      "efficient - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 84.34 - time: 48.84\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6 efficient---deconv.1\n",
      "Attempt 1\n",
      "efficient - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 88.776 - time: 568.10\n",
      "efficient - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 87.45 - time: 50.71\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "efficient - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 88.88 - time: 545.68\n",
      "efficient - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 87.76 - time: 49.01\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "efficient - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 89.22 - time: 621.97\n",
      "efficient - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 88.09 - time: 50.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7 mobilev2---BN\n",
      "Attempt 1\n",
      "mobilev2 - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 93.42 - time: 451.53\n",
      "mobilev2 - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 90.73 - time: 65.72\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "mobilev2 - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 93.356 - time: 521.64\n",
      "mobilev2 - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 90.67 - time: 69.50\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "mobilev2 - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 93.244 - time: 503.55\n",
      "mobilev2 - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 90.71 - time: 76.06\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8 mobilev2---deconv.1\n",
      "Attempt 1\n",
      "mobilev2 - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 95.508 - time: 1810.48\n",
      "mobilev2 - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 92.22 - time: 81.40\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "mobilev2 - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 95.57 - time: 1766.51\n",
      "mobilev2 - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 92.07 - time: 61.81\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "mobilev2 - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 95.71 - time: 1637.50\n",
      "mobilev2 - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 92.03 - time: 56.76\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9 pnasnetA---BN\n",
      "Attempt 1\n",
      "pnasnetA - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 65.386 - time: 369.46\n",
      "pnasnetA - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 65.66 - time: 45.06\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "pnasnetA - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 46.346 - time: 370.00\n",
      "pnasnetA - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 46.31 - time: 46.03\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "pnasnetA - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 65.346 - time: 369.91\n",
      "pnasnetA - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 64.56 - time: 46.17\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10 pnasnetA---deconv.1\n",
      "Attempt 1\n",
      "pnasnetA - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 83.32 - time: 1595.45\n",
      "pnasnetA - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 81.94 - time: 42.82\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "pnasnetA - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 83.348 - time: 1591.27\n",
      "pnasnetA - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 81.45 - time: 46.74\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "pnasnetA - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 83.24 - time: 1592.78\n",
      "pnasnetA - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 81.44 - time: 45.45\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11 preact---BN\n",
      "Attempt 1\n",
      "preact - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 93.296 - time: 636.06\n",
      "preact - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 89.65 - time: 101.83\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "preact - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 94.27 - time: 592.51\n",
      "preact - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 90.57 - time: 94.32\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "preact - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 94.732 - time: 689.99\n",
      "preact - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 90.74 - time: 97.09\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12 preact---deconv.1\n",
      "Attempt 1\n",
      "preact - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 98.868 - time: 1097.81\n",
      "preact - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 94.3 - time: 63.00\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "preact - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 98.888 - time: 1048.43\n",
      "preact - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 94.16 - time: 62.74\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "preact - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 98.894 - time: 1037.29\n",
      "preact - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 94.32 - time: 65.84\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "13 resnet---BN\n",
      "Attempt 1\n",
      "resnet - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 97.178 - time: 1237.81\n",
      "resnet - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 92.32 - time: 171.35\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "resnet - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 97.264 - time: 1309.77\n",
      "resnet - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 92.33 - time: 163.28\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "resnet - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 97.252 - time: 1157.50\n",
      "resnet - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 92.39 - time: 159.91\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14 resnet---deconv.1\n",
      "Attempt 1\n",
      "resnet - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 99.0 - time: 3559.51\n",
      "resnet - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 94.2 - time: 127.77\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "resnet - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 98.924 - time: 3581.87\n",
      "resnet - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 94.06 - time: 136.87\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "resnet - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 98.996 - time: 3619.60\n",
      "resnet - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 94.34 - time: 136.99\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "15 resnext---BN\n",
      "Attempt 1\n",
      "resnext - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 96.72 - time: 904.01\n",
      "resnext - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 92.61 - time: 82.99\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "resnext - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 96.772 - time: 896.13\n",
      "resnext - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 92.37 - time: 78.88\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "resnext - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 96.772 - time: 919.30\n",
      "resnext - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 92.04 - time: 98.90\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "16 resnext---deconv.1\n",
      "Attempt 1\n",
      "resnext - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 98.558 - time: 2614.81\n",
      "resnext - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 93.49 - time: 108.24\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "resnext - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 98.582 - time: 2578.39\n",
      "resnext - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 93.53 - time: 92.23\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "resnext - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 98.57 - time: 2594.16\n",
      "resnext - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 93.69 - time: 86.59\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "17 senet---BN\n",
      "Attempt 1\n",
      "senet - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 97.25 - time: 337.94\n",
      "senet - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 92.89 - time: 48.81\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "senet - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 97.258 - time: 357.15\n",
      "senet - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 92.84 - time: 46.63\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "senet - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 97.252 - time: 337.57\n",
      "senet - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 92.5 - time: 48.70\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "18 senet---deconv.1\n",
      "Attempt 1\n",
      "senet - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 98.764 - time: 1747.25\n",
      "senet - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 94.17 - time: 51.92\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "senet - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 98.814 - time: 1399.06\n",
      "senet - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 94.44 - time: 49.08\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "senet - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 98.73 - time: 1407.07\n",
      "senet - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 94.15 - time: 50.37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19 vgg16---BN\n",
      "Attempt 1\n",
      "vgg16 - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 94.872 - time: 414.66\n",
      "vgg16 - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 90.18 - time: 91.41\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "vgg16 - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 95.038 - time: 452.15\n",
      "vgg16 - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 90.6 - time: 100.62\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "vgg16 - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 94.784 - time: 355.24\n",
      "vgg16 - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 90.3 - time: 86.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "20 vgg16---deconv.1\n",
      "Attempt 1\n",
      "vgg16 - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 98.238 - time: 978.15\n",
      "vgg16 - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 92.96 - time: 69.48\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "vgg16 - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 98.238 - time: 1325.19\n",
      "vgg16 - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 92.81 - time: 83.78\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "vgg16 - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 98.13 - time: 1245.88\n",
      "vgg16 - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 92.72 - time: 85.62\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# file_list = glob.glob('checkpoints/data?.txt')\n",
    "file_list_ = glob.glob('checkpoints/*ep.20*')\n",
    "file_list = sorted(file_list_)\n",
    "\n",
    "count = 0\n",
    "for num, dir in enumerate(file_list):\n",
    "    three_folders = glob.glob(f'{dir}/*')\n",
    "    print(num+1, dir.split(\",\")[1], end='---')\n",
    "    \n",
    "    arch = dir.split(\",\")[1]\n",
    "    mode_ = dir.split(\",\")[9]\n",
    "    if mode_ == 'deconv.0':\n",
    "        mode_ = 'BN'\n",
    "    epochs_ = dir.split(\",\")[2]\n",
    "    print(mode_)\n",
    "\n",
    "    \n",
    "    for i, folder in enumerate(three_folders):\n",
    "        files = glob.glob(f'{folder}/*.log')\n",
    "        # print(files)\n",
    "        print(f\"Attempt {i+1}\")\n",
    "        for file in files:\n",
    "            if os.path.basename(file) != 'train_batch.log':\n",
    "                df = pd.read_csv(file, sep='\\t', engine='python')\n",
    "                type_ = os.path.basename(file).replace('.log','')\n",
    "                # print(type_)\n",
    "                # print(df)\n",
    "                print(f\"{arch} - {epochs_} - mode: {mode_} - Attempt_{i+1} - {type_} - top1_acc: {max(df['top1'])} - time: {sum(df['time']):.2f}\")\n",
    "        \n",
    "        print('\\n')\n",
    "            \n",
    "\n",
    "    # count+=1\n",
    "    # if count == 3:\n",
    "    #     break\n",
    "    print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74024038-ebc5-4ab0-add1-3b480c516e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2653c972-51de-4b8d-bc84-870f50860d7b",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">CIFAR-100</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73860c07-1ea9-4731-b19a-16ebfc6798f5",
   "metadata": {},
   "source": [
    "# VGG-16 --- vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf65b9-07f7-4354-b7db-b810156f1711",
   "metadata": {},
   "source": [
    "#### batch normalization --- vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f4c0b85-a91e-42a1-9689-e17869d620ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "| Preparing CIFAR-100 dataset...\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n",
      "100%|███████████████████████| 169001437/169001437 [00:01<00:00, 86719800.89it/s]\n",
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "2024-03-19 19:38:41.530404: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "==> Building model..\n",
      "Namespace(msg=1, resume='', use_gpu=True, num_workers=16, result_path='checkpoints/cifar100,vgg16,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-19-19.38', checkpoint_path='checkpoints/cifar100,vgg16,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-19-19.38', checkpoint_epoch=-1, print_freq=20, seed=0, optimizer='SGD', lr=0.1, lr_scheduler='cosine', momentum=0.9, weight_decay=0.001, batch_size=128, epochs=1, milestone=0.4, multistep_gamma=0.1, arch='vgg16', dataset='cifar100', init='kaiming_1', save_plot=True, tensorboard=True, loss='CE', method=3, batchnorm=True, deconv=None, delinear=None, block_fc=0, block=64, deconv_iter=5, eps=1e-05, bias=True, stride=3, freeze=False, log_path='checkpoints/cifar100,vgg16,ep.1,SGD,0.1,cosine,bs.128,wd.0.001,bn.True,deconv.0,delinear.True,b.64,stride.3,it.5,eps.1e-05,bias.True,bfc.0/03-19-19.38', channel_deconv=None, start_epoch=0, in_planes=3, input_size=32, num_outputs=100, train_epoch_logger=<net_util.Logger object at 0x7f9fbf00f4f0>, train_batch_logger=<net_util.Logger object at 0x7f9fbf00f520>, test_epoch_logger=<net_util.Logger object at 0x7f9fbf00f370>, writer=<torch.utils.tensorboard.writer.SummaryWriter object at 0x7f9fbf00e3b0>, criterion=CrossEntropyLoss(), logger_n_iter=0)\n",
      "14774436 trainable parameters in the network.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "Current learning rate: 0.1\n",
      "training at epoch 1\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Loss: 4.786 | top1: 0.781% ,top5: 5.469%\n",
      "Loss: 4.942 | top1: 1.172% ,top5: 3.906%\n",
      "Loss: 5.106 | top1: 1.042% ,top5: 4.688%\n",
      "Loss: 5.235 | top1: 1.758% ,top5: 4.883%\n",
      "Loss: 5.351 | top1: 2.031% ,top5: 5.000%\n",
      "Loss: 5.570 | top1: 1.953% ,top5: 5.339%\n",
      "Loss: 5.795 | top1: 1.786% ,top5: 5.692%\n",
      "Loss: 6.179 | top1: 1.660% ,top5: 5.762%\n",
      "Loss: 6.410 | top1: 1.736% ,top5: 5.990%\n",
      "Loss: 6.585 | top1: 1.719% ,top5: 5.703%\n",
      "Loss: 6.620 | top1: 1.562% ,top5: 5.682%\n",
      "Loss: 6.621 | top1: 1.562% ,top5: 5.599%\n",
      "Loss: 6.637 | top1: 1.442% ,top5: 5.589%\n",
      "Loss: 6.527 | top1: 1.339% ,top5: 5.357%\n",
      "Loss: 6.477 | top1: 1.302% ,top5: 5.312%\n",
      "Loss: 6.492 | top1: 1.270% ,top5: 5.225%\n",
      "Loss: 6.474 | top1: 1.287% ,top5: 5.239%\n",
      "Loss: 6.455 | top1: 1.215% ,top5: 5.122%\n",
      "Loss: 6.444 | top1: 1.192% ,top5: 5.263%\n",
      "Loss: 6.461 | top1: 1.211% ,top5: 5.352%\n",
      "Loss: 6.391 | top1: 1.190% ,top5: 5.283%\n",
      "Loss: 6.375 | top1: 1.136% ,top5: 5.362%\n",
      "Loss: 6.342 | top1: 1.155% ,top5: 5.469%\n",
      "Loss: 6.295 | top1: 1.107% ,top5: 5.404%\n",
      "Loss: 6.348 | top1: 1.094% ,top5: 5.469%\n",
      "Loss: 6.295 | top1: 1.052% ,top5: 5.409%\n",
      "Loss: 6.236 | top1: 1.042% ,top5: 5.382%\n",
      "Loss: 6.194 | top1: 1.004% ,top5: 5.385%\n",
      "Loss: 6.168 | top1: 1.024% ,top5: 5.442%\n",
      "Loss: 6.127 | top1: 1.042% ,top5: 5.547%\n",
      "Loss: 6.088 | top1: 1.033% ,top5: 5.544%\n",
      "Loss: 6.056 | top1: 1.001% ,top5: 5.542%\n",
      "Loss: 6.011 | top1: 0.994% ,top5: 5.516%\n",
      "Loss: 5.989 | top1: 0.988% ,top5: 5.515%\n",
      "Loss: 5.963 | top1: 0.982% ,top5: 5.536%\n",
      "Loss: 5.941 | top1: 0.998% ,top5: 5.512%\n",
      "Loss: 5.905 | top1: 0.971% ,top5: 5.553%\n",
      "Loss: 5.881 | top1: 0.946% ,top5: 5.469%\n",
      "Loss: 5.852 | top1: 0.962% ,top5: 5.509%\n",
      "Loss: 5.831 | top1: 0.957% ,top5: 5.469%\n",
      "Loss: 5.810 | top1: 0.972% ,top5: 5.602%\n",
      "Loss: 5.783 | top1: 1.004% ,top5: 5.580%\n",
      "Loss: 5.767 | top1: 0.981% ,top5: 5.578%\n",
      "Loss: 5.754 | top1: 0.977% ,top5: 5.575%\n",
      "Loss: 5.735 | top1: 0.972% ,top5: 5.521%\n",
      "Loss: 5.716 | top1: 0.968% ,top5: 5.452%\n",
      "Loss: 5.693 | top1: 0.981% ,top5: 5.436%\n",
      "Loss: 5.676 | top1: 0.960% ,top5: 5.420%\n",
      "Loss: 5.662 | top1: 0.957% ,top5: 5.357%\n",
      "Loss: 5.645 | top1: 0.938% ,top5: 5.297%\n",
      "Loss: 5.626 | top1: 0.934% ,top5: 5.331%\n",
      "Loss: 5.611 | top1: 0.962% ,top5: 5.424%\n",
      "Loss: 5.591 | top1: 0.988% ,top5: 5.469%\n",
      "Loss: 5.572 | top1: 0.984% ,top5: 5.512%\n",
      "Loss: 5.554 | top1: 1.065% ,top5: 5.597%\n",
      "Loss: 5.539 | top1: 1.102% ,top5: 5.594%\n",
      "Loss: 5.523 | top1: 1.124% ,top5: 5.551%\n",
      "Loss: 5.511 | top1: 1.131% ,top5: 5.563%\n",
      "Loss: 5.502 | top1: 1.126% ,top5: 5.535%\n",
      "Loss: 5.499 | top1: 1.146% ,top5: 5.573%\n",
      "Loss: 5.493 | top1: 1.140% ,top5: 5.520%\n",
      "Loss: 5.484 | top1: 1.147% ,top5: 5.519%\n",
      "Loss: 5.473 | top1: 1.166% ,top5: 5.518%\n",
      "Loss: 5.464 | top1: 1.172% ,top5: 5.469%\n",
      "Loss: 5.453 | top1: 1.178% ,top5: 5.469%\n",
      "Loss: 5.445 | top1: 1.184% ,top5: 5.457%\n",
      "Loss: 5.445 | top1: 1.189% ,top5: 5.469%\n",
      "Loss: 5.434 | top1: 1.195% ,top5: 5.515%\n",
      "Loss: 5.422 | top1: 1.200% ,top5: 5.469%\n",
      "Loss: 5.414 | top1: 1.183% ,top5: 5.424%\n",
      "Loss: 5.406 | top1: 1.188% ,top5: 5.392%\n",
      "Loss: 5.395 | top1: 1.183% ,top5: 5.382%\n",
      "Loss: 5.386 | top1: 1.209% ,top5: 5.394%\n",
      "Loss: 5.380 | top1: 1.214% ,top5: 5.405%\n",
      "Loss: 5.372 | top1: 1.198% ,top5: 5.375%\n",
      "Loss: 5.362 | top1: 1.192% ,top5: 5.325%\n",
      "Loss: 5.354 | top1: 1.197% ,top5: 5.347%\n",
      "Loss: 5.348 | top1: 1.182% ,top5: 5.339%\n",
      "Loss: 5.340 | top1: 1.177% ,top5: 5.330%\n",
      "Loss: 5.331 | top1: 1.191% ,top5: 5.361%\n",
      "Loss: 5.323 | top1: 1.196% ,top5: 5.421%\n",
      "Loss: 5.317 | top1: 1.191% ,top5: 5.412%\n",
      "Loss: 5.308 | top1: 1.186% ,top5: 5.412%\n",
      "Loss: 5.301 | top1: 1.190% ,top5: 5.450%\n",
      "Loss: 5.293 | top1: 1.186% ,top5: 5.441%\n",
      "Loss: 5.285 | top1: 1.190% ,top5: 5.423%\n",
      "Loss: 5.277 | top1: 1.194% ,top5: 5.487%\n",
      "Loss: 5.270 | top1: 1.181% ,top5: 5.522%\n",
      "Loss: 5.262 | top1: 1.194% ,top5: 5.521%\n",
      "Loss: 5.255 | top1: 1.181% ,top5: 5.503%\n",
      "Loss: 5.249 | top1: 1.176% ,top5: 5.520%\n",
      "Loss: 5.242 | top1: 1.180% ,top5: 5.537%\n",
      "Loss: 5.235 | top1: 1.176% ,top5: 5.528%\n",
      "Loss: 5.228 | top1: 1.172% ,top5: 5.535%\n",
      "Loss: 5.222 | top1: 1.192% ,top5: 5.526%\n",
      "Loss: 5.216 | top1: 1.188% ,top5: 5.485%\n",
      "Loss: 5.209 | top1: 1.184% ,top5: 5.453%\n",
      "Loss: 5.203 | top1: 1.180% ,top5: 5.445%\n",
      "Loss: 5.197 | top1: 1.176% ,top5: 5.445%\n",
      "Loss: 5.191 | top1: 1.180% ,top5: 5.453%\n",
      "Loss: 5.185 | top1: 1.183% ,top5: 5.469%\n",
      "Loss: 5.179 | top1: 1.172% ,top5: 5.476%\n",
      "Loss: 5.174 | top1: 1.176% ,top5: 5.507%\n",
      "Loss: 5.168 | top1: 1.179% ,top5: 5.529%\n",
      "Loss: 5.163 | top1: 1.168% ,top5: 5.536%\n",
      "Loss: 5.157 | top1: 1.172% ,top5: 5.542%\n",
      "Loss: 5.152 | top1: 1.176% ,top5: 5.527%\n",
      "Loss: 5.147 | top1: 1.179% ,top5: 5.541%\n",
      "Loss: 5.142 | top1: 1.168% ,top5: 5.526%\n",
      "Loss: 5.138 | top1: 1.165% ,top5: 5.526%\n",
      "Loss: 5.134 | top1: 1.168% ,top5: 5.525%\n",
      "Loss: 5.130 | top1: 1.179% ,top5: 5.580%\n",
      "Loss: 5.125 | top1: 1.168% ,top5: 5.559%\n",
      "Loss: 5.121 | top1: 1.165% ,top5: 5.537%\n",
      "Loss: 5.116 | top1: 1.168% ,top5: 5.537%\n",
      "Loss: 5.112 | top1: 1.172% ,top5: 5.529%\n",
      "Loss: 5.107 | top1: 1.169% ,top5: 5.522%\n",
      "Loss: 5.103 | top1: 1.165% ,top5: 5.542%\n",
      "Loss: 5.100 | top1: 1.162% ,top5: 5.521%\n",
      "Loss: 5.095 | top1: 1.159% ,top5: 5.540%\n",
      "Loss: 5.092 | top1: 1.162% ,top5: 5.533%\n",
      "Loss: 5.089 | top1: 1.159% ,top5: 5.526%\n",
      "Loss: 5.084 | top1: 1.175% ,top5: 5.539%\n",
      "Loss: 5.080 | top1: 1.166% ,top5: 5.519%\n",
      "Loss: 5.076 | top1: 1.175% ,top5: 5.544%\n",
      "Loss: 5.072 | top1: 1.172% ,top5: 5.556%\n",
      "Loss: 5.068 | top1: 1.175% ,top5: 5.567%\n",
      "Loss: 5.065 | top1: 1.172% ,top5: 5.566%\n",
      "Loss: 5.061 | top1: 1.169% ,top5: 5.566%\n",
      "Loss: 5.057 | top1: 1.172% ,top5: 5.577%\n",
      "Loss: 5.053 | top1: 1.181% ,top5: 5.588%\n",
      "Loss: 5.050 | top1: 1.184% ,top5: 5.599%\n",
      "Loss: 5.046 | top1: 1.175% ,top5: 5.598%\n",
      "Loss: 5.043 | top1: 1.178% ,top5: 5.615%\n",
      "Loss: 5.040 | top1: 1.181% ,top5: 5.625%\n",
      "Loss: 5.037 | top1: 1.178% ,top5: 5.653%\n",
      "Loss: 5.033 | top1: 1.180% ,top5: 5.657%\n",
      "Loss: 5.030 | top1: 1.178% ,top5: 5.650%\n",
      "Loss: 5.027 | top1: 1.186% ,top5: 5.677%\n",
      "Loss: 5.023 | top1: 1.177% ,top5: 5.686%\n",
      "Loss: 5.020 | top1: 1.175% ,top5: 5.690%\n",
      "Loss: 5.017 | top1: 1.166% ,top5: 5.672%\n",
      "Loss: 5.014 | top1: 1.175% ,top5: 5.704%\n",
      "Loss: 5.011 | top1: 1.172% ,top5: 5.691%\n",
      "Loss: 5.008 | top1: 1.180% ,top5: 5.722%\n",
      "Loss: 5.005 | top1: 1.188% ,top5: 5.742%\n",
      "Loss: 5.002 | top1: 1.190% ,top5: 5.750%\n",
      "Loss: 4.999 | top1: 1.188% ,top5: 5.754%\n",
      "Loss: 4.997 | top1: 1.180% ,top5: 5.736%\n",
      "Loss: 4.994 | top1: 1.177% ,top5: 5.755%\n",
      "Loss: 4.992 | top1: 1.174% ,top5: 5.743%\n",
      "Loss: 4.989 | top1: 1.172% ,top5: 5.736%\n",
      "Loss: 4.986 | top1: 1.169% ,top5: 5.734%\n",
      "Loss: 4.984 | top1: 1.182% ,top5: 5.733%\n",
      "Loss: 4.981 | top1: 1.174% ,top5: 5.716%\n",
      "Loss: 4.979 | top1: 1.187% ,top5: 5.719%\n",
      "Loss: 4.976 | top1: 1.179% ,top5: 5.732%\n",
      "Loss: 4.974 | top1: 1.177% ,top5: 5.726%\n",
      "Loss: 4.972 | top1: 1.169% ,top5: 5.739%\n",
      "Loss: 4.970 | top1: 1.162% ,top5: 5.723%\n",
      "Loss: 4.968 | top1: 1.160% ,top5: 5.736%\n",
      "Loss: 4.965 | top1: 1.153% ,top5: 5.724%\n",
      "Loss: 4.963 | top1: 1.150% ,top5: 5.723%\n",
      "Loss: 4.961 | top1: 1.148% ,top5: 5.726%\n",
      "Loss: 4.958 | top1: 1.160% ,top5: 5.753%\n",
      "Loss: 4.956 | top1: 1.158% ,top5: 5.742%\n",
      "Loss: 4.954 | top1: 1.151% ,top5: 5.759%\n",
      "Loss: 4.951 | top1: 1.144% ,top5: 5.766%\n",
      "Loss: 4.950 | top1: 1.142% ,top5: 5.769%\n",
      "Loss: 4.948 | top1: 1.135% ,top5: 5.772%\n",
      "Loss: 4.945 | top1: 1.138% ,top5: 5.761%\n",
      "Loss: 4.944 | top1: 1.145% ,top5: 5.755%\n",
      "Loss: 4.942 | top1: 1.147% ,top5: 5.762%\n",
      "Loss: 4.939 | top1: 1.140% ,top5: 5.765%\n",
      "Loss: 4.938 | top1: 1.134% ,top5: 5.746%\n",
      "Loss: 4.935 | top1: 1.141% ,top5: 5.753%\n",
      "Loss: 4.933 | top1: 1.139% ,top5: 5.760%\n",
      "Loss: 4.932 | top1: 1.132% ,top5: 5.758%\n",
      "Loss: 4.930 | top1: 1.144% ,top5: 5.783%\n",
      "Loss: 4.928 | top1: 1.150% ,top5: 5.794%\n",
      "Loss: 4.926 | top1: 1.148% ,top5: 5.788%\n",
      "Loss: 4.924 | top1: 1.150% ,top5: 5.786%\n",
      "Loss: 4.922 | top1: 1.161% ,top5: 5.810%\n",
      "Loss: 4.920 | top1: 1.163% ,top5: 5.825%\n",
      "Loss: 4.918 | top1: 1.166% ,top5: 5.840%\n",
      "Loss: 4.916 | top1: 1.163% ,top5: 5.868%\n",
      "Loss: 4.915 | top1: 1.166% ,top5: 5.857%\n",
      "Loss: 4.913 | top1: 1.159% ,top5: 5.851%\n",
      "Loss: 4.911 | top1: 1.166% ,top5: 5.853%\n",
      "Loss: 4.910 | top1: 1.168% ,top5: 5.859%\n",
      "Loss: 4.908 | top1: 1.170% ,top5: 5.861%\n",
      "Loss: 4.906 | top1: 1.164% ,top5: 5.855%\n",
      "Loss: 4.905 | top1: 1.166% ,top5: 5.853%\n",
      "Loss: 4.903 | top1: 1.164% ,top5: 5.859%\n",
      "Loss: 4.901 | top1: 1.166% ,top5: 5.869%\n",
      "Loss: 4.900 | top1: 1.172% ,top5: 5.871%\n",
      "Loss: 4.898 | top1: 1.170% ,top5: 5.885%\n",
      "Loss: 4.896 | top1: 1.172% ,top5: 5.895%\n",
      "Loss: 4.895 | top1: 1.170% ,top5: 5.889%\n",
      "Loss: 4.893 | top1: 1.172% ,top5: 5.883%\n",
      "Loss: 4.891 | top1: 1.166% ,top5: 5.881%\n",
      "Loss: 4.890 | top1: 1.172% ,top5: 5.879%\n",
      "Loss: 4.889 | top1: 1.170% ,top5: 5.888%\n",
      "Loss: 4.887 | top1: 1.172% ,top5: 5.875%\n",
      "Loss: 4.885 | top1: 1.170% ,top5: 5.880%\n",
      "Loss: 4.884 | top1: 1.172% ,top5: 5.871%\n",
      "Loss: 4.882 | top1: 1.170% ,top5: 5.854%\n",
      "Loss: 4.881 | top1: 1.164% ,top5: 5.859%\n",
      "Loss: 4.879 | top1: 1.170% ,top5: 5.858%\n",
      "Loss: 4.878 | top1: 1.168% ,top5: 5.844%\n",
      "Loss: 4.876 | top1: 1.166% ,top5: 5.854%\n",
      "Loss: 4.875 | top1: 1.165% ,top5: 5.863%\n",
      "Loss: 4.873 | top1: 1.163% ,top5: 5.854%\n",
      "Loss: 4.872 | top1: 1.172% ,top5: 5.874%\n",
      "Loss: 4.870 | top1: 1.166% ,top5: 5.890%\n",
      "Loss: 4.869 | top1: 1.183% ,top5: 5.903%\n",
      "Loss: 4.867 | top1: 1.192% ,top5: 5.912%\n",
      "Loss: 4.866 | top1: 1.190% ,top5: 5.910%\n",
      "Loss: 4.865 | top1: 1.188% ,top5: 5.904%\n",
      "Loss: 4.863 | top1: 1.200% ,top5: 5.913%\n",
      "Loss: 4.862 | top1: 1.202% ,top5: 5.911%\n",
      "Loss: 4.861 | top1: 1.197% ,top5: 5.919%\n",
      "Loss: 4.860 | top1: 1.198% ,top5: 5.931%\n",
      "Loss: 4.858 | top1: 1.203% ,top5: 5.933%\n",
      "Loss: 4.857 | top1: 1.201% ,top5: 5.948%\n",
      "Loss: 4.856 | top1: 1.203% ,top5: 5.970%\n",
      "Loss: 4.854 | top1: 1.208% ,top5: 5.975%\n",
      "Loss: 4.853 | top1: 1.210% ,top5: 5.979%\n",
      "Loss: 4.852 | top1: 1.221% ,top5: 6.004%\n",
      "Loss: 4.851 | top1: 1.223% ,top5: 6.002%\n",
      "Loss: 4.849 | top1: 1.228% ,top5: 6.000%\n",
      "Loss: 4.848 | top1: 1.222% ,top5: 5.997%\n",
      "Loss: 4.847 | top1: 1.224% ,top5: 6.012%\n",
      "Loss: 4.846 | top1: 1.229% ,top5: 6.016%\n",
      "Loss: 4.845 | top1: 1.223% ,top5: 6.004%\n",
      "Loss: 4.843 | top1: 1.228% ,top5: 6.015%\n",
      "Loss: 4.842 | top1: 1.226% ,top5: 6.016%\n",
      "Loss: 4.841 | top1: 1.228% ,top5: 6.017%\n",
      "Loss: 4.840 | top1: 1.232% ,top5: 6.015%\n",
      "Loss: 4.839 | top1: 1.237% ,top5: 6.032%\n",
      "Loss: 4.837 | top1: 1.235% ,top5: 6.039%\n",
      "Loss: 4.836 | top1: 1.240% ,top5: 6.050%\n",
      "Loss: 4.835 | top1: 1.241% ,top5: 6.054%\n",
      "Loss: 4.834 | top1: 1.239% ,top5: 6.058%\n",
      "Loss: 4.833 | top1: 1.234% ,top5: 6.046%\n",
      "Loss: 4.832 | top1: 1.232% ,top5: 6.066%\n",
      "Loss: 4.831 | top1: 1.230% ,top5: 6.057%\n",
      "Loss: 4.830 | top1: 1.229% ,top5: 6.036%\n",
      "Loss: 4.829 | top1: 1.230% ,top5: 6.046%\n",
      "Loss: 4.828 | top1: 1.228% ,top5: 6.050%\n",
      "Loss: 4.827 | top1: 1.226% ,top5: 6.035%\n",
      "Loss: 4.826 | top1: 1.240% ,top5: 6.048%\n",
      "Loss: 4.825 | top1: 1.238% ,top5: 6.046%\n",
      "Loss: 4.824 | top1: 1.243% ,top5: 6.047%\n",
      "Loss: 4.823 | top1: 1.244% ,top5: 6.060%\n",
      "Loss: 4.822 | top1: 1.248% ,top5: 6.061%\n",
      "Loss: 4.821 | top1: 1.246% ,top5: 6.062%\n",
      "Loss: 4.820 | top1: 1.248% ,top5: 6.071%\n",
      "Loss: 4.819 | top1: 1.246% ,top5: 6.066%\n",
      "Loss: 4.818 | top1: 1.244% ,top5: 6.073%\n",
      "Loss: 4.817 | top1: 1.239% ,top5: 6.073%\n",
      "Loss: 4.816 | top1: 1.237% ,top5: 6.077%\n",
      "Loss: 4.815 | top1: 1.233% ,top5: 6.075%\n",
      "Loss: 4.814 | top1: 1.234% ,top5: 6.084%\n",
      "Loss: 4.813 | top1: 1.235% ,top5: 6.079%\n",
      "Loss: 4.813 | top1: 1.231% ,top5: 6.065%\n",
      "Loss: 4.812 | top1: 1.238% ,top5: 6.077%\n",
      "Loss: 4.811 | top1: 1.233% ,top5: 6.078%\n",
      "Loss: 4.810 | top1: 1.231% ,top5: 6.067%\n",
      "Loss: 4.809 | top1: 1.236% ,top5: 6.073%\n",
      "Loss: 4.808 | top1: 1.234% ,top5: 6.068%\n",
      "Loss: 4.807 | top1: 1.229% ,top5: 6.055%\n",
      "Loss: 4.806 | top1: 1.228% ,top5: 6.053%\n",
      "Loss: 4.805 | top1: 1.229% ,top5: 6.065%\n",
      "Loss: 4.805 | top1: 1.227% ,top5: 6.062%\n",
      "Loss: 4.804 | top1: 1.226% ,top5: 6.052%\n",
      "Loss: 4.803 | top1: 1.227% ,top5: 6.055%\n",
      "Loss: 4.802 | top1: 1.234% ,top5: 6.062%\n",
      "Loss: 4.801 | top1: 1.240% ,top5: 6.068%\n",
      "Loss: 4.800 | top1: 1.239% ,top5: 6.057%\n",
      "Loss: 4.800 | top1: 1.234% ,top5: 6.044%\n",
      "Loss: 4.799 | top1: 1.236% ,top5: 6.048%\n",
      "Loss: 4.798 | top1: 1.237% ,top5: 6.043%\n",
      "Loss: 4.797 | top1: 1.238% ,top5: 6.044%\n",
      "Loss: 4.796 | top1: 1.236% ,top5: 6.055%\n",
      "Loss: 4.795 | top1: 1.235% ,top5: 6.056%\n",
      "Loss: 4.795 | top1: 1.233% ,top5: 6.059%\n",
      "Loss: 4.794 | top1: 1.229% ,top5: 6.052%\n",
      "Loss: 4.793 | top1: 1.233% ,top5: 6.061%\n",
      "Loss: 4.792 | top1: 1.231% ,top5: 6.059%\n",
      "Loss: 4.791 | top1: 1.227% ,top5: 6.046%\n",
      "Loss: 4.791 | top1: 1.225% ,top5: 6.039%\n",
      "Loss: 4.790 | top1: 1.227% ,top5: 6.050%\n",
      "Loss: 4.789 | top1: 1.228% ,top5: 6.048%\n",
      "Loss: 4.788 | top1: 1.226% ,top5: 6.049%\n",
      "Loss: 4.788 | top1: 1.230% ,top5: 6.052%\n",
      "Loss: 4.787 | top1: 1.226% ,top5: 6.050%\n",
      "Loss: 4.786 | top1: 1.224% ,top5: 6.040%\n",
      "Loss: 4.786 | top1: 1.223% ,top5: 6.033%\n",
      "Loss: 4.785 | top1: 1.221% ,top5: 6.034%\n",
      "Loss: 4.784 | top1: 1.217% ,top5: 6.045%\n",
      "Loss: 4.784 | top1: 1.218% ,top5: 6.051%\n",
      "Loss: 4.783 | top1: 1.220% ,top5: 6.057%\n",
      "Loss: 4.782 | top1: 1.221% ,top5: 6.057%\n",
      "Loss: 4.782 | top1: 1.217% ,top5: 6.068%\n",
      "Loss: 4.781 | top1: 1.215% ,top5: 6.071%\n",
      "Loss: 4.781 | top1: 1.219% ,top5: 6.074%\n",
      "Loss: 4.780 | top1: 1.220% ,top5: 6.070%\n",
      "Loss: 4.780 | top1: 1.216% ,top5: 6.070%\n",
      "Loss: 4.779 | top1: 1.215% ,top5: 6.074%\n",
      "Loss: 4.778 | top1: 1.216% ,top5: 6.072%\n",
      "Loss: 4.778 | top1: 1.219% ,top5: 6.065%\n",
      "Loss: 4.777 | top1: 1.223% ,top5: 6.070%\n",
      "Loss: 4.776 | top1: 1.232% ,top5: 6.081%\n",
      "Loss: 4.776 | top1: 1.235% ,top5: 6.086%\n",
      "Loss: 4.776 | top1: 1.231% ,top5: 6.084%\n",
      "Loss: 4.775 | top1: 1.230% ,top5: 6.082%\n",
      "Loss: 4.774 | top1: 1.233% ,top5: 6.088%\n",
      "Loss: 4.773 | top1: 1.229% ,top5: 6.083%\n",
      "Loss: 4.773 | top1: 1.228% ,top5: 6.084%\n",
      "Loss: 4.772 | top1: 1.234% ,top5: 6.094%\n",
      "Loss: 4.772 | top1: 1.233% ,top5: 6.090%\n",
      "Loss: 4.771 | top1: 1.241% ,top5: 6.102%\n",
      "Loss: 4.770 | top1: 1.244% ,top5: 6.105%\n",
      "Loss: 4.770 | top1: 1.245% ,top5: 6.111%\n",
      "Loss: 4.769 | top1: 1.246% ,top5: 6.109%\n",
      "Loss: 4.769 | top1: 1.250% ,top5: 6.111%\n",
      "Loss: 4.768 | top1: 1.248% ,top5: 6.112%\n",
      "Loss: 4.767 | top1: 1.247% ,top5: 6.115%\n",
      "Loss: 4.767 | top1: 1.248% ,top5: 6.117%\n",
      "Loss: 4.766 | top1: 1.251% ,top5: 6.120%\n",
      "Loss: 4.766 | top1: 1.250% ,top5: 6.125%\n",
      "Loss: 4.765 | top1: 1.250% ,top5: 6.121%\n",
      "Loss: 4.765 | top1: 1.251% ,top5: 6.128%\n",
      "Loss: 4.764 | top1: 1.252% ,top5: 6.131%\n",
      "Loss: 4.763 | top1: 1.251% ,top5: 6.138%\n",
      "Loss: 4.763 | top1: 1.250% ,top5: 6.136%\n",
      "Loss: 4.762 | top1: 1.246% ,top5: 6.132%\n",
      "Loss: 4.762 | top1: 1.242% ,top5: 6.128%\n",
      "Loss: 4.761 | top1: 1.239% ,top5: 6.121%\n",
      "Loss: 4.760 | top1: 1.235% ,top5: 6.122%\n",
      "Loss: 4.760 | top1: 1.234% ,top5: 6.115%\n",
      "Loss: 4.759 | top1: 1.230% ,top5: 6.127%\n",
      "Loss: 4.759 | top1: 1.238% ,top5: 6.132%\n",
      "Loss: 4.758 | top1: 1.243% ,top5: 6.148%\n",
      "Loss: 4.757 | top1: 1.246% ,top5: 6.155%\n",
      "Loss: 4.757 | top1: 1.250% ,top5: 6.162%\n",
      "Loss: 4.756 | top1: 1.246% ,top5: 6.151%\n",
      "Loss: 4.756 | top1: 1.247% ,top5: 6.158%\n",
      "Loss: 4.755 | top1: 1.252% ,top5: 6.165%\n",
      "Loss: 4.755 | top1: 1.260% ,top5: 6.181%\n",
      "Loss: 4.754 | top1: 1.258% ,top5: 6.172%\n",
      "Loss: 4.754 | top1: 1.255% ,top5: 6.168%\n",
      "Loss: 4.753 | top1: 1.254% ,top5: 6.164%\n",
      "Loss: 4.753 | top1: 1.252% ,top5: 6.158%\n",
      "Loss: 4.752 | top1: 1.255% ,top5: 6.162%\n",
      "Loss: 4.752 | top1: 1.256% ,top5: 6.158%\n",
      "Loss: 4.751 | top1: 1.257% ,top5: 6.158%\n",
      "Loss: 4.751 | top1: 1.253% ,top5: 6.154%\n",
      "Loss: 4.750 | top1: 1.254% ,top5: 6.155%\n",
      "Loss: 4.750 | top1: 1.253% ,top5: 6.148%\n",
      "Loss: 4.749 | top1: 1.250% ,top5: 6.151%\n",
      "Loss: 4.749 | top1: 1.253% ,top5: 6.149%\n",
      "Loss: 4.748 | top1: 1.256% ,top5: 6.147%\n",
      "Loss: 4.748 | top1: 1.261% ,top5: 6.154%\n",
      "Loss: 4.747 | top1: 1.259% ,top5: 6.167%\n",
      "Loss: 4.747 | top1: 1.264% ,top5: 6.167%\n",
      "Loss: 4.746 | top1: 1.267% ,top5: 6.165%\n",
      "Loss: 4.746 | top1: 1.266% ,top5: 6.163%\n",
      "Loss: 4.746 | top1: 1.267% ,top5: 6.163%\n",
      "Loss: 4.745 | top1: 1.272% ,top5: 6.159%\n",
      "Loss: 4.745 | top1: 1.271% ,top5: 6.151%\n",
      "Loss: 4.745 | top1: 1.269% ,top5: 6.147%\n",
      "Loss: 4.744 | top1: 1.268% ,top5: 6.141%\n",
      "Loss: 4.744 | top1: 1.267% ,top5: 6.144%\n",
      "Loss: 4.743 | top1: 1.267% ,top5: 6.146%\n",
      "Loss: 4.743 | top1: 1.270% ,top5: 6.146%\n",
      "Loss: 4.742 | top1: 1.273% ,top5: 6.153%\n",
      "Loss: 4.742 | top1: 1.276% ,top5: 6.168%\n",
      "Loss: 4.741 | top1: 1.281% ,top5: 6.170%\n",
      "Loss: 4.741 | top1: 1.280% ,top5: 6.172%\n",
      "Loss: 4.741 | top1: 1.278% ,top5: 6.166%\n",
      "Loss: 4.740 | top1: 1.283% ,top5: 6.168%\n",
      "Loss: 4.740 | top1: 1.284% ,top5: 6.169%\n",
      "Loss: 4.739 | top1: 1.280% ,top5: 6.161%\n",
      "Loss: 4.739 | top1: 1.279% ,top5: 6.161%\n",
      "Loss: 4.739 | top1: 1.276% ,top5: 6.159%\n",
      "Loss: 4.738 | top1: 1.275% ,top5: 6.161%\n",
      "Loss: 4.738 | top1: 1.273% ,top5: 6.160%\n",
      "Loss: 4.737 | top1: 1.274% ,top5: 6.164%\n",
      "Loss: 4.738 | top1: 1.272% ,top5: 6.158%\n",
      "Loss: 4.738 | top1: 1.272%, top5: 6.158% elasped time:  25 seconds.\n",
      "Testing at epoch 1\n",
      "Loss: 4.581 | top1: 1.562% ,top5: 3.125%\n",
      "Loss: 4.543 | top1: 1.172% ,top5: 6.250%\n",
      "Loss: 4.553 | top1: 1.823% ,top5: 6.771%\n",
      "Loss: 4.563 | top1: 1.367% ,top5: 6.445%\n",
      "Loss: 4.562 | top1: 1.250% ,top5: 6.875%\n",
      "Loss: 4.565 | top1: 1.172% ,top5: 6.250%\n",
      "Loss: 4.568 | top1: 1.451% ,top5: 6.585%\n",
      "Loss: 4.557 | top1: 1.660% ,top5: 6.738%\n",
      "Loss: 4.557 | top1: 1.736% ,top5: 7.465%\n",
      "Loss: 4.559 | top1: 1.641% ,top5: 7.344%\n",
      "Loss: 4.563 | top1: 1.705% ,top5: 7.244%\n",
      "Loss: 4.564 | top1: 1.693% ,top5: 7.096%\n",
      "Loss: 4.558 | top1: 1.743% ,top5: 7.392%\n",
      "Loss: 4.558 | top1: 1.674% ,top5: 7.366%\n",
      "Loss: 4.560 | top1: 1.615% ,top5: 7.292%\n",
      "Loss: 4.560 | top1: 1.660% ,top5: 7.227%\n",
      "Loss: 4.560 | top1: 1.562% ,top5: 7.123%\n",
      "Loss: 4.562 | top1: 1.562% ,top5: 7.031%\n",
      "Loss: 4.561 | top1: 1.604% ,top5: 7.113%\n",
      "Loss: 4.560 | top1: 1.523% ,top5: 6.953%\n",
      "Loss: 4.559 | top1: 1.562% ,top5: 7.068%\n",
      "Loss: 4.569 | top1: 1.562% ,top5: 7.102%\n",
      "Loss: 4.570 | top1: 1.495% ,top5: 7.099%\n",
      "Loss: 4.571 | top1: 1.530% ,top5: 7.227%\n",
      "Loss: 4.571 | top1: 1.531% ,top5: 7.219%\n",
      "Loss: 4.572 | top1: 1.593% ,top5: 7.181%\n",
      "Loss: 4.573 | top1: 1.534% ,top5: 7.118%\n",
      "Loss: 4.572 | top1: 1.535% ,top5: 7.059%\n",
      "Loss: 4.573 | top1: 1.509% ,top5: 6.977%\n",
      "Loss: 4.572 | top1: 1.458% ,top5: 7.057%\n",
      "Loss: 4.572 | top1: 1.512% ,top5: 7.082%\n",
      "Loss: 4.572 | top1: 1.489% ,top5: 7.031%\n",
      "Loss: 4.571 | top1: 1.444% ,top5: 6.960%\n",
      "Loss: 4.573 | top1: 1.448% ,top5: 6.893%\n",
      "Loss: 4.579 | top1: 1.451% ,top5: 6.830%\n",
      "Loss: 4.581 | top1: 1.411% ,top5: 6.858%\n",
      "Loss: 4.579 | top1: 1.394% ,top5: 6.841%\n",
      "Loss: 4.580 | top1: 1.398% ,top5: 6.928%\n",
      "Loss: 4.579 | top1: 1.382% ,top5: 6.951%\n",
      "Loss: 4.579 | top1: 1.367% ,top5: 6.914%\n",
      "Loss: 4.579 | top1: 1.372% ,top5: 6.860%\n",
      "Loss: 4.579 | top1: 1.358% ,top5: 6.808%\n",
      "Loss: 4.579 | top1: 1.363% ,top5: 6.795%\n",
      "Loss: 4.578 | top1: 1.367% ,top5: 6.818%\n",
      "Loss: 4.577 | top1: 1.389% ,top5: 6.858%\n",
      "Loss: 4.577 | top1: 1.410% ,top5: 6.827%\n",
      "Loss: 4.576 | top1: 1.413% ,top5: 6.815%\n",
      "Loss: 4.576 | top1: 1.416% ,top5: 6.803%\n",
      "Loss: 4.575 | top1: 1.403% ,top5: 6.824%\n",
      "Loss: 4.574 | top1: 1.375% ,top5: 6.797%\n",
      "Loss: 4.576 | top1: 1.379% ,top5: 6.740%\n",
      "Loss: 4.576 | top1: 1.367% ,top5: 6.716%\n",
      "Loss: 4.577 | top1: 1.356% ,top5: 6.781%\n",
      "Loss: 4.577 | top1: 1.345% ,top5: 6.771%\n",
      "Loss: 4.577 | top1: 1.321% ,top5: 6.747%\n",
      "Loss: 4.577 | top1: 1.325% ,top5: 6.738%\n",
      "Loss: 4.576 | top1: 1.329% ,top5: 6.826%\n",
      "Loss: 4.577 | top1: 1.347% ,top5: 6.816%\n",
      "Loss: 4.577 | top1: 1.337% ,top5: 6.793%\n",
      "Loss: 4.576 | top1: 1.341% ,top5: 6.784%\n",
      "Loss: 4.580 | top1: 1.345% ,top5: 6.801%\n",
      "Loss: 4.580 | top1: 1.323% ,top5: 6.767%\n",
      "Loss: 4.579 | top1: 1.364% ,top5: 6.845%\n",
      "Loss: 4.579 | top1: 1.392% ,top5: 6.812%\n",
      "Loss: 4.578 | top1: 1.418% ,top5: 6.863%\n",
      "Loss: 4.578 | top1: 1.420% ,top5: 6.818%\n",
      "Loss: 4.578 | top1: 1.423% ,top5: 6.763%\n",
      "Loss: 4.578 | top1: 1.413% ,top5: 6.733%\n",
      "Loss: 4.578 | top1: 1.415% ,top5: 6.726%\n",
      "Loss: 4.578 | top1: 1.429% ,top5: 6.696%\n",
      "Loss: 4.578 | top1: 1.441% ,top5: 6.712%\n",
      "Loss: 4.578 | top1: 1.443% ,top5: 6.706%\n",
      "Loss: 4.577 | top1: 1.466% ,top5: 6.742%\n",
      "Loss: 4.576 | top1: 1.467% ,top5: 6.725%\n",
      "Loss: 4.576 | top1: 1.479% ,top5: 6.740%\n",
      "Loss: 4.576 | top1: 1.480% ,top5: 6.754%\n",
      "Loss: 4.575 | top1: 1.491% ,top5: 6.767%\n",
      "Loss: 4.575 | top1: 1.472% ,top5: 6.731%\n",
      "Loss: 4.575 | top1: 1.470% ,top5: 6.720%\n",
      "Loss: 4.575 | top1: 1.470%, top5: 6.720%, elasped time:   3 seconds. Best Acc: 1.470%\n",
      "Training finished successfully. Model size:  14774436\n",
      "Best acc:  1.47\n"
     ]
    }
   ],
   "source": [
    "!python main.py --lr .1 --optimizer SGD --arch vgg16 --epochs 1 --dataset cifar100 --batch-size 128 --msg True --deconv False --block-fc 0 --wd .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11070ff4-44c7-4c11-9bab-ebb937916a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#SBATCH -c 8\n",
      "#SBATCH -p gpu\n",
      "#SBATCH --gres gpu:1\n",
      "\n",
      "enable_lmod\n",
      "module load container_env pytorch-gpu/1.13.0\n",
      "\n",
      "export CUDA_HOME=/cm/shared/applications/cuda-toolkit/11.7.1/\n",
      "export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CUDA_HOME\n",
      "\n",
      "# crun.tensorflow-gpu -p ~/envs/cs834_project python lemos_kerasnlp_for_slurm_job_training.py -itr $iteration -ep $epochs\n",
      "\n",
      "crun.pytorch-gpu -p ~/envs/cs834_project python main.py --lr .1 --optimizer SGD --arch $architecture --epochs $epochs --dataset cifar100  --batch-size 128 --msg True --deconv True --block-fc 512 --wd .001"
     ]
    }
   ],
   "source": [
    "!cat single_experiment_net_deconv_cifar100.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7efe066b-4bfe-4e42-a4b3-74708d9c2a85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "1 densenet121---BN\n",
      "Attempt 1\n",
      "densenet121 - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 92.956 - time: 1141.66\n",
      "densenet121 - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 75.45 - time: 94.32\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "densenet121 - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 93.052 - time: 1124.73\n",
      "densenet121 - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 75.78 - time: 94.72\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "densenet121 - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 92.858 - time: 1315.25\n",
      "densenet121 - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 75.36 - time: 95.62\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2 densenet121---deconv.1\n",
      "Attempt 1\n",
      "densenet121 - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 98.18 - time: 4716.44\n",
      "densenet121 - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 78.57 - time: 110.58\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "densenet121 - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 98.094 - time: 4668.14\n",
      "densenet121 - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 78.38 - time: 108.34\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "densenet121 - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 98.082 - time: 4652.39\n",
      "densenet121 - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 78.03 - time: 107.49\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3 dpn---BN\n",
      "Attempt 1\n",
      "dpn - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 85.966 - time: 2743.38\n",
      "dpn - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 72.58 - time: 165.81\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "dpn - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 84.148 - time: 2738.34\n",
      "dpn - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 70.87 - time: 161.78\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "dpn - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 59.666 - time: 1676.22\n",
      "dpn - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 53.7 - time: 99.83\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4 dpn---deconv.1\n",
      "Attempt 1\n",
      "dpn - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 96.846 - time: 6452.24\n",
      "dpn - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 75.24 - time: 177.21\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "dpn - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 96.662 - time: 6393.99\n",
      "dpn - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 75.42 - time: 171.86\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "dpn - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 96.81 - time: 6412.53\n",
      "dpn - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 75.32 - time: 175.71\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 efficient---BN\n",
      "Attempt 1\n",
      "efficient - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 61.534 - time: 521.30\n",
      "efficient - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 54.72 - time: 46.98\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "efficient - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 59.424 - time: 531.74\n",
      "efficient - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 52.91 - time: 47.79\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "efficient - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 63.504 - time: 566.00\n",
      "efficient - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 57.38 - time: 53.17\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6 efficient---deconv.1\n",
      "Attempt 1\n",
      "efficient - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 66.948 - time: 540.29\n",
      "efficient - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 61.52 - time: 50.09\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "efficient - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 66.882 - time: 581.31\n",
      "efficient - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 61.45 - time: 51.93\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "efficient - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 66.678 - time: 542.79\n",
      "efficient - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 61.15 - time: 49.10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7 mobilev2---BN\n",
      "Attempt 1\n",
      "mobilev2 - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 75.356 - time: 459.78\n",
      "mobilev2 - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 66.95 - time: 47.26\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "mobilev2 - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 74.996 - time: 450.52\n",
      "mobilev2 - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 67.53 - time: 49.24\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "mobilev2 - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 74.826 - time: 438.87\n",
      "mobilev2 - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 67.36 - time: 46.14\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8 mobilev2---deconv.1\n",
      "Attempt 1\n",
      "mobilev2 - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 82.998 - time: 1619.90\n",
      "mobilev2 - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 71.43 - time: 55.36\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "mobilev2 - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 82.958 - time: 1648.80\n",
      "mobilev2 - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 71.52 - time: 57.99\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "mobilev2 - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 82.834 - time: 1671.56\n",
      "mobilev2 - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 71.56 - time: 57.64\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9 pnasnetA---BN\n",
      "Attempt 1\n",
      "pnasnetA - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 35.064 - time: 393.29\n",
      "pnasnetA - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 35.75 - time: 51.06\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "pnasnetA - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 34.404 - time: 368.54\n",
      "pnasnetA - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 34.7 - time: 44.31\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "pnasnetA - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 37.544 - time: 373.52\n",
      "pnasnetA - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 37.26 - time: 45.86\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10 pnasnetA---deconv.1\n",
      "Attempt 1\n",
      "pnasnetA - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 58.248 - time: 1619.32\n",
      "pnasnetA - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 55.78 - time: 47.62\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "pnasnetA - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 58.36 - time: 1609.33\n",
      "pnasnetA - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 55.33 - time: 51.38\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "pnasnetA - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 58.214 - time: 1600.28\n",
      "pnasnetA - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 54.7 - time: 43.83\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11 preact---BN\n",
      "Attempt 1\n",
      "preact - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 87.774 - time: 279.47\n",
      "preact - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 71.31 - time: 42.41\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "preact - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 87.624 - time: 280.93\n",
      "preact - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 71.56 - time: 43.30\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "preact - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 87.792 - time: 324.48\n",
      "preact - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 71.29 - time: 46.29\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12 preact---deconv.1\n",
      "Attempt 1\n",
      "preact - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 94.714 - time: 1020.78\n",
      "preact - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 75.68 - time: 44.29\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "preact - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 94.79 - time: 1011.68\n",
      "preact - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 75.91 - time: 42.26\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "preact - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 94.624 - time: 1017.93\n",
      "preact - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 76.48 - time: 44.90\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "13 resnet18d---BN\n",
      "Attempt 1\n",
      "resnet18d - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 68.176 - time: 227.75\n",
      "resnet18d - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 57.48 - time: 37.15\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "resnet18d - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 68.174 - time: 223.51\n",
      "resnet18d - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 57.57 - time: 38.09\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "resnet18d - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 67.634 - time: 230.28\n",
      "resnet18d - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 57.09 - time: 32.76\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14 resnet18d---deconv.1\n",
      "Attempt 1\n",
      "resnet18d - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 69.244 - time: 866.29\n",
      "resnet18d - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 59.47 - time: 41.67\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "resnet18d - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 69.23 - time: 925.68\n",
      "resnet18d - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 59.67 - time: 42.74\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "resnet18d - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 69.148 - time: 887.73\n",
      "resnet18d - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 59.92 - time: 39.74\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "15 resnext---BN\n",
      "Attempt 1\n",
      "resnext - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 84.364 - time: 911.81\n",
      "resnext - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 71.12 - time: 79.05\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "resnext - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 84.47 - time: 907.84\n",
      "resnext - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 71.94 - time: 78.63\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "resnext - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 84.484 - time: 909.25\n",
      "resnext - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 71.2 - time: 79.10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "16 resnext---deconv.1\n",
      "Attempt 1\n",
      "resnext - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 98.348 - time: 2588.31\n",
      "resnext - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 77.52 - time: 87.17\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "resnext - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 98.316 - time: 2620.82\n",
      "resnext - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 77.36 - time: 89.11\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "resnext - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 98.328 - time: 2600.71\n",
      "resnext - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 77.4 - time: 87.92\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "17 senet---BN\n",
      "Attempt 1\n",
      "senet - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 87.998 - time: 345.24\n",
      "senet - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 71.79 - time: 45.20\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "senet - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 87.768 - time: 346.59\n",
      "senet - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 72.24 - time: 46.47\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "senet - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 87.866 - time: 349.95\n",
      "senet - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 72.15 - time: 47.83\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "18 senet---deconv.1\n",
      "Attempt 1\n",
      "senet - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 94.35 - time: 1779.25\n",
      "senet - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 76.09 - time: 57.47\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "senet - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 93.936 - time: 1401.54\n",
      "senet - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 75.7 - time: 51.56\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "senet - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 94.098 - time: 1481.68\n",
      "senet - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 76.33 - time: 51.74\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19 vgg16---BN\n",
      "Attempt 1\n",
      "vgg16 - ep.20 - mode: BN - Attempt_1 - train - top1_acc: 69.24 - time: 209.97\n",
      "vgg16 - ep.20 - mode: BN - Attempt_1 - test - top1_acc: 61.47 - time: 39.47\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "vgg16 - ep.20 - mode: BN - Attempt_2 - train - top1_acc: 68.836 - time: 205.16\n",
      "vgg16 - ep.20 - mode: BN - Attempt_2 - test - top1_acc: 61.28 - time: 39.89\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "vgg16 - ep.20 - mode: BN - Attempt_3 - train - top1_acc: 69.046 - time: 199.29\n",
      "vgg16 - ep.20 - mode: BN - Attempt_3 - test - top1_acc: 60.88 - time: 36.72\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "20 vgg16---deconv.1\n",
      "Attempt 1\n",
      "vgg16 - ep.20 - mode: deconv.1 - Attempt_1 - train - top1_acc: 88.626 - time: 696.82\n",
      "vgg16 - ep.20 - mode: deconv.1 - Attempt_1 - test - top1_acc: 71.75 - time: 40.80\n",
      "\n",
      "\n",
      "Attempt 2\n",
      "vgg16 - ep.20 - mode: deconv.1 - Attempt_2 - train - top1_acc: 88.516 - time: 698.52\n",
      "vgg16 - ep.20 - mode: deconv.1 - Attempt_2 - test - top1_acc: 71.98 - time: 46.62\n",
      "\n",
      "\n",
      "Attempt 3\n",
      "vgg16 - ep.20 - mode: deconv.1 - Attempt_3 - train - top1_acc: 88.15 - time: 694.62\n",
      "vgg16 - ep.20 - mode: deconv.1 - Attempt_3 - test - top1_acc: 71.49 - time: 37.54\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# file_list_ = glob.glob('checkpoints/*ep.20*')\n",
    "# file_list_ = glob.glob('checkpoints/cifar10,*ep.100*')\n",
    "# file_list_ = glob.glob('checkpoints/cifar100,*ep.1,*')\n",
    "# file_list_ = glob.glob('checkpoints/cifar100,*ep.20*')\n",
    "file_list_ = glob.glob('checkpoints/cifar100,*ep.20*')\n",
    "\n",
    "file_list = sorted(file_list_)\n",
    "print(len(file_list))\n",
    "file_list\n",
    "\n",
    "count = 0\n",
    "for num, dir in enumerate(file_list):\n",
    "    three_folders = glob.glob(f'{dir}/*')\n",
    "    print(num+1, dir.split(\",\")[1], end='---')\n",
    "    \n",
    "    arch = dir.split(\",\")[1]\n",
    "    mode_ = dir.split(\",\")[9]\n",
    "    if mode_ == 'deconv.0':\n",
    "        mode_ = 'BN'\n",
    "    epochs_ = dir.split(\",\")[2]\n",
    "    print(mode_)\n",
    "\n",
    "    \n",
    "    for i, folder in enumerate(three_folders):\n",
    "        files = glob.glob(f'{folder}/*.log')\n",
    "        # print(files)\n",
    "        print(f\"Attempt {i+1}\")\n",
    "        for file in files:\n",
    "            if os.path.basename(file) != 'train_batch.log':\n",
    "                df = pd.read_csv(file, sep='\\t', engine='python')\n",
    "                type_ = os.path.basename(file).replace('.log','')\n",
    "                # print(type_)\n",
    "                # print(df)\n",
    "                print(f\"{arch} - {epochs_} - mode: {mode_} - Attempt_{i+1} - {type_} - top1_acc: {max(df['top1'])} - time: {sum(df['time']):.2f}\")\n",
    "        \n",
    "        print('\\n')\n",
    "    # count+=1\n",
    "    # if count == 3:\n",
    "    #     break\n",
    "    print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2b291-6a3c-4f1a-9132-5cab5888fdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12801024-5b5f-4c73-9b0c-2a6a4d3db476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f814477-1c84-46a5-ae26-496cb0488cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-24 23:51:48.660579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "=> creating model 'resnet18'\n",
      "Namespace(data='data/imagenet/ILSVRC/Data/CLS-LOC', arch='resnet18', workers=32, epochs=90, start_epoch=0, batch_size=256, lr=0.1, lr_scheduler='cosine', scheduler_step_size=30, milestone=0.3, multistep_gamma=0.1, momentum=0.9, weight_decay=0.0001, print_freq=10, resume='', evaluate=False, pretrained=False, world_size=-1, rank=-1, dist_url='tcp://224.66.41.62:23456', dist_backend='nccl', seed=None, gpu=None, multiprocessing_distributed=False, dataset='imagenet', tensorboard=True, save_plot=True, deconv=False, delinear=functools.partial(<class 'models.deconv.Delinear'>, block=64, eps=1e-05, n_iter=5), block=64, deconv_iter=5, eps=1e-05, bias=True, block_fc=64, test_run=False, test_iter=500, stride=3, log_dir='checkpoints/imagenet,resnet18,ep:90,0.1,cosine,wd:0.0001,bs:256,seed:None,deconv:False,delinear:True,stride:3,b:64,it:5,eps:1e-05,bias:True,bfc:64/03-24-23:51', channel_deconv=None, train_losses=[], train_top1=[], train_top5=[], eval_losses=[], eval_top1=[], eval_top5=[], cur_losses=[], writer=<torch.utils.tensorboard.writer.SummaryWriter object at 0x7f9a8b43ffd0>, distributed=False)\n",
      "11689512 trainable parameters in the network.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oruma001/CS895/project/codebase/main_imagenet.py\", line 672, in <module>\n",
      "    main()\n",
      "  File \"/home/oruma001/CS895/project/codebase/main_imagenet.py\", line 179, in main\n",
      "    main_worker(args.gpu, ngpus_per_node, args)\n",
      "  File \"/home/oruma001/CS895/project/codebase/main_imagenet.py\", line 310, in main_worker\n",
      "    train_dataset = datasets.ImageFolder(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 309, in __init__\n",
      "    super().__init__(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 144, in __init__\n",
      "    classes, class_to_idx = self.find_classes(self.root)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 218, in find_classes\n",
      "    return find_classes(directory)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 40, in find_classes\n",
      "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/imagenet/ILSVRC/Data/CLS-LOC/train'\n"
     ]
    }
   ],
   "source": [
    "!python main_imagenet.py -a resnet18 -j 32 data/imagenet/ILSVRC/Data/CLS-LOC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11ce05b-195b-4605-b6f4-aa453ce5da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/imagenet-object-localization-challenge/data\n",
    "# dataset is ~160GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b6848e-000b-4941-9ab8-2d1ee3c3e913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c12ef-c92c-4fb5-8fb8-bb40446c07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cityescapes\n",
    "# is not --- syntax error --- had to use != at train 166 line\n",
    "ModuleNotFoundError: No module named 'torchvision.models.utils'\n",
    "from torch.hub import load_state_dict_from_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e06db4b-d728-451e-9255-7f0da575b2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-11 18:44:48.966205: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-11 18:44:49.630482: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-11 18:44:52.076565: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-11 18:44:56.928367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "| distributed init (rank 0): env://\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oruma001/CS895/project/codebase/Segmentation/train.py\", line 611, in <module>\n",
      "    main(args)\n",
      "  File \"/home/oruma001/CS895/project/codebase/Segmentation/train.py\", line 189, in main\n",
      "    utils.init_distributed_mode(args)\n",
      "  File \"/home/oruma001/CS895/project/codebase/Segmentation/utils.py\", line 298, in init_distributed_mode\n",
      "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\", line 754, in init_process_group\n",
      "    store, rank, world_size = next(rendezvous_iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/rendezvous.py\", line 243, in _env_rendezvous_handler\n",
      "    master_addr = _get_env_or_raise(\"MASTER_ADDR\")\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/rendezvous.py\", line 221, in _get_env_or_raise\n",
      "    raise _env_error(env_var)\n",
      "ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set\n"
     ]
    }
   ],
   "source": [
    "!python Segmentation/train.py --dataset cityscapes --model deeplabv3_resnet50 -b 8 --epochs 30 --deconv False --pretrained-backbone False --lr 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46aff7-9d7d-4d55-9127-9c7c39150912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a167c2-c094-4333-873e-f55a2f010fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325496b-0a78-4575-8e56-b16f6824eff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d03713-f873-4640-8d0f-9efa0c17ae48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41866586-f8ec-48fe-a3f0-f64d4c7d8437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4bbd7-e0cb-4a96-8359-61ef9d68be4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
